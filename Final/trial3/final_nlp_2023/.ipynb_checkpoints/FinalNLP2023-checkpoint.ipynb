{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exam CS/INFO 662/762 Fall 2023\n",
    "CS/INFO 762: 100 points ; CS/INFO 662  90 points\n",
    "\n",
    "### <font color='red'>Due Dec 9th, 11:59am</font> - Submission via Canvas (.ipynb file)\n",
    "\n",
    "## STUDENT NAME: <font color='red'>YOUR_NAME_HERE</font>\n",
    "\n",
    "\n",
    "* Question 1a: Medical Mention Normalization with SAPBERT (PhD Students must include one graph feature) - 35/25 points\n",
    "* Question 1b: Compute Recall - 15 points\n",
    "* Question 1c: Random Forest: Feature Importance - 10 points\n",
    "* Question 2: Language Model Questions (Long Written Answer) - 40 points\n",
    "\n",
    "<font color='red'>As always WORK ON YOUR OWN for this final exam. Like last year, the final exam will be run through plagarism detection software. You may email me for clarification, but don't post on Stack Overflow, Quota, Reddit, etc..  You MAY use ChatGPT for ANY question, but the usual rules for citation and prompt inclusion in your answer apply.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed\n",
    "#!pip uninstall --yes flair\n",
    "!pip install obonet\n",
    "!pip install py-rouge\n",
    "!pip install node2vec\n",
    "!pip install rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/ozborn/Conda_Env/py311cuda121/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disease Ontology is currently size:11432 with 11462 edges\n",
      "Human Phenotype Ontology is currently size:17664 with 21975 edges\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import networkx\n",
    "import obonet\n",
    "import os\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "import heapq\n",
    "import pandas as pd\n",
    "#import scispacy\n",
    "import spacy\n",
    "import numpy as np\n",
    "import torch\n",
    "from io import StringIO\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\")  \n",
    "model = AutoModel.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\").cuda()\n",
    "\n",
    "do_url = 'https://raw.githubusercontent.com/DiseaseOntology/HumanDiseaseOntology/main/src/ontology/HumanDO.obo'\n",
    "hpo_url = 'http://purl.obolibrary.org/obo/hp.obo'\n",
    "do = obonet.read_obo(do_url)\n",
    "hpo = obonet.read_obo(hpo_url)\n",
    "print('Disease Ontology is currently size:'+str(len(do))+\" with \"+str(do.number_of_edges())+' edges')\n",
    "print('Human Phenotype Ontology is currently size:'+str(len(hpo))+\" with \"+str(hpo.number_of_edges())+' edges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Concept Normalization\n",
    "This question requires you to write use the SAPBERT embeddings you are familiar with from assignment #2 to generate candidate concepts for each input medical mentions for a merged overlapping knowledge graph of both the Disease Ontology (DO) and Human Phenotyper Ontology (HPO). \n",
    "\n",
    "### Set Up Knowledge Graph and Corpus Preparation \n",
    "This code is provided to you and creates:\n",
    "* The merged knowledge graph (kgs) from the both Disease Ontology (DO) and the Human Phenotype Ontology (HPO) as a dataframe. You also have access to the original graphs in obo format to get graph features, for example you can use node2vec.\n",
    "* The input corpus and medical mentions (labelled data) as a dataframe, \"mention_mapping\". It is built from the input corpus and you can assume that NER has already been done to identify the mentions to map. They are in the \"mention\" column and the correct concept (CUI) it should be mapped to is in the \"CUI\" column. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO Vocabulary: hpokg\n",
      "            HPOID       CUI  DOID                          HPO:Name DO:Name\n",
      "0      HP:0000001  C0444868  None                               All    None\n",
      "1      HP:0000002  C4025901  None        Abnormality of body height    None\n",
      "2      HP:0000003  C3714581  None      Multicystic kidney dysplasia    None\n",
      "3      HP:0000005  C1708511  None               Mode of inheritance    None\n",
      "4      HP:0000006  C0443147  None    Autosomal dominant inheritance    None\n",
      "...           ...       ...   ...                               ...     ...\n",
      "17659  HP:5201010      None  None  Microform cleft of the upper lip    None\n",
      "17660  HP:5201011      None  None      Complete bilateral cleft lip    None\n",
      "17661  HP:5201012      None  None    Incomplete bilateral cleft lip    None\n",
      "17662  HP:5201013      None  None     Microform bilateral cleft lip    None\n",
      "17663  HP:5201014      None  None    Asymmetric bilateral cleft lip    None\n",
      "\n",
      "[17664 rows x 5 columns]\n",
      "HPO and DO Joint Vocabulary:kgs\n",
      "            HPOID       CUI          DOID                  HPO:Name  \\\n",
      "9      HP:0000011  C0005697    DOID:12143        Neurogenic bladder   \n",
      "13     HP:0000015  C0156273    DOID:11353      Bladder diverticulum   \n",
      "20     HP:0000023  C0019294  DOID:0060320           Inguinal hernia   \n",
      "21     HP:0000024  C0033581    DOID:14654               Prostatitis   \n",
      "24     HP:0000027  C0004509    DOID:14227               Azoospermia   \n",
      "...           ...       ...           ...                       ...   \n",
      "16448  HP:0200018  C0155015    DOID:13910               Protanomaly   \n",
      "16451  HP:0200022  C0205770     DOID:2626  Choroid plexus papilloma   \n",
      "16452  HP:0200023  C0033117     DOID:9286                  Priapism   \n",
      "16480  HP:0200058  C0018923  DOID:0001816              Angiosarcoma   \n",
      "16532  HP:0200151  C1136033     DOID:3663    Cutaneous mastocytosis   \n",
      "\n",
      "                        DO:Name  \n",
      "9            neurogenic bladder  \n",
      "13         bladder diverticulum  \n",
      "20              inguinal hernia  \n",
      "21                  prostatitis  \n",
      "24                  azoospermia  \n",
      "...                         ...  \n",
      "16448       red color blindness  \n",
      "16451  choroid plexus papilloma  \n",
      "16452                  priapism  \n",
      "16480              angiosarcoma  \n",
      "16532    cutaneous mastocytosis  \n",
      "\n",
      "[970 rows x 5 columns]\n",
      "Input Corpus Mentions:mention_mapping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CUI</th>\n",
       "      <th>start1</th>\n",
       "      <th>stop1</th>\n",
       "      <th>start2</th>\n",
       "      <th>stop2</th>\n",
       "      <th>start3</th>\n",
       "      <th>stop3</th>\n",
       "      <th>mention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N000</td>\n",
       "      <td>C0011854</td>\n",
       "      <td>248</td>\n",
       "      <td>283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>insulin dependent diabetes mellitus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N001</td>\n",
       "      <td>C4303631</td>\n",
       "      <td>298</td>\n",
       "      <td>327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a right above-knee amputation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N003</td>\n",
       "      <td>C0085671</td>\n",
       "      <td>537</td>\n",
       "      <td>553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dressing changes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N004</td>\n",
       "      <td>C0011079</td>\n",
       "      <td>558</td>\n",
       "      <td>569</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>debridement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N005</td>\n",
       "      <td>C0003232</td>\n",
       "      <td>611</td>\n",
       "      <td>622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>antibiotics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6679</th>\n",
       "      <td>N139</td>\n",
       "      <td>C0442519</td>\n",
       "      <td>4695</td>\n",
       "      <td>4699</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6680</th>\n",
       "      <td>N140</td>\n",
       "      <td>C0699203</td>\n",
       "      <td>4731</td>\n",
       "      <td>4737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>motrin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6681</th>\n",
       "      <td>N141</td>\n",
       "      <td>C0593507</td>\n",
       "      <td>4740</td>\n",
       "      <td>4745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>advil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6682</th>\n",
       "      <td>N142</td>\n",
       "      <td>C0332575</td>\n",
       "      <td>4863</td>\n",
       "      <td>4870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>redness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6683</th>\n",
       "      <td>N143</td>\n",
       "      <td>C0205217</td>\n",
       "      <td>4853</td>\n",
       "      <td>4862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>increased</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6684 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID       CUI  start1  stop1  start2  stop2  start3  stop3  \\\n",
       "0     N000  C0011854     248    283     NaN    NaN     NaN    NaN   \n",
       "1     N001  C4303631     298    327     NaN    NaN     NaN    NaN   \n",
       "2     N003  C0085671     537    553     NaN    NaN     NaN    NaN   \n",
       "3     N004  C0011079     558    569     NaN    NaN     NaN    NaN   \n",
       "4     N005  C0003232     611    622     NaN    NaN     NaN    NaN   \n",
       "...    ...       ...     ...    ...     ...    ...     ...    ...   \n",
       "6679  N139  C0442519    4695   4699     NaN    NaN     NaN    NaN   \n",
       "6680  N140  C0699203    4731   4737     NaN    NaN     NaN    NaN   \n",
       "6681  N141  C0593507    4740   4745     NaN    NaN     NaN    NaN   \n",
       "6682  N142  C0332575    4863   4870     NaN    NaN     NaN    NaN   \n",
       "6683  N143  C0205217    4853   4862     NaN    NaN     NaN    NaN   \n",
       "\n",
       "                                  mention  \n",
       "0     insulin dependent diabetes mellitus  \n",
       "1           a right above-knee amputation  \n",
       "2                        dressing changes  \n",
       "3                             debridement  \n",
       "4                             antibiotics  \n",
       "...                                   ...  \n",
       "6679                                 home  \n",
       "6680                               motrin  \n",
       "6681                                advil  \n",
       "6682                              redness  \n",
       "6683                            increased  \n",
       "\n",
       "[6684 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createIndex(graph,prefix):\n",
    "    id2cui = {}\n",
    "    cui2id = {}\n",
    "    id_to_xref = {id_: data.get('xref') for id_, data in graph.nodes(data=True)}\n",
    "    for graph_id,xrefs in id_to_xref.items():\n",
    "        if(xrefs is None):\n",
    "            cui = None\n",
    "        else:\n",
    "            cui = next((x for x in xrefs if x.startswith(prefix)),None)\n",
    "            if(cui is not None):\n",
    "                cui = cui.replace(prefix,'')\n",
    "        id2cui[graph_id]=cui\n",
    "        if(cui is not None):\n",
    "            cui2id[cui]=graph_id\n",
    "    return(id2cui,cui2id)\n",
    "\n",
    "\n",
    "def convertCui2Doid(cui):\n",
    "    if cui in cui2do:\n",
    "        return cui2do[cui]\n",
    "    return None\n",
    "\n",
    "def hpoId2Name(oboid):\n",
    "    return hpoid_to_name[oboid]\n",
    "\n",
    "def doId2Name(oboid):\n",
    "    if(oboid is None):\n",
    "        return None\n",
    "    if (doid_to_name[oboid]):\n",
    "        return doid_to_name[oboid]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_mentions(filename,bardoc):\n",
    "    all_mentions = []\n",
    "    with open(filename, 'r') as file: \n",
    "        textdoc = file.read()\n",
    "        for line in bardoc.splitlines():\n",
    "            #print(line)\n",
    "            start = int(line.split(\"||\")[2])\n",
    "            stop = int(line.split(\"||\")[3])\n",
    "            mention = textdoc[start:stop]\n",
    "            if(not line.endswith(\"||||||\")):\n",
    "                start = int(line.split(\"||\")[4])\n",
    "                stop = int(line.split(\"||\")[5])\n",
    "                extramention = textdoc[start:stop]\n",
    "                mention = mention+' '+extramention\n",
    "                if(not line.endswith(\"||||\")):\n",
    "                    start = int(line.split(\"||\")[6])\n",
    "                    stop = int(line.split(\"||\")[7])\n",
    "                    extramention = textdoc[start:stop]\n",
    "                    mention = mention+' '+extramention\n",
    "            #print(mention)\n",
    "            all_mentions.append(mention)\n",
    "    return all_mentions\n",
    "\n",
    "def read_files(directory):\n",
    "    all_data = []\n",
    "    for file in os.listdir(directory):\n",
    "        #print(file)\n",
    "        if file.endswith(\".norm\"):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            with open(file_path, 'r') as file:\n",
    "                csv_string = file.read()\n",
    "            #normed = [line+\"||||\" for line in csv_string.splitlines() if line.count('|')==6]\n",
    "            normed = [line if line.count('|') == 14 else (line+\"||||\" if line.count('|') == 10 else line+\"||||||||\") for line in csv_string.splitlines()]\n",
    "            clean = '\\n'.join(normed)\n",
    "            note_file = (str(file.name).replace(\"train_norm\",\"train_note\").replace(\"norm\",\"txt\"))\n",
    "            mentions = get_mentions(note_file,clean)\n",
    "            df = pd.read_csv(StringIO(clean),engine='python',names=['ID', 'CUI', 'start1', 'stop1','start2','stop2','start3','stop3'],sep=\"\\|\\|\")\n",
    "            df['mention']=mentions\n",
    "        all_data.append(df)\n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "\n",
    "hpo2cui,cui2hpo = createIndex(hpo,'UMLS:')\n",
    "do2cui,cui2do = createIndex(do,'UMLS_CUI:')\n",
    "\n",
    "hpoid_to_name = {id_: data.get('name') for id_, data in hpo.nodes(data=True)}\n",
    "doid_to_name = {id_: data.get('name') for id_, data in do.nodes(data=True)}\n",
    "\n",
    "df = pd.DataFrame(list(hpo2cui.items()))\n",
    "df.columns=['HPOID','CUI']\n",
    "df['DOID'] = df['CUI'].apply(convertCui2Doid)\n",
    "df['HPO:Name'] = df['HPOID'].apply(hpoId2Name)\n",
    "df['DO:Name'] = df['DOID'].apply(doId2Name)\n",
    "hpokg = df.copy()\n",
    "print(\"HPO Vocabulary: hpokg\")\n",
    "print(hpokg)\n",
    "kgs = df.mask(df.eq('None')).dropna()\n",
    "\n",
    "# Graph properties that may be useful\n",
    "id_to_isa = {id_: data.get('is_a') for id_, data in hpo.nodes(data=True)}\n",
    "id_to_xref = {id_: data.get('xref') for id_, data in do.nodes(data=True)}\n",
    "result = next(iter(id_to_xref.values()))   \n",
    "\n",
    "print(\"HPO and DO Joint Vocabulary:kgs\")\n",
    "print(kgs)\n",
    "mention_mapping = read_files(\"train/train_norm/\")\n",
    "print(\"Input Corpus Mentions:mention_mapping\")\n",
    "mention_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1a: Generation of Candidate Concepts and their Features (35 points PhD/ 25 points MS)\n",
    "\n",
    "\n",
    "#### Write code to find the best N candidate concepts for the mention using SAPBERT in the small (for final exam performance purposes) merged kgs vocabularuy.\n",
    "\n",
    "The signature of the function should look something like this:\n",
    "``` \n",
    "def getCandidates(mention_embeddings, vocabulary_embeddings, max_candidates):\n",
    "```\n",
    "* mention_embeddings would be SAPBERT embeddings of the mentions\n",
    "* vocabulary_embeddings would be SAPBERT embeddings of the kgs vocabulary. You generate them using just DO concept text, just HPO concept text or perform a function to aggregate them.\n",
    "* max_candidates (max candidates to return from kgs)\n",
    "\n",
    "This function returns a list of the best N matches between the mention and the target merged knowledge graph based on feature similarity between the input node and the target node. Each match in the list is a tuple can contain any elements you need, but it should at least contain\n",
    " * a reference to the target concept, ie) row index|vocabulary_id\n",
    " * score (optional) or anything else you think you need\n",
    " \n",
    " \n",
    "#### Write code to get a set of features for each candidate concepts that can be used for ranking the top N concepts to pick the most correct concept\n",
    "The getFeatures function should generate features for an input mention text and one possible candidate mapping.\n",
    "```\n",
    "def getFeatures(mention_text, candidate_tuple_from_getCandidates)\n",
    "```\n",
    "These features will be used in Part 1b) to generate training data for a machine learning ranking algorithm.\n",
    "\n",
    "Masters student need at least 2 features in their getFeatures code, some examples of lexical features include:\n",
    "* counts of matching words or characters\n",
    "* longest common subsequence (RougeL)\n",
    "* ngram overlap, etc...\n",
    "\n",
    "PhD Students will need an additional graph-based feature using relations in the ontology or ontology node vector representations such as node2vec. For example, one relevant feature may be checking the similarity of the input node to the parent node of the target. They can also be generated per random-walks like node2vec.\n",
    "\n",
    "\n",
    "Hints:\n",
    " * stop words, stemming, lemmatizationm, headword matching are nice but not required for this tiny (mostly matching) gold data set\n",
    " * my advice is to do the minimal amount of work and come back later if you want to add more features\n",
    " * you may use ANY additional libraries as need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Candidate Generation Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1b: Compute Recall@3 and Generate Data for ML Algorithm in Part 1c (15 points)\n",
    " * Use your function in Part 1a) to generate 3 candidates for every mention and compute recall at n=3 candidates (For each mention, what is the fraction of times that the getCandidates returned the correct concept (CUI)?). Your re-ranking algorithm will not be able to do better than this. (5 points)\n",
    " * Many mentions represent concepts not included in our small merged kgs. Despite this, your recall performance may still not match your expectations using just SAPBERT embeddings. Explain why this might be (5 points).  \n",
    " * Create a labelled candidate ranking data set (5 points). For each mention, there will be 3 examples of which only 1 will have the correct CUI. Each example will have features (X) from part 1a and a label (Y). The label will be 1 if the features are sourced from the correct CUI and 0 if not. Use your getFeatures function to populate X. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute Recall for Candidate Generation Code\n",
    "\n",
    "\n",
    "# Create X (data), Y (label) for ranking algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1c: Random Forest Candidate Ranker and Feature Analysis (10 points)\n",
    "\n",
    " * Split your data into training and testing data and then train scikit-learn's RandomForestClassifier to predict if a candidate node is the correct match. Output a classification report with accuracy.\n",
    " \n",
    " * Use scikit-learn's RandomForestClassifier to compute the relative importance of your features for this algorithm and graph them. Give your features reasonable names so they look nice on a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from matplotlib import pyplot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2a (20 points)\n",
    "\n",
    "One of the issues with medical normalization is that training data is sparse, some disease are over-represented, whereas some rare disease have a dictionary entry but few examples in clinical text. Making at least one reference to a paper discussed in class:\n",
    "\n",
    "\n",
    "* Describe how you could use a LLM (like GPT-4) to generate a synthetic corpus for concept normalization to an ontology like the Human Phenotype Ontology described here? Assume you would like to generate synthetic data for concepts not included in typical training data. (10 points)\n",
    "\n",
    "\n",
    "* Propose an evaluation method for your synthetic text generation method. How would you evaluate whether your approach is successfull? (10 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2b (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of 2023, transfer learning using large language models such as GPT-4, etc.. is the current best practise for a large number of tasks. There has been speculation in the popular press that these models will function as artificial general intelligences, making domain specific models redundant.\n",
    "\n",
    "* Making references to at least one paper discussed in class, describe performance results indicating that this is not the case. (10 points)\n",
    "\n",
    "* Describe at least 2 benefits of using a domain specific language model that has been fine-tuned on a task,  relative to a model like GPT-4 (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py311cuda121]",
   "language": "python",
   "name": "conda-env-py311cuda121-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
