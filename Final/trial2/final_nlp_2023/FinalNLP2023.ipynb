{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exam CS/INFO 662/762 Fall 2023\n",
    "CS/INFO 762: 100 points ; CS/INFO 662  90 points\n",
    "\n",
    "### <font color='red'>Due Dec 9th, 11:59am</font> - Submission via Canvas (.ipynb file)\n",
    "\n",
    "## STUDENT NAME: <font color='red'>MICHAEL GATHARA</font>\n",
    "\n",
    "\n",
    "* Question 1a: Medical Mention Normalization with SAPBERT (PhD Students must include one graph feature) - 35/25 points\n",
    "* Question 1b: Compute Recall - 15 points\n",
    "* Question 1c: Random Forest: Feature Importance - 10 points\n",
    "* Question 2: Language Model Questions (Long Written Answer) - 40 points\n",
    "\n",
    "<font color='red'>As always WORK ON YOUR OWN for this final exam. Like last year, the final exam will be run through plagarism detection software. You may email me for clarification, but don't post on Stack Overflow, Quota, Reddit, etc..  You MAY use ChatGPT for ANY question, but the usual rules for citation and prompt inclusion in your answer apply.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed\n",
    "#!pip uninstall --yes flair\n",
    "!pip install obonet\n",
    "!pip install py-rouge\n",
    "!pip install node2vec\n",
    "!pip install rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disease Ontology is currently size:11432 with 11462 edges\n",
      "Human Phenotype Ontology is currently size:17664 with 21975 edges\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import networkx\n",
    "import obonet\n",
    "import os\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "import heapq\n",
    "import pandas as pd\n",
    "#import scispacy\n",
    "import spacy\n",
    "import numpy as np\n",
    "import torch\n",
    "from io import StringIO\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\")  \n",
    "model = AutoModel.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\").cuda()\n",
    "\n",
    "do_url = 'https://raw.githubusercontent.com/DiseaseOntology/HumanDiseaseOntology/main/src/ontology/HumanDO.obo'\n",
    "hpo_url = 'http://purl.obolibrary.org/obo/hp.obo'\n",
    "do = obonet.read_obo(do_url)\n",
    "hpo = obonet.read_obo(hpo_url)\n",
    "print('Disease Ontology is currently size:'+str(len(do))+\" with \"+str(do.number_of_edges())+' edges')\n",
    "print('Human Phenotype Ontology is currently size:'+str(len(hpo))+\" with \"+str(hpo.number_of_edges())+' edges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Concept Normalization\n",
    "This question requires you to write use the SAPBERT embeddings you are familiar with from assignment #2 to generate candidate concepts for each input medical mentions for a merged overlapping knowledge graph of both the Disease Ontology (DO) and Human Phenotyper Ontology (HPO). \n",
    "\n",
    "### Set Up Knowledge Graph and Corpus Preparation \n",
    "This code is provided to you and creates:\n",
    "* The merged knowledge graph (kgs) from the both Disease Ontology (DO) and the Human Phenotype Ontology (HPO) as a dataframe. You also have access to the original graphs in obo format to get graph features, for example you can use node2vec.\n",
    "* The input corpus and medical mentions (labelled data) as a dataframe, \"mention_mapping\". It is built from the input corpus and you can assume that NER has already been done to identify the mentions to map. They are in the \"mention\" column and the correct concept (CUI) it should be mapped to is in the \"CUI\" column. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO Vocabulary: hpokg\n",
      "            HPOID       CUI  DOID                          HPO:Name DO:Name\n",
      "0      HP:0000001  C0444868  None                               All    None\n",
      "1      HP:0000002  C4025901  None        Abnormality of body height    None\n",
      "2      HP:0000003  C3714581  None      Multicystic kidney dysplasia    None\n",
      "3      HP:0000005  C1708511  None               Mode of inheritance    None\n",
      "4      HP:0000006  C0443147  None    Autosomal dominant inheritance    None\n",
      "...           ...       ...   ...                               ...     ...\n",
      "17659  HP:5201010      None  None  Microform cleft of the upper lip    None\n",
      "17660  HP:5201011      None  None      Complete bilateral cleft lip    None\n",
      "17661  HP:5201012      None  None    Incomplete bilateral cleft lip    None\n",
      "17662  HP:5201013      None  None     Microform bilateral cleft lip    None\n",
      "17663  HP:5201014      None  None    Asymmetric bilateral cleft lip    None\n",
      "\n",
      "[17664 rows x 5 columns]\n",
      "HPO and DO Joint Vocabulary:kgs\n",
      "            HPOID       CUI          DOID                  HPO:Name  \\\n",
      "9      HP:0000011  C0005697    DOID:12143        Neurogenic bladder   \n",
      "13     HP:0000015  C0156273    DOID:11353      Bladder diverticulum   \n",
      "20     HP:0000023  C0019294  DOID:0060320           Inguinal hernia   \n",
      "21     HP:0000024  C0033581    DOID:14654               Prostatitis   \n",
      "24     HP:0000027  C0004509    DOID:14227               Azoospermia   \n",
      "...           ...       ...           ...                       ...   \n",
      "16448  HP:0200018  C0155015    DOID:13910               Protanomaly   \n",
      "16451  HP:0200022  C0205770     DOID:2626  Choroid plexus papilloma   \n",
      "16452  HP:0200023  C0033117     DOID:9286                  Priapism   \n",
      "16480  HP:0200058  C0018923  DOID:0001816              Angiosarcoma   \n",
      "16532  HP:0200151  C1136033     DOID:3663    Cutaneous mastocytosis   \n",
      "\n",
      "                        DO:Name  \n",
      "9            neurogenic bladder  \n",
      "13         bladder diverticulum  \n",
      "20              inguinal hernia  \n",
      "21                  prostatitis  \n",
      "24                  azoospermia  \n",
      "...                         ...  \n",
      "16448       red color blindness  \n",
      "16451  choroid plexus papilloma  \n",
      "16452                  priapism  \n",
      "16480              angiosarcoma  \n",
      "16532    cutaneous mastocytosis  \n",
      "\n",
      "[970 rows x 5 columns]\n",
      "Input Corpus Mentions:mention_mapping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CUI</th>\n",
       "      <th>start1</th>\n",
       "      <th>stop1</th>\n",
       "      <th>start2</th>\n",
       "      <th>stop2</th>\n",
       "      <th>start3</th>\n",
       "      <th>stop3</th>\n",
       "      <th>mention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N000</td>\n",
       "      <td>C0011854</td>\n",
       "      <td>248</td>\n",
       "      <td>283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>insulin dependent diabetes mellitus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N001</td>\n",
       "      <td>C4303631</td>\n",
       "      <td>298</td>\n",
       "      <td>327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a right above-knee amputation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N003</td>\n",
       "      <td>C0085671</td>\n",
       "      <td>537</td>\n",
       "      <td>553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dressing changes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N004</td>\n",
       "      <td>C0011079</td>\n",
       "      <td>558</td>\n",
       "      <td>569</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>debridement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N005</td>\n",
       "      <td>C0003232</td>\n",
       "      <td>611</td>\n",
       "      <td>622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>antibiotics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6679</th>\n",
       "      <td>N139</td>\n",
       "      <td>C0442519</td>\n",
       "      <td>4695</td>\n",
       "      <td>4699</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6680</th>\n",
       "      <td>N140</td>\n",
       "      <td>C0699203</td>\n",
       "      <td>4731</td>\n",
       "      <td>4737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>motrin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6681</th>\n",
       "      <td>N141</td>\n",
       "      <td>C0593507</td>\n",
       "      <td>4740</td>\n",
       "      <td>4745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>advil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6682</th>\n",
       "      <td>N142</td>\n",
       "      <td>C0332575</td>\n",
       "      <td>4863</td>\n",
       "      <td>4870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>redness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6683</th>\n",
       "      <td>N143</td>\n",
       "      <td>C0205217</td>\n",
       "      <td>4853</td>\n",
       "      <td>4862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>increased</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6684 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID       CUI  start1  stop1  start2  stop2  start3  stop3  \\\n",
       "0     N000  C0011854     248    283     NaN    NaN     NaN    NaN   \n",
       "1     N001  C4303631     298    327     NaN    NaN     NaN    NaN   \n",
       "2     N003  C0085671     537    553     NaN    NaN     NaN    NaN   \n",
       "3     N004  C0011079     558    569     NaN    NaN     NaN    NaN   \n",
       "4     N005  C0003232     611    622     NaN    NaN     NaN    NaN   \n",
       "...    ...       ...     ...    ...     ...    ...     ...    ...   \n",
       "6679  N139  C0442519    4695   4699     NaN    NaN     NaN    NaN   \n",
       "6680  N140  C0699203    4731   4737     NaN    NaN     NaN    NaN   \n",
       "6681  N141  C0593507    4740   4745     NaN    NaN     NaN    NaN   \n",
       "6682  N142  C0332575    4863   4870     NaN    NaN     NaN    NaN   \n",
       "6683  N143  C0205217    4853   4862     NaN    NaN     NaN    NaN   \n",
       "\n",
       "                                  mention  \n",
       "0     insulin dependent diabetes mellitus  \n",
       "1           a right above-knee amputation  \n",
       "2                        dressing changes  \n",
       "3                             debridement  \n",
       "4                             antibiotics  \n",
       "...                                   ...  \n",
       "6679                                 home  \n",
       "6680                               motrin  \n",
       "6681                                advil  \n",
       "6682                              redness  \n",
       "6683                            increased  \n",
       "\n",
       "[6684 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createIndex(graph,prefix):\n",
    "    id2cui = {}\n",
    "    cui2id = {}\n",
    "    id_to_xref = {id_: data.get('xref') for id_, data in graph.nodes(data=True)}\n",
    "    for graph_id,xrefs in id_to_xref.items():\n",
    "        if(xrefs is None):\n",
    "            cui = None\n",
    "        else:\n",
    "            cui = next((x for x in xrefs if x.startswith(prefix)),None)\n",
    "            if(cui is not None):\n",
    "                cui = cui.replace(prefix,'')\n",
    "        id2cui[graph_id]=cui\n",
    "        if(cui is not None):\n",
    "            cui2id[cui]=graph_id\n",
    "    return(id2cui,cui2id)\n",
    "\n",
    "\n",
    "def convertCui2Doid(cui):\n",
    "    if cui in cui2do:\n",
    "        return cui2do[cui]\n",
    "    return None\n",
    "\n",
    "def hpoId2Name(oboid):\n",
    "    return hpoid_to_name[oboid]\n",
    "\n",
    "def doId2Name(oboid):\n",
    "    if(oboid is None):\n",
    "        return None\n",
    "    if (doid_to_name[oboid]):\n",
    "        return doid_to_name[oboid]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_mentions(filename,bardoc):\n",
    "    all_mentions = []\n",
    "    with open(filename, 'r') as file: \n",
    "        textdoc = file.read()\n",
    "        for line in bardoc.splitlines():\n",
    "            #print(line)\n",
    "            start = int(line.split(\"||\")[2])\n",
    "            stop = int(line.split(\"||\")[3])\n",
    "            mention = textdoc[start:stop]\n",
    "            if(not line.endswith(\"||||||\")):\n",
    "                start = int(line.split(\"||\")[4])\n",
    "                stop = int(line.split(\"||\")[5])\n",
    "                extramention = textdoc[start:stop]\n",
    "                mention = mention+' '+extramention\n",
    "                if(not line.endswith(\"||||\")):\n",
    "                    start = int(line.split(\"||\")[6])\n",
    "                    stop = int(line.split(\"||\")[7])\n",
    "                    extramention = textdoc[start:stop]\n",
    "                    mention = mention+' '+extramention\n",
    "            #print(mention)\n",
    "            all_mentions.append(mention)\n",
    "    return all_mentions\n",
    "\n",
    "def read_files(directory):\n",
    "    all_data = []\n",
    "    for file in os.listdir(directory):\n",
    "        #print(file)\n",
    "        if file.endswith(\".norm\"):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            with open(file_path, 'r') as file:\n",
    "                csv_string = file.read()\n",
    "            #normed = [line+\"||||\" for line in csv_string.splitlines() if line.count('|')==6]\n",
    "            normed = [line if line.count('|') == 14 else (line+\"||||\" if line.count('|') == 10 else line+\"||||||||\") for line in csv_string.splitlines()]\n",
    "            clean = '\\n'.join(normed)\n",
    "            note_file = (str(file.name).replace(\"train_norm\",\"train_note\").replace(\"norm\",\"txt\"))\n",
    "            mentions = get_mentions(note_file,clean)\n",
    "            df = pd.read_csv(StringIO(clean),engine='python',names=['ID', 'CUI', 'start1', 'stop1','start2','stop2','start3','stop3'],sep=\"\\|\\|\")\n",
    "            df['mention']=mentions\n",
    "        all_data.append(df)\n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "\n",
    "hpo2cui,cui2hpo = createIndex(hpo,'UMLS:')\n",
    "do2cui,cui2do = createIndex(do,'UMLS_CUI:')\n",
    "\n",
    "hpoid_to_name = {id_: data.get('name') for id_, data in hpo.nodes(data=True)}\n",
    "doid_to_name = {id_: data.get('name') for id_, data in do.nodes(data=True)}\n",
    "\n",
    "df = pd.DataFrame(list(hpo2cui.items()))\n",
    "df.columns=['HPOID','CUI']\n",
    "df['DOID'] = df['CUI'].apply(convertCui2Doid)\n",
    "df['HPO:Name'] = df['HPOID'].apply(hpoId2Name)\n",
    "df['DO:Name'] = df['DOID'].apply(doId2Name)\n",
    "hpokg = df.copy()\n",
    "print(\"HPO Vocabulary: hpokg\")\n",
    "print(hpokg)\n",
    "kgs = df.mask(df.eq('None')).dropna()\n",
    "\n",
    "# Graph properties that may be useful\n",
    "id_to_isa = {id_: data.get('is_a') for id_, data in hpo.nodes(data=True)}\n",
    "id_to_xref = {id_: data.get('xref') for id_, data in do.nodes(data=True)}\n",
    "result = next(iter(id_to_xref.values()))   \n",
    "\n",
    "print(\"HPO and DO Joint Vocabulary:kgs\")\n",
    "print(kgs)\n",
    "mention_mapping = read_files(\"train/train_norm/\")\n",
    "print(\"Input Corpus Mentions:mention_mapping\")\n",
    "mention_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1a: Generation of Candidate Concepts and their Features (35 points PhD/ 25 points MS)\n",
    "\n",
    "\n",
    "#### Write code to find the best N candidate concepts for the mention using SAPBERT in the small (for final exam performance purposes) merged kgs vocabularuy.\n",
    "\n",
    "The signature of the function should look something like this:\n",
    "``` \n",
    "def getCandidates(mention_embeddings, vocabulary_embeddings, max_candidates):\n",
    "```\n",
    "* mention_embeddings would be SAPBERT embeddings of the mentions\n",
    "* vocabulary_embeddings would be SAPBERT embeddings of the kgs vocabulary. You generate them using just DO concept text, just HPO concept text or perform a function to aggregate them.\n",
    "* max_candidates (max candidates to return from kgs)\n",
    "\n",
    "This function returns a list of the best N matches between the mention and the target merged knowledge graph based on feature similarity between the input node and the target node. Each match in the list is a tuple can contain any elements you need, but it should at least contain\n",
    " * a reference to the target concept, ie) row index|vocabulary_id\n",
    " * score (optional) or anything else you think you need\n",
    " \n",
    " \n",
    "#### Write code to get a set of features for each candidate concepts that can be used for ranking the top N concepts to pick the most correct concept\n",
    "The getFeatures function should generate features for an input mention text and one possible candidate mapping.\n",
    "```\n",
    "def getFeatures(mention_text, candidate_tuple_from_getCandidates)\n",
    "```\n",
    "These features will be used in Part 1b) to generate training data for a machine learning ranking algorithm.\n",
    "\n",
    "Masters student need at least 2 features in their getFeatures code, some examples of lexical features include:\n",
    "* counts of matching words or characters\n",
    "* longest common subsequence (RougeL)\n",
    "* ngram overlap, etc...\n",
    "\n",
    "PhD Students will need an additional graph-based feature using relations in the ontology or ontology node vector representations such as node2vec. For example, one relevant feature may be checking the similarity of the input node to the parent node of the target. They can also be generated per random-walks like node2vec.\n",
    "\n",
    "\n",
    "Hints:\n",
    " * stop words, stemming, lemmatizationm, headword matching are nice but not required for this tiny (mostly matching) gold data set\n",
    " * my advice is to do the minimal amount of work and come back later if you want to add more features\n",
    " * you may use ANY additional libraries as need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from rouge_score import rouge_scorer\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "# Global RougeScorer initialization to avoid repeated instantiation\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n",
    "\n",
    "# Candidate Generation Code\n",
    "def getCandidates(mention_embeddings, vocabulary_embeddings, max_candidates):\n",
    "    # Use NearestNeighbors for efficient nearest neighbor search\n",
    "    neigh = NearestNeighbors(n_neighbors=max_candidates, metric='cosine')\n",
    "    neigh.fit(vocabulary_embeddings)\n",
    "\n",
    "    distances, indices = neigh.kneighbors(mention_embeddings.reshape(1, -1))\n",
    "    \n",
    "    candidate_list = [(index, 1 - distance) for index, distance in zip(indices[0], distances[0])]\n",
    "    \n",
    "    return candidate_list\n",
    "\n",
    "def getFeatures(mention_text, candidate_tuple_from_getCandidates):\n",
    "    # Unpack candidate information (e.g., index and score)\n",
    "    candidate_index, similarity_score = candidate_tuple_from_getCandidates\n",
    "    \n",
    "    candidate_concept = kgs.iloc[candidate_index]\n",
    "    \n",
    "    # Calculate RougeL score\n",
    "    rouge_l_score = scorer.score(candidate_concept['HPO:Name'], mention_text)['rougeL'].fmeasure\n",
    "\n",
    "    # Calculate ngram overlap using efficient Counter arithmetic\n",
    "    mention_tokens = mention_text.lower().split()\n",
    "    candidate_tokens = candidate_concept['HPO:Name'].lower().split()\n",
    "    \n",
    "    mention_ngrams = Counter(chain(*[zip(mention_tokens[i:], mention_tokens) for i in range(2)]))\n",
    "    candidate_ngrams = Counter(chain(*[zip(candidate_tokens[i:], candidate_tokens) for i in range(2)]))\n",
    "    \n",
    "    ngram_overlap = sum((mention_ngrams & candidate_ngrams).values())\n",
    "    \n",
    "    return {'RougeL_Score': rouge_l_score, 'Ngram_Overlap': ngram_overlap, 'Cosine_Similarity': similarity_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1b: Compute Recall@3 and Generate Data for ML Algorithm in Part 1c (15 points)\n",
    " * Use your function in Part 1a) to generate 3 candidates for every mention and compute recall at n=3 candidates (For each mention, what is the fraction of times that the getCandidates returned the correct concept (CUI)?). Your re-ranking algorithm will not be able to do better than this. (5 points)\n",
    " * Many mentions represent concepts not included in our small merged kgs. Despite this, your recall performance may still not match your expectations using just SAPBERT embeddings. Explain why this might be (5 points).  \n",
    " <font color=\"red\">The SAPBERT embeddings may not have the concepts that we include within our kgs, thus leading to relatively low recall and accuracy. This can be improved by increased the amount of data, maybe even using synthetic data, to make the SAPBERT embeddings better</font>\n",
    " * Create a labelled candidate ranking data set (5 points). For each mention, there will be 3 examples of which only 1 will have the correct CUI. Each example will have features (X) from part 1a and a label (Y). The label will be 1 if the features are sourced from the correct CUI and 0 if not. Use your getFeatures function to populate X. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text, device='cuda'):\n",
    "    # Tokenize the input text using the provided tokenizer\n",
    "    tokens = tokenizer.encode_plus(text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    tokens = {key: value.to(device) for key, value in tokens.items()}\n",
    "    \n",
    "    # Forward pass the tokens through the model to get the embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().detach().cpu().numpy()\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad40381fe03c41feb24a74fb38ecd534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a374a42f32488cadf5636d0e25d6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocabulary_texts = list(kgs['HPO:Name']) # Assuming 'DO:Name' contains the text data for vocabulary\n",
    "# test = list(kgs['DO:Name']) + list(kgs['HPO:Name'])\n",
    "# Generate SAPBERT embeddings for the vocabulary texts\n",
    "vocabulary_embeddings = []\n",
    "for text in tqdm(vocabulary_texts):\n",
    "    embedding = get_embeddings(text)\n",
    "    vocabulary_embeddings.append(embedding)\n",
    "    \n",
    "mention_embeddings = []\n",
    "for text in tqdm(mention_mapping['mention']):\n",
    "    embedding = get_embeddings(text)\n",
    "    mention_embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary_embeddings = np.array(vocabulary_embeddings)\n",
    "# mention_embeddings = np.array(mention_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f09254764d40579dd2b1037c5f6a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall at 3 candidates: 0.06149012567324955\n",
      "CPU times: user 1min 4s, sys: 211 ms, total: 1min 4s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute Recall for Candidate Generation Code\n",
    "# Create X (data), Y (label) for ranking algorithm.\n",
    "# Initialize variables to track recall calculation\n",
    "total_mentions = len(mention_mapping)\n",
    "correct_candidates = 0\n",
    "\n",
    "total_mentions = len(mention_mapping)\n",
    "correct_candidates = 0\n",
    "\n",
    "for mention in tqdm(mention_mapping['mention']):\n",
    "    mention_embedding = get_embeddings(mention)\n",
    "    \n",
    "    candidates = getCandidates(mention_embedding, vocabulary_embeddings, 3)\n",
    "    \n",
    "    correct_cui = mention_mapping[mention_mapping['mention'] == mention]['CUI'].values[0]\n",
    "    \n",
    "    top_candidates = [kgs.iloc[candidate[0]]['CUI'] for candidate in candidates]\n",
    "    if correct_cui in top_candidates:\n",
    "        correct_candidates += 1\n",
    "\n",
    "# Compute recall at n=3 candidates\n",
    "recall_at_3 = correct_candidates / total_mentions\n",
    "print(f\"Recall at 3 candidates: {recall_at_3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4275073461294a0ca412e21fc43bd633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Loop through mentions to generate labeled dataset samples\n",
    "for mention in tqdm(mention_mapping['mention']):\n",
    "    mention_text = mention_mapping[mention_mapping['mention'] == mention]['mention'].values[0]\n",
    "    mention_embedding = get_embeddings(mention_text)\n",
    "    \n",
    "    # Get candidates for the mention\n",
    "    candidates = getCandidates(mention_embedding, vocabulary_embeddings, 3)\n",
    "    \n",
    "    # Retrieve correct CUI for the mention\n",
    "    correct_cui = mention_mapping[mention_mapping['mention'] == mention]['CUI'].values[0]\n",
    "    \n",
    "    # Create labeled samples for each candidate\n",
    "    for candidate in candidates:\n",
    "        candidate_index, _ = candidate\n",
    "        candidate_cui = kgs.iloc[candidate_index]['CUI']\n",
    "\n",
    "        # Generate features for the candidate\n",
    "        candidate_features = getFeatures(mention_text, candidate)\n",
    "\n",
    "        # Assign label based on correct CUI match\n",
    "        label = 1 if candidate_cui == correct_cui else 0\n",
    "\n",
    "        # Append features and label to the dataset\n",
    "        X.append(candidate_features)\n",
    "        Y.append(label)\n",
    "\n",
    "# Convert X and Y lists to pandas DataFrame\n",
    "dataset = pd.DataFrame(X)\n",
    "dataset['Label'] = Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1c: Random Forest Candidate Ranker and Feature Analysis (10 points)\n",
    "\n",
    " * Split your data into training and testing data and then train scikit-learn's RandomForestClassifier to predict if a candidate node is the correct match. Output a classification report with accuracy.\n",
    " \n",
    " * Use scikit-learn's RandomForestClassifier to compute the relative importance of your features for this algorithm and graph them. Give your features reasonable names so they look nice on a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Assuming `data` is a DataFrame with your features and labels\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.drop(columns=['Label']), dataset['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Generate classification report and accuracy\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "accuracy = rf_classifier.score(X_test, y_test)\n",
    "print(f\"Classification Report:\\n{classification_rep}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate feature importance\n",
    "feature_importance = rf_classifier.feature_importances_\n",
    "\n",
    "# Get feature names from the dataset\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame of feature importance for better visualization\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plotting feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Feature Importance in RandomForestClassifier')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2a (20 points)\n",
    "\n",
    "One of the issues with medical normalization is that training data is sparse, some disease are over-represented, whereas some rare disease have a dictionary entry but few examples in clinical text. Making at least one reference to a paper discussed in class:\n",
    "\n",
    "\n",
    "* Describe how you could use a LLM (like GPT-4) to generate a synthetic corpus for concept normalization to an ontology like the Human Phenotype Ontology described here? Assume you would like to generate synthetic data for concepts not included in typical training data. (10 points)\n",
    "\n",
    "\n",
    "* Propose an evaluation method for your synthetic text generation method. How would you evaluate whether your approach is successfull? (10 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Describe how you could use a LLM (like GPT-4) to generate a synthetic corpus for concept normalization to an ontology like the Human Phenotype Ontology described here? Assume you would like to generate synthetic data for concepts not included in typical training data. (10 points)</font>\n",
    "<br>\n",
    "<br>\n",
    "The best thing about GPT4 is that it excels in text generation. This makes it perfect for generating synthetic corpora. One of the biggest problems is that it needs a couple examples of data to copy as the accuracy increases with more examples, often called x-shot prompting. As the data is already rare, it may be hard to find examples unless a doctor/specialist feeds it data. This is well highlighted in the paper <font color=\"red\">Can Synthetic Text Help Clinical Named Entity Recognition? A Study of Electronic Health Records in French</font> (Hiebel et al., 2023). The paper highlights the generation of data automatically, but touches on the difficulty of evaluating this data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Propose an evaluation method for your synthetic text generation method. How would you evaluate whether your approach is successfull? (10 points)</font>\n",
    "\n",
    "There are two modes of evaluation that we have to consider. Intrinsic and Extrinsic. Whereas intrinsic evaluation is evaluated against some predetermined ground truth and extrensic evaluation is evaluated based on its impact on the performance of other systems.\n",
    "\n",
    "* <font color=\"red\">Intrinsic Evaluation</font>\n",
    "In terms of intrinsic evaluation, it is a bit more difficult since this requires a ground truth to compare against and we do not have a lot of truth data on rare diseases. In the paper above, they suggest evaluating using ngram overlap and named entity recognition. These two methods already offer a great way to evaluate the data; However, I would also propose that a researcher to look into utilizing Ontology coverage whereas the breadth of the HPO ensures the presence and frequency of the new terms in the corpora are represented adequately. One may also consider use-case simulation whereas you simulate utilizing the new findings to make clinical decisions which are similar in nature to those made before.\n",
    "\n",
    "\n",
    "* <font color=\"red\">Extrinsic Evaluation</font>\n",
    "The best method for extrinsic evaluation is by humans, in this case being domain experts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2b (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of 2023, transfer learning using large language models such as GPT-4, etc.. is the current best practise for a large number of tasks. There has been speculation in the popular press that these models will function as artificial general intelligences, making domain specific models redundant.\n",
    "\n",
    "* Making references to at least one paper discussed in class, describe performance results indicating that this is not the case. (10 points)\n",
    "\n",
    "* Describe at least 2 benefits of using a domain specific language model that has been fine-tuned on a task,  relative to a model like GPT-4 (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Making references to at least one paper discussed in class, describe performance results indicating that this is not the case. (10 points)</font>\n",
    "\n",
    "The paper that discusses this topic is `BioGPT: generative pre-trained transformer for biomedical text generation and mining` (Luo et al.) and `Modeling Parallel Programs using Large Language Models` (Nichols et al.) whereas both specialized models outperfomed the more general LLM. These two papers discuss creating domain specific models, BioGPT in terms of medicine and Modeling Parallel Programs in terms of MPI/HPC coding LLM's. We can see that if a general LLM is taken and fine tuned in a specific domain then it becomes better at that domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Describe at least 2 benefits of using a domain specific language model that has been fine-tuned on a task, relative to a model like GPT-4 (10 points)</font>\n",
    "There are a couple benefits to using domain specific LLMs. Some of the two top ones are:\n",
    "1. Increased knowledge in the specific domain. Having an LLM with more knowledge in your domain makes the LLM more useful to you. For example programming LLM's tend to outperform general LLM's in programming.\n",
    "2. Increased lower-shot accuracy. As you add more domain specific knowledge, it will lead to lower-shot accuracy rating increasing thus increasing overall accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp2023v2]",
   "language": "python",
   "name": "conda-env-nlp2023v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
