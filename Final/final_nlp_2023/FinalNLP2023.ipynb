{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exam CS/INFO 662/762 Fall 2023\n",
    "CS/INFO 762: 100 points ; CS/INFO 662  90 points\n",
    "\n",
    "### <font color='red'>Due Dec 9th, 11:59am</font> - Submission via Canvas (.ipynb file)\n",
    "\n",
    "## STUDENT NAME: <font color='red'>MICHAEL GATHARA</font>\n",
    "\n",
    "\n",
    "* Question 1a: Medical Mention Normalization with SAPBERT (PhD Students must include one graph feature) - 35/25 points\n",
    "* Question 1b: Compute Recall - 15 points\n",
    "* Question 1c: Random Forest: Feature Importance - 10 points\n",
    "* Question 2: Language Model Questions (Long Written Answer) - 40 points\n",
    "\n",
    "<font color='red'>As always WORK ON YOUR OWN for this final exam. Like last year, the final exam will be run through plagarism detection software. You may email me for clarification, but don't post on Stack Overflow, Quota, Reddit, etc..  You MAY use ChatGPT for ANY question, but the usual rules for citation and prompt inclusion in your answer apply.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: obonet in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (1.0.0)\n",
      "Requirement already satisfied: networkx in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from obonet) (2.8.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: py-rouge in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: node2vec in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (0.4.6)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.19.5 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from node2vec) (1.22.3)\n",
      "Requirement already satisfied: joblib<2.0.0,>=1.1.0 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from node2vec) (1.2.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.55.1 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from node2vec) (4.65.0)\n",
      "Requirement already satisfied: gensim<5.0.0,>=4.1.2 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from node2vec) (4.3.2)\n",
      "Requirement already satisfied: networkx<3.0,>=2.5 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from node2vec) (2.8.8)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (6.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: rouge-score in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from rouge-score) (1.22.3)\n",
      "Requirement already satisfied: absl-py in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from rouge-score) (2.0.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: tqdm in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from nltk->rouge-score) (4.65.0)\n",
      "Requirement already satisfied: joblib in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from nltk->rouge-score) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from nltk->rouge-score) (2023.3.23)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# If needed\n",
    "#!pip uninstall --yes flair\n",
    "%pip install obonet\n",
    "%pip install py-rouge\n",
    "%pip install node2vec\n",
    "%pip install rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disease Ontology is currently size:11432 with 11462 edges\n",
      "Human Phenotype Ontology is currently size:17664 with 21975 edges\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import networkx\n",
    "import obonet\n",
    "import os\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "import heapq\n",
    "import pandas as pd\n",
    "#import scispacy\n",
    "import spacy\n",
    "import numpy as np\n",
    "import torch\n",
    "from io import StringIO\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\")  \n",
    "model = AutoModel.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\").cuda()\n",
    "\n",
    "do_url = 'https://raw.githubusercontent.com/DiseaseOntology/HumanDiseaseOntology/main/src/ontology/HumanDO.obo'\n",
    "hpo_url = 'http://purl.obolibrary.org/obo/hp.obo'\n",
    "do = obonet.read_obo(do_url)\n",
    "hpo = obonet.read_obo(hpo_url)\n",
    "print('Disease Ontology is currently size:'+str(len(do))+\" with \"+str(do.number_of_edges())+' edges')\n",
    "print('Human Phenotype Ontology is currently size:'+str(len(hpo))+\" with \"+str(hpo.number_of_edges())+' edges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Concept Normalization\n",
    "This question requires you to write use the SAPBERT embeddings you are familiar with from assignment #2 to generate candidate concepts for each input medical mentions for a merged overlapping knowledge graph of both the Disease Ontology (DO) and Human Phenotyper Ontology (HPO). \n",
    "\n",
    "### Set Up Knowledge Graph and Corpus Preparation \n",
    "This code is provided to you and creates:\n",
    "* The merged knowledge graph (kgs) from the both Disease Ontology (DO) and the Human Phenotype Ontology (HPO) as a dataframe. You also have access to the original graphs in obo format to get graph features, for example you can use node2vec.\n",
    "* The input corpus and medical mentions (labelled data) as a dataframe, \"mention_mapping\". It is built from the input corpus and you can assume that NER has already been done to identify the mentions to map. They are in the \"mention\" column and the correct concept (CUI) it should be mapped to is in the \"CUI\" column. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO Vocabulary: hpokg\n",
      "            HPOID       CUI  DOID                          HPO:Name DO:Name\n",
      "0      HP:0000001  C0444868  None                               All    None\n",
      "1      HP:0000002  C4025901  None        Abnormality of body height    None\n",
      "2      HP:0000003  C3714581  None      Multicystic kidney dysplasia    None\n",
      "3      HP:0000005  C1708511  None               Mode of inheritance    None\n",
      "4      HP:0000006  C0443147  None    Autosomal dominant inheritance    None\n",
      "...           ...       ...   ...                               ...     ...\n",
      "17659  HP:5201010      None  None  Microform cleft of the upper lip    None\n",
      "17660  HP:5201011      None  None      Complete bilateral cleft lip    None\n",
      "17661  HP:5201012      None  None    Incomplete bilateral cleft lip    None\n",
      "17662  HP:5201013      None  None     Microform bilateral cleft lip    None\n",
      "17663  HP:5201014      None  None    Asymmetric bilateral cleft lip    None\n",
      "\n",
      "[17664 rows x 5 columns]\n",
      "HPO and DO Joint Vocabulary:kgs\n",
      "            HPOID       CUI          DOID                  HPO:Name  \\\n",
      "9      HP:0000011  C0005697    DOID:12143        Neurogenic bladder   \n",
      "13     HP:0000015  C0156273    DOID:11353      Bladder diverticulum   \n",
      "20     HP:0000023  C0019294  DOID:0060320           Inguinal hernia   \n",
      "21     HP:0000024  C0033581    DOID:14654               Prostatitis   \n",
      "24     HP:0000027  C0004509    DOID:14227               Azoospermia   \n",
      "...           ...       ...           ...                       ...   \n",
      "16448  HP:0200018  C0155015    DOID:13910               Protanomaly   \n",
      "16451  HP:0200022  C0205770     DOID:2626  Choroid plexus papilloma   \n",
      "16452  HP:0200023  C0033117     DOID:9286                  Priapism   \n",
      "16480  HP:0200058  C0018923  DOID:0001816              Angiosarcoma   \n",
      "16532  HP:0200151  C1136033     DOID:3663    Cutaneous mastocytosis   \n",
      "\n",
      "                        DO:Name  \n",
      "9            neurogenic bladder  \n",
      "13         bladder diverticulum  \n",
      "20              inguinal hernia  \n",
      "21                  prostatitis  \n",
      "24                  azoospermia  \n",
      "...                         ...  \n",
      "16448       red color blindness  \n",
      "16451  choroid plexus papilloma  \n",
      "16452                  priapism  \n",
      "16480              angiosarcoma  \n",
      "16532    cutaneous mastocytosis  \n",
      "\n",
      "[970 rows x 5 columns]\n",
      "Input Corpus Mentions:mention_mapping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CUI</th>\n",
       "      <th>start1</th>\n",
       "      <th>stop1</th>\n",
       "      <th>start2</th>\n",
       "      <th>stop2</th>\n",
       "      <th>start3</th>\n",
       "      <th>stop3</th>\n",
       "      <th>mention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N000</td>\n",
       "      <td>C0011854</td>\n",
       "      <td>248</td>\n",
       "      <td>283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>insulin dependent diabetes mellitus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N001</td>\n",
       "      <td>C4303631</td>\n",
       "      <td>298</td>\n",
       "      <td>327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a right above-knee amputation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N003</td>\n",
       "      <td>C0085671</td>\n",
       "      <td>537</td>\n",
       "      <td>553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dressing changes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N004</td>\n",
       "      <td>C0011079</td>\n",
       "      <td>558</td>\n",
       "      <td>569</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>debridement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N005</td>\n",
       "      <td>C0003232</td>\n",
       "      <td>611</td>\n",
       "      <td>622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>antibiotics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6679</th>\n",
       "      <td>N139</td>\n",
       "      <td>C0442519</td>\n",
       "      <td>4695</td>\n",
       "      <td>4699</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6680</th>\n",
       "      <td>N140</td>\n",
       "      <td>C0699203</td>\n",
       "      <td>4731</td>\n",
       "      <td>4737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>motrin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6681</th>\n",
       "      <td>N141</td>\n",
       "      <td>C0593507</td>\n",
       "      <td>4740</td>\n",
       "      <td>4745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>advil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6682</th>\n",
       "      <td>N142</td>\n",
       "      <td>C0332575</td>\n",
       "      <td>4863</td>\n",
       "      <td>4870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>redness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6683</th>\n",
       "      <td>N143</td>\n",
       "      <td>C0205217</td>\n",
       "      <td>4853</td>\n",
       "      <td>4862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>increased</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6684 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID       CUI  start1  stop1  start2  stop2  start3  stop3  \\\n",
       "0     N000  C0011854     248    283     NaN    NaN     NaN    NaN   \n",
       "1     N001  C4303631     298    327     NaN    NaN     NaN    NaN   \n",
       "2     N003  C0085671     537    553     NaN    NaN     NaN    NaN   \n",
       "3     N004  C0011079     558    569     NaN    NaN     NaN    NaN   \n",
       "4     N005  C0003232     611    622     NaN    NaN     NaN    NaN   \n",
       "...    ...       ...     ...    ...     ...    ...     ...    ...   \n",
       "6679  N139  C0442519    4695   4699     NaN    NaN     NaN    NaN   \n",
       "6680  N140  C0699203    4731   4737     NaN    NaN     NaN    NaN   \n",
       "6681  N141  C0593507    4740   4745     NaN    NaN     NaN    NaN   \n",
       "6682  N142  C0332575    4863   4870     NaN    NaN     NaN    NaN   \n",
       "6683  N143  C0205217    4853   4862     NaN    NaN     NaN    NaN   \n",
       "\n",
       "                                  mention  \n",
       "0     insulin dependent diabetes mellitus  \n",
       "1           a right above-knee amputation  \n",
       "2                        dressing changes  \n",
       "3                             debridement  \n",
       "4                             antibiotics  \n",
       "...                                   ...  \n",
       "6679                                 home  \n",
       "6680                               motrin  \n",
       "6681                                advil  \n",
       "6682                              redness  \n",
       "6683                            increased  \n",
       "\n",
       "[6684 rows x 9 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createIndex(graph,prefix):\n",
    "    id2cui = {}\n",
    "    cui2id = {}\n",
    "    id_to_xref = {id_: data.get('xref') for id_, data in graph.nodes(data=True)}\n",
    "    for graph_id,xrefs in id_to_xref.items():\n",
    "        if(xrefs is None):\n",
    "            cui = None\n",
    "        else:\n",
    "            cui = next((x for x in xrefs if x.startswith(prefix)),None)\n",
    "            if(cui is not None):\n",
    "                cui = cui.replace(prefix,'')\n",
    "        id2cui[graph_id]=cui\n",
    "        if(cui is not None):\n",
    "            cui2id[cui]=graph_id\n",
    "    return(id2cui,cui2id)\n",
    "\n",
    "\n",
    "def convertCui2Doid(cui):\n",
    "    if cui in cui2do:\n",
    "        return cui2do[cui]\n",
    "    return None\n",
    "\n",
    "def hpoId2Name(oboid):\n",
    "    return hpoid_to_name[oboid]\n",
    "\n",
    "def doId2Name(oboid):\n",
    "    if(oboid is None):\n",
    "        return None\n",
    "    if (doid_to_name[oboid]):\n",
    "        return doid_to_name[oboid]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_mentions(filename,bardoc):\n",
    "    all_mentions = []\n",
    "    with open(filename, 'r') as file: \n",
    "        textdoc = file.read()\n",
    "        for line in bardoc.splitlines():\n",
    "            #print(line)\n",
    "            start = int(line.split(\"||\")[2])\n",
    "            stop = int(line.split(\"||\")[3])\n",
    "            mention = textdoc[start:stop]\n",
    "            if(not line.endswith(\"||||||\")):\n",
    "                start = int(line.split(\"||\")[4])\n",
    "                stop = int(line.split(\"||\")[5])\n",
    "                extramention = textdoc[start:stop]\n",
    "                mention = mention+' '+extramention\n",
    "                if(not line.endswith(\"||||\")):\n",
    "                    start = int(line.split(\"||\")[6])\n",
    "                    stop = int(line.split(\"||\")[7])\n",
    "                    extramention = textdoc[start:stop]\n",
    "                    mention = mention+' '+extramention\n",
    "            #print(mention)\n",
    "            all_mentions.append(mention)\n",
    "    return all_mentions\n",
    "\n",
    "def read_files(directory):\n",
    "    all_data = []\n",
    "    for file in os.listdir(directory):\n",
    "        #print(file)\n",
    "        if file.endswith(\".norm\"):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            with open(file_path, 'r') as file:\n",
    "                csv_string = file.read()\n",
    "            #normed = [line+\"||||\" for line in csv_string.splitlines() if line.count('|')==6]\n",
    "            normed = [line if line.count('|') == 14 else (line+\"||||\" if line.count('|') == 10 else line+\"||||||||\") for line in csv_string.splitlines()]\n",
    "            clean = '\\n'.join(normed)\n",
    "            note_file = (str(file.name).replace(\"train_norm\",\"train_note\").replace(\"norm\",\"txt\"))\n",
    "            mentions = get_mentions(note_file,clean)\n",
    "            df = pd.read_csv(StringIO(clean),engine='python',names=['ID', 'CUI', 'start1', 'stop1','start2','stop2','start3','stop3'],sep=\"\\|\\|\")\n",
    "            df['mention']=mentions\n",
    "        all_data.append(df)\n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "\n",
    "hpo2cui,cui2hpo = createIndex(hpo,'UMLS:')\n",
    "do2cui,cui2do = createIndex(do,'UMLS_CUI:')\n",
    "# id_to_cui = {**hpo2cui, **do2cui}\n",
    "id_to_cui = {k: v for k, v in id_to_cui.items() if v is not None}\n",
    "\n",
    "hpoid_to_name = {id_: data.get('name') for id_, data in hpo.nodes(data=True)}\n",
    "doid_to_name = {id_: data.get('name') for id_, data in do.nodes(data=True)}\n",
    "\n",
    "df = pd.DataFrame(list(hpo2cui.items()))\n",
    "df.columns=['HPOID','CUI']\n",
    "df['DOID'] = df['CUI'].apply(convertCui2Doid)\n",
    "df['HPO:Name'] = df['HPOID'].apply(hpoId2Name)\n",
    "df['DO:Name'] = df['DOID'].apply(doId2Name)\n",
    "hpokg = df.copy()\n",
    "print(\"HPO Vocabulary: hpokg\")\n",
    "print(hpokg)\n",
    "kgs = df.mask(df.eq('None')).dropna()\n",
    "\n",
    "# Graph properties that may be useful\n",
    "id_to_isa = {id_: data.get('is_a') for id_, data in hpo.nodes(data=True)}\n",
    "id_to_xref = {id_: data.get('xref') for id_, data in do.nodes(data=True)}\n",
    "result = next(iter(id_to_xref.values()))   \n",
    "ontology_map = {\n",
    "    \n",
    "}\n",
    "for doid, name in doid_to_name.items():\n",
    "    if name is not None:\n",
    "        ontology_map[name] = doid\n",
    "\n",
    "print(\"HPO and DO Joint Vocabulary:kgs\")\n",
    "print(kgs)\n",
    "mention_mapping = read_files(\"train/train_norm/\")\n",
    "print(\"Input Corpus Mentions:mention_mapping\")\n",
    "mention_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1a: Generation of Candidate Concepts and their Features (35 points PhD/ 25 points MS)\n",
    "\n",
    "\n",
    "#### Write code to find the best N candidate concepts for the mention using SAPBERT in the small (for final exam performance purposes) merged kgs vocabularuy.\n",
    "\n",
    "The signature of the function should look something like this:\n",
    "``` \n",
    "def getCandidates(mention_embeddings, vocabulary_embeddings, max_candidates):\n",
    "```\n",
    "* mention_embeddings would be SAPBERT embeddings of the mentions\n",
    "* vocabulary_embeddings would be SAPBERT embeddings of the kgs vocabulary. You generate them using just DO concept text, just HPO concept text or perform a function to aggregate them.\n",
    "* max_candidates (max candidates to return from kgs)\n",
    "\n",
    "This function returns a list of the best N matches between the mention and the target merged knowledge graph based on feature similarity between the input node and the target node. Each match in the list is a tuple can contain any elements you need, but it should at least contain\n",
    " * a reference to the target concept, ie) row index|vocabulary_id\n",
    " * score (optional) or anything else you think you need\n",
    " \n",
    " \n",
    "#### Write code to get a set of features for each candidate concepts that can be used for ranking the top N concepts to pick the most correct concept\n",
    "The getFeatures function should generate features for an input mention text and one possible candidate mapping.\n",
    "```\n",
    "def getFeatures(mention_text, candidate_tuple_from_getCandidates)\n",
    "```\n",
    "These features will be used in Part 1b) to generate training data for a machine learning ranking algorithm.\n",
    "\n",
    "Masters student need at least 2 features in their getFeatures code, some examples of lexical features include:\n",
    "* counts of matching words or characters\n",
    "* longest common subsequence (RougeL)\n",
    "* ngram overlap, etc...\n",
    "\n",
    "PhD Students will need an additional graph-based feature using relations in the ontology or ontology node vector representations such as node2vec. For example, one relevant feature may be checking the similarity of the input node to the parent node of the target. They can also be generated per random-walks like node2vec.\n",
    "\n",
    "\n",
    "Hints:\n",
    " * stop words, stemming, lemmatizationm, headword matching are nice but not required for this tiny (mostly matching) gold data set\n",
    " * my advice is to do the minimal amount of work and come back later if you want to add more features\n",
    " * you may use ANY additional libraries as need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1b: Compute Recall@3 and Generate Data for ML Algorithm in Part 1c (15 points)\n",
    " * Use your function in Part 1a) to generate 3 candidates for every mention and compute recall at n=3 candidates (For each mention, what is the fraction of times that the getCandidates returned the correct concept (CUI)?). Your re-ranking algorithm will not be able to do better than this. (5 points)\n",
    " * Many mentions represent concepts not included in our small merged kgs. Despite this, your recall performance may still not match your expectations using just SAPBERT embeddings. Explain why this might be (5 points).  \n",
    " * Create a labelled candidate ranking data set (5 points). For each mention, there will be 3 examples of which only 1 will have the correct CUI. Each example will have features (X) from part 1a and a label (Y). The label will be 1 if the features are sourced from the correct CUI and 0 if not. Use your getFeatures function to populate X. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49 µs, sys: 0 ns, total: 49 µs\n",
      "Wall time: 53.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "\n",
    "# Candidate Generation Code\n",
    "# def getCandidates(mention_embeddings, vocabulary_embeddings, max_candidates):\n",
    "#     \"\"\"\n",
    "#     * mention_embeddings would be SAPBERT embeddings of the mentions\n",
    "#     * vocabulary_embeddings would be SAPBERT embeddings of the kgs vocabulary. You generate them using just DO concept text, just HPO concept text or perform a function to aggregate them.\n",
    "#     * max_candidates (max candidates to return from kgs)\n",
    "#     \"\"\"\n",
    "#     # Initialize an empty list to store the candidate concepts\n",
    "#     candidates = []\n",
    "\n",
    "#     # Compute cosine similarity between each mention embedding and all vocabulary embeddings\n",
    "#     # This will result in a matrix of similarity scores\n",
    "#     similarity_scores = cosine_similarity(mention_embeddings, vocabulary_embeddings)\n",
    "\n",
    "#     # Iterate over each mention\n",
    "#     for mention_index, scores in enumerate(similarity_scores):\n",
    "#         # Sort the scores in descending order and get the indices (which correspond to vocabulary IDs)\n",
    "#         ranked_vocabulary_indices = np.argsort(scores)[::-1]\n",
    "\n",
    "#         # Select the top N candidates based on max_candidates\n",
    "#         top_candidates = ranked_vocabulary_indices[:max_candidates]\n",
    "\n",
    "#         # Format and add the candidates for this mention to the list\n",
    "#         # Including the vocabulary index (or ID) and the corresponding similarity score\n",
    "#         mention_candidates = [(vocab_index, scores[vocab_index]) for vocab_index in top_candidates]\n",
    "#         candidates.append(mention_candidates)\n",
    "\n",
    "#     return candidates[0]\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# def getCandidates(mention_embedding, vocabulary_embeddings, vocabulary_terms, max_candidates):\n",
    "#     # Compute cosine similarity between mention embedding and all vocabulary embeddings\n",
    "#     similarity_scores = cosine_similarity([mention_embedding], vocabulary_embeddings)[0]\n",
    "#     print(\"Here\")\n",
    "\n",
    "#     # Get the indices of the top N scoring terms\n",
    "#     top_candidate_indices = np.argsort(similarity_scores)[::-1][:max_candidates]\n",
    "#     print(\"There\")\n",
    "\n",
    "#     # Retrieve the top N candidates with their text, index, and score\n",
    "#     candidates = [(vocabulary_terms[i], i, similarity_scores[i]) for i in top_candidate_indices]\n",
    "#     print(\"Almost out\")\n",
    "    \n",
    "#     return candidates\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# def getCandidates(mention_embedding, vocabulary_embeddings, vocabulary_terms, max_candidates):\n",
    "#     # Ensure mention_embedding is 2D\n",
    "#     if mention_embedding.ndim == 1:\n",
    "#         mention_embedding = mention_embedding.reshape(1, -1)\n",
    "#     elif mention_embedding.ndim > 2:\n",
    "#         raise ValueError(f\"mention_embedding should be 2D, but got shape {mention_embedding.shape}\")\n",
    "\n",
    "#     # Compute cosine similarity between mention embedding and all vocabulary embeddings\n",
    "#     similarity_scores = cosine_similarity(mention_embedding, vocabulary_embeddings)[0]\n",
    "\n",
    "#     # Get the indices of the top N scoring terms\n",
    "#     top_candidate_indices = np.argsort(similarity_scores)[::-1][:max_candidates]\n",
    "\n",
    "#     # Retrieve the top N candidates with their text, index, and score\n",
    "#     candidates = [(vocabulary_terms[i], i, similarity_scores[i]) for i in top_candidate_indices]\n",
    "\n",
    "#     return candidates\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# def getCandidates(mention_embedding, vocabulary_embeddings, vocabulary_ids, max_candidates, id_to_cui):\n",
    "#     if mention_embedding.ndim == 1:\n",
    "#         mention_embedding = mention_embedding.reshape(1, -1)\n",
    "#     elif mention_embedding.ndim > 2:\n",
    "#         raise ValueError(f\"mention_embedding should be 2D, but got shape {mention_embedding.shape}\")\n",
    "    \n",
    "#     # Compute cosine similarity between mention embedding and all vocabulary embeddings\n",
    "#     similarity_scores = cosine_similarity(mention_embedding, vocabulary_embeddings)[0]\n",
    "\n",
    "#     # Get the indices of the top N scoring terms\n",
    "#     top_candidate_indices = np.argsort(similarity_scores)[::-1][:max_candidates]\n",
    "\n",
    "#     # Retrieve the top N candidates with their CUIs and similarity scores\n",
    "#     candidates = [(id_to_cui[vocabulary_ids[i]], i, similarity_scores[i]) for i in top_candidate_indices if vocabulary_ids[i] in id_to_cui]\n",
    "#     print(candidates)\n",
    "#     return candidates\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# def getCandidates(mention_embedding, vocabulary_embeddings, vocabulary_terms, max_candidates, id_to_cui):\n",
    "#     # Ensure mention_embedding is 2D\n",
    "#     if mention_embedding.ndim == 1:\n",
    "#         mention_embedding = mention_embedding.reshape(1, -1)\n",
    "#     elif mention_embedding.ndim > 2:\n",
    "#         raise ValueError(f\"mention_embedding should be 2D, but got shape {mention_embedding.shape}\")\n",
    "\n",
    "#     # Compute cosine similarity between mention embedding and all vocabulary embeddings\n",
    "#     similarity_scores = cosine_similarity(mention_embedding, vocabulary_embeddings)[0]\n",
    "\n",
    "#     # Get the indices of the top N scoring terms\n",
    "#     top_candidate_indices = np.argsort(similarity_scores)[::-1][:max_candidates]\n",
    "\n",
    "#     # Retrieve the top N candidates with their CUIs and similarity scores\n",
    "#     candidates = []\n",
    "#     for i in top_candidate_indices:\n",
    "#         ontology_id = vocabulary_terms[i]  # This should be an ontology ID\n",
    "#         if ontology_id in id_to_cui:\n",
    "#             cui = id_to_cui[ontology_id]\n",
    "#             candidates.append((cui, i, similarity_scores[i]))\n",
    "#         else:\n",
    "#             print(f\"Ontology ID not found in id_to_cui: {ontology_id}\")\n",
    "#     print(candidates)\n",
    "\n",
    "#     return candidates\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "# def getCandidates(mention_embedding, vocabulary_embeddings, vocabulary_ids, max_candidates, id_to_cui):\n",
    "#     # Check the dimensions of the mention embedding and reshape if necessary\n",
    "#     if mention_embedding.ndim == 1:\n",
    "#         mention_embedding = mention_embedding.reshape(1, -1)\n",
    "#     elif mention_embedding.ndim > 2:\n",
    "#         raise ValueError(f\"mention_embedding should be 2D, but got shape {mention_embedding.shape}\")\n",
    "\n",
    "#     # Compute cosine similarity between the mention embedding and all vocabulary embeddings\n",
    "#     similarity_scores = cosine_similarity(mention_embedding, vocabulary_embeddings)[0]\n",
    "\n",
    "#     # Debug: Print a few similarity scores to check\n",
    "# #     print(f\"Sample similarity scores: {similarity_scores[:10]}\")\n",
    "\n",
    "#     # Get the indices of the top N scoring terms\n",
    "#     top_candidate_indices = np.argsort(similarity_scores)[::-1][:max_candidates]\n",
    "# #     print(f\"Top candidate indices: {type(top_candidate_indices)}\")\n",
    "\n",
    "#     # Retrieve the top N candidates with their CUIs and similarity scores\n",
    "#     candidates = []\n",
    "#     for i in top_candidate_indices:\n",
    "#         description = vocabulary_ids[i]\n",
    "#         if description in ontology_map:\n",
    "#             ontology_id = ontology_map[description]\n",
    "#             if ontology_id in id_to_cui:\n",
    "#                 cui = id_to_cui[ontology_id]\n",
    "#                 candidates.append((cui, i, similarity_scores[i]))\n",
    "    \n",
    "#     # Debug: Print candidates to check\n",
    "# #     print(\"Candidates:\", candidates)\n",
    "\n",
    "#     return candidates\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# def getCandidates(mention_embeddings, vocabulary_embeddings, max_candidates):\n",
    "#     # Compute cosine similarity between mention_embeddings and vocabulary_embeddings\n",
    "#     similarities = cosine_similarity(mention_embeddings.reshape(1, -1), vocabulary_embeddings)\n",
    "#     similarities = similarities.flatten()  # Flatten the similarity matrix\n",
    "    \n",
    "#     # Get indices of top N similar concepts\n",
    "#     top_indices = np.argsort(similarities)[::-1][:max_candidates]\n",
    "    \n",
    "#     # Create a list of candidate tuples with required information (e.g., index and score)\n",
    "#     candidate_list = [(index, similarities[index]) for index in top_indices]\n",
    "    \n",
    "#     return candidate_list\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def getCandidates(mention_embeddings, vocabulary_embeddings, max_candidates):\n",
    "    # Compute cosine similarity between mention_embeddings and vocabulary_embeddings\n",
    "    similarities = cosine_similarity(mention_embeddings.reshape(1, -1), vocabulary_embeddings)[0]\n",
    "    similarities = similarities.flatten()  # Flatten the similarity matrix\n",
    "    \n",
    "    # Get indices of top N similar concepts\n",
    "    top_indices = np.argsort(similarities)[::-1][:max_candidates]\n",
    "    \n",
    "    # Create a list of candidate tuples with required information (e.g., index and score)\n",
    "    candidate_list = [(index, similarities[index]) for index in top_indices]\n",
    "    \n",
    "    return candidate_list\n",
    "\n",
    "def getFeatures(mention_text, candidate_tuple_from_getCandidates):\n",
    "    # Unpack the candidate tuple\n",
    "    index, similarity_score = candidate_tuple_from_getCandidates\n",
    "\n",
    "    # Feature 1: Length of candidate concept (simple example)\n",
    "    length_feature = len(candidate_id)\n",
    "\n",
    "    # Feature 2: Similarity of mention text to candidate concept (using SequenceMatcher for simplicity)\n",
    "    sm = SequenceMatcher(None, mention_text, candidate_id)\n",
    "    text_similarity_feature = sm.ratio()\n",
    "\n",
    "    # Graph-based Feature (for PhD students): This is a placeholder.\n",
    "    # You'll need to replace it with an actual graph-based feature calculation.\n",
    "    # For example, it could be the similarity between the mention and the parent node of the target in an ontology.\n",
    "    graph_based_feature = np.random.random()  # Placeholder - replace with actual feature calculation\n",
    "\n",
    "    # Compile features into a vector\n",
    "    feature_vector = [length_feature, text_similarity_feature, graph_based_feature]\n",
    "\n",
    "    return feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_embedding(mention_text):\n",
    "#     # Tokenize the mention text\n",
    "#     inputs = tokenizer(mention_text, padding=True, truncation=True, return_tensors=\"pt\").to('cuda')\n",
    "    \n",
    "#     # Get the embeddings from the model\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "    \n",
    "#     # For SapBERT, the pooled output is typically at outputs.pooler_output or outputs.last_hidden_state.mean(dim=1)\n",
    "#     # You will need to confirm the correct output for your model version\n",
    "#     embeddings = outputs.pooler_output if hasattr(outputs, 'pooler_output') else outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "#     # Move embeddings to CPU and convert to numpy if necessary\n",
    "# #     embeddings = embeddings.cpu().numpy()\n",
    "#     embeddings = embeddings.reshape(1, -1)\n",
    "    \n",
    "#     return embeddings.cpu().numpy()\n",
    "def get_embeddings(text, model, tokenizer, device='cuda'):\n",
    "    # Tokenize the input text using the provided tokenizer\n",
    "    tokens = tokenizer.encode_plus(text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    \n",
    "    # Move the input tokens to the specified device (cuda/cpu)\n",
    "    tokens = {key: value.to(device) for key, value in tokens.items()}\n",
    "    \n",
    "    # Forward pass the tokens through the model to get the embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    \n",
    "    # Move the embeddings to CPU if the output is on CUDA\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().detach().cpu().numpy()\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# mention_mapping['mention_embedding'] = mention_mapping['mention'].apply(get_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'do' and 'hpo' are networkx graphs loaded from OBO files\n",
    "# do_terms = list(set(do.nodes))\n",
    "# hpo_terms = list(set(hpo.nodes))\n",
    "# # Combine the names or descriptions from both ontologies into one list\n",
    "do_terms = [do.nodes[node]['name'] for node in do.nodes]\n",
    "# hpo_terms = [hpo.nodes[node]['name'] for node in hpo.nodes]\n",
    "kgs_terms = list(set(do_terms))\n",
    "# print(f\"DO terms: {do_terms}\")\n",
    "# print(f\"HPO terms: {hpo_terms}\")\n",
    "\n",
    "# # Now we combine the terms from DO and HPO into one list\n",
    "# kgs_terms = do_terms + hpo_terms\n",
    "\n",
    "# Combine the terms from DO and HPO into one list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7097af922d0a49008a99e0b5497e9ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize a list to hold the embeddings\n",
    "kgs_embeddings_list = []\n",
    "\n",
    "# For each term in the knowledge graph, generate an embedding\n",
    "for term in tqdm(kgs_terms):\n",
    "    # Here we assume that each term is a string that SapBERT can process\n",
    "    # If the terms are not strings or need pre-processing, you'll need to handle that\n",
    "    embedding = get_embedding(term)\n",
    "    kgs_embeddings_list.append(embedding)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# for i in range(3):\n",
    "#     embedding = get_embedding(kgs_terms[i])\n",
    "#     kgs_embeddings_list.append(embedding)\n",
    "\n",
    "# Convert the list of embeddings into a NumPy array for efficient computation\n",
    "# kgs_embeddings = np.hstack(kgs_embeddings_list)\n",
    "# print(kgs_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_ids = do_terms\n",
    "kgs_embeddings = []\n",
    "for text in kg_ids:\n",
    "    kgs_embeddings.append(get_embedding(text))\n",
    "# kgs_embeddings = np.vstack(kgs_embeddings_list).reshape(-1, 768)\n",
    "# print(kgs_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405bef3027124c35b645203593aafc11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocabulary_texts = list(kgs['DO:Name'])  # Assuming 'DO:Name' contains the text data for vocabulary\n",
    "\n",
    "# Generate SAPBERT embeddings for the vocabulary texts\n",
    "vocabulary_embeddings = []\n",
    "for text in tqdm(vocabulary_texts):\n",
    "    embedding = get_embedding(text)\n",
    "    vocabulary_embeddings.append(embedding)\n",
    "\n",
    "# Convert the list of embeddings to a numpy array\n",
    "# vocabulary_embeddings = np.array(vocabulary_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a888a0a211d4debae83f7c659291ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320b3307a02b4709b0a65fdb6b053ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e657746e5c421487a3f2afa77404fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall at 3 candidates: 0.06149012567324955\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e80c234cb04dbb84ecb41ebbfba90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'candidate_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:169\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:183\u001b[0m, in \u001b[0;36mgetFeatures\u001b[0;34m(mention_text, candidate_tuple_from_getCandidates)\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'candidate_id' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute Recall for Candidate Generation Code\n",
    "X = []  # to hold the feature vectors\n",
    "Y = []  # to hold the labels (1 for correct CUI, 0 for incorrect)\n",
    "\n",
    "# Compute recall\n",
    "correct_predictions = 0\n",
    "total_mentions = len(mention_mapping)\n",
    "\n",
    "halt = total_mentions\n",
    "# halt = 100\n",
    "i = 0\n",
    "\n",
    "# print(id_to_cui)\n",
    "\n",
    "# For each mention in the mention_mapping DataFrame\n",
    "# for index, row in tqdm(mention_mapping.iterrows()):\n",
    "#     if i > halt:\n",
    "#         break\n",
    "#     mention = row['mention']\n",
    "#     correct_cui = row['CUI']\n",
    "\n",
    "#     # Convert mention to embedding using a predefined function, such as:\n",
    "#     # mention_embedding = convert_to_embedding(mention)\n",
    "#     # For simplicity, let's assume the embeddings are part of the DataFrame:\n",
    "# #     mention_embedding = row['mention_embedding']\n",
    "    \n",
    "#     # Double-check the shape of mention_embedding\n",
    "#     if mention_embedding.ndim != 2 or mention_embedding.shape[0] != 1:\n",
    "#         raise ValueError(f\"Unexpected shape for mention_embedding: {mention_embedding.shape}\")\n",
    "    \n",
    "# #     print(f\"mention_embeddings: {mention_embedding}\")\n",
    "# #     print(f\"kgs_embeddings: {kgs_embeddings}\")\n",
    "# #     print(f\"kg_ids: {kg_ids}\")\n",
    "    \n",
    "# #     candidates = getCandidates(mention_embedding, kgs_embeddings, kg_ids, 6, id_to_cui)\n",
    "#     candidates = getCandidates(mention_embedding, vocabulary_embeddings, 3)\n",
    "    \n",
    "#     # Check if the correct CUI is in the candidates\n",
    "# #     print(f\"Correct CUI: {correct_cui}\")\n",
    "# #     print(f\"Candidates: {candidates}\")\n",
    "\n",
    "#     # Check if the correct CUI is in the candidates\n",
    "# #     if any(correct_cui == cand[0] for cand in candidates):\n",
    "# #         print(\"correct\")\n",
    "# #         correct_predictions += 1\n",
    "# #     else:\n",
    "#         # Debugging: Print out the mention text when the correct CUI is not found\n",
    "# #         print(f\"Correct CUI not found for mention: {mention}\")\n",
    "#     correct_cui = mention_mapping[mention_mapping['mention'] == mention]['CUI'].values[0]\n",
    "#     top_candidates = [kgs.iloc[candidate[0]]['CUI'] for candidate in candidates]\n",
    "#     if correct_cui in top_candidates:\n",
    "#         correct_predictions += 1\n",
    "    \n",
    "# #     # Generate features and labels for the candidates\n",
    "# #     for candidate in candidates:\n",
    "# #         # Extract features for the candidate concept\n",
    "# #         try:\n",
    "# #             features = getFeatures(mention, candidate)\n",
    "# #         except:\n",
    "# #             print(f\"This mention, candidate failed: {mention} - {candidate}\")\n",
    "        \n",
    "# #         # Check if the candidate's CUI is the correct one and assign the label\n",
    "# #         label = 1 if any(correct_cui == cand[0] for cand in candidates) else 0\n",
    "        \n",
    "# #         # Append the features and label to the X and Y lists\n",
    "# #         X.append(features)\n",
    "# #         Y.append(label)\n",
    "#     i += 1\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the recall\n",
    "# recall_at_3 = correct_predictions / total_mentions\n",
    "# print(f\"The correct preds: {correct_predictions} and total mentions: {total_mentions}\")\n",
    "# print(f'Recall at n=3: {recall_at_3}')\n",
    "\n",
    "# Convert X and Y to the appropriate format for training, such as numpy arrays\n",
    "# This is required for the machine learning ranking algorithm in the next step\n",
    "# X = np.array(X)\n",
    "# Y = np.array(Y)\n",
    "\n",
    "\n",
    "# Create X (data), Y (label) for ranking algorithm.\n",
    "def get_embeddings(text, model, tokenizer, device='cuda'):\n",
    "    # Tokenize the input text using the provided tokenizer\n",
    "    tokens = tokenizer.encode_plus(text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    \n",
    "    # Move the input tokens to the specified device (cuda/cpu)\n",
    "    tokens = {key: value.to(device) for key, value in tokens.items()}\n",
    "    \n",
    "    # Forward pass the tokens through the model to get the embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    \n",
    "    # Move the embeddings to CPU if the output is on CUDA\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().detach().cpu().numpy()\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Create a list to store vocabulary texts\n",
    "vocabulary_texts = list(kgs['DO:Name'])  # Assuming 'DO:Name' contains the text data for vocabulary\n",
    "\n",
    "# Generate SAPBERT embeddings for the vocabulary texts\n",
    "vocabulary_embeddings = []\n",
    "for text in tqdm(vocabulary_texts):\n",
    "    embedding = get_embeddings(text, model, tokenizer)\n",
    "    vocabulary_embeddings.append(embedding)\n",
    "    \n",
    "mention_embeddings = []\n",
    "for text in tqdm(mention_mapping['mention']):\n",
    "    embedding = get_embeddings(text, model, tokenizer)\n",
    "    mention_embeddings.append(embedding)\n",
    "\n",
    "# Convert the list of embeddings to a numpy array\n",
    "vocabulary_embeddings = np.array(vocabulary_embeddings)\n",
    "# mention_embeddings = np.array(mention_embeddings)\n",
    "# vocabulary_embeddings = vocabulary_embeddings\n",
    "\n",
    "\n",
    "# Compute Recall for Candidate Generation Code\n",
    "\n",
    "# Initialize variables to track recall calculation\n",
    "total_mentions = len(mention_mapping)\n",
    "correct_candidates = 0\n",
    "\n",
    "# Loop through each mention and compute recall at n=3 candidates\n",
    "for mention in tqdm(mention_mapping['mention']):\n",
    "    mention_embedding = get_embeddings(mention,model,tokenizer)\n",
    "    \n",
    "    # Get candidates for the mention\n",
    "    candidates = getCandidates(mention_embedding, vocabulary_embeddings, 3)\n",
    "    \n",
    "    # Retrieve correct CUI for the mention\n",
    "    correct_cui = mention_mapping[mention_mapping['mention'] == mention]['CUI'].values[0]\n",
    "    \n",
    "    # Check if correct CUI is in the top 3 candidates\n",
    "    top_candidates = [kgs.iloc[candidate[0]]['CUI'] for candidate in candidates]\n",
    "    if correct_cui in top_candidates:\n",
    "        correct_candidates += 1\n",
    "\n",
    "# Compute recall at n=3 candidates\n",
    "recall_at_3 = correct_candidates / total_mentions\n",
    "print(f\"Recall at 3 candidates: {recall_at_3}\")\n",
    "\n",
    "\n",
    "# # Create X (data), Y (label) for ranking algorithm.\n",
    "\n",
    "# Initialize lists to store dataset samples\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Loop through mentions to generate labeled dataset samples\n",
    "for mention in tqdm(mention_mapping['mention']):\n",
    "    mention_text = mention_mapping[mention_mapping['mention'] == mention]['mention'].values[0]\n",
    "    mention_embedding = get_embeddings(mention, model, tokenizer)\n",
    "    \n",
    "    # Get candidates for the mention\n",
    "    candidates = getCandidates(mention_embedding, vocabulary_embeddings, 3)\n",
    "    \n",
    "    # Retrieve correct CUI for the mention\n",
    "    correct_cui = mention_mapping[mention_mapping['mention'] == mention]['CUI'].values[0]\n",
    "    \n",
    "    # Create labeled samples for each candidate\n",
    "    for candidate in candidates:\n",
    "        candidate_index, _ = candidate\n",
    "        candidate_cui = kgs.iloc[candidate_index]['CUI']\n",
    "\n",
    "        # Generate features for the candidate\n",
    "        candidate_features = getFeatures(mention_text, candidate)\n",
    "\n",
    "        # Assign label based on correct CUI match\n",
    "        label = 1 if candidate_cui == correct_cui else 0\n",
    "\n",
    "        # Append features and label to the dataset\n",
    "        X.append(candidate_features)\n",
    "        Y.append(label)\n",
    "\n",
    "# Convert X and Y lists to pandas DataFrame\n",
    "dataset = pd.DataFrame(X)\n",
    "dataset['Label'] = Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1c: Random Forest Candidate Ranker and Feature Analysis (10 points)\n",
    "\n",
    " * Split your data into training and testing data and then train scikit-learn's RandomForestClassifier to predict if a candidate node is the correct match. Output a classification report with accuracy.\n",
    " \n",
    " * Use scikit-learn's RandomForestClassifier to compute the relative importance of your features for this algorithm and graph them. Give your features reasonable names so they look nice on a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0. 0. 0. ... 0. 0. 0.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Step 2: Training the model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m clf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Step 3: Making predictions and evaluating the model\u001b[39;00m\n\u001b[1;32m     15\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m/data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:345\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 345\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m/data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages/sklearn/utils/validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[0;32m-> 1106\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages/sklearn/utils/validation.py:902\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    903\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    905\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    906\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    907\u001b[0m         )\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    912\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    913\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0. 0. 0. ... 0. 0. 0.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Step 1: Splitting the dataset\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Training the model\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# Step 3: Making predictions and evaluating the model\n",
    "Y_pred = clf.predict(X_test)\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(Y_test, Y_pred))\n",
    "\n",
    "# Step 4: Analyzing feature importance\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "# Assuming you have named your features\n",
    "feature_names = [\"Length of Candidate Concept\", \"Text Similarity\", \"Graph-Based Feature\"]\n",
    "\n",
    "# Step 5: Graphing feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(feature_names, feature_importances)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance in RandomForest Classifier')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2a (20 points)\n",
    "\n",
    "One of the issues with medical normalization is that training data is sparse, some disease are over-represented, whereas some rare disease have a dictionary entry but few examples in clinical text. Making at least one reference to a paper discussed in class:\n",
    "\n",
    "\n",
    "* Describe how you could use a LLM (like GPT-4) to generate a synthetic corpus for concept normalization to an ontology like the Human Phenotype Ontology described here? Assume you would like to generate synthetic data for concepts not included in typical training data. (10 points)\n",
    "\n",
    "\n",
    "* Propose an evaluation method for your synthetic text generation method. How would you evaluate whether your approach is successfull? (10 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Describe how you could use a LLM (like GPT-4) to generate a synthetic corpus for concept normalization to an ontology like the Human Phenotype Ontology described here? Assume you would like to generate synthetic data for concepts not included in typical training data. (10 points)</font>\n",
    "<br>\n",
    "<br>\n",
    "The best thing about GPT4 is that it excels in text generation. This makes it perfect for generating synthetic corpora. One of the biggest problems is that it needs a couple examples of data to copy as the accuracy increases with more examples, often called x-shot prompting. As the data is already rare, it may be hard to find examples unless a doctor/specialist feeds it data. This is well highlighted in the paper <font color=\"red\">Can Synthetic Text Help Clinical Named Entity Recognition? A Study of Electronic Health Records in French</font> (Hiebel et al., 2023). The paper highlights the generation of data automatically, but touches on the difficulty of evaluating this data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Propose an evaluation method for your synthetic text generation method. How would you evaluate whether your approach is successfull? (10 points)</font>\n",
    "\n",
    "There are two modes of evaluation that we have to consider. Intrinsic and Extrinsic. Whereas intrinsic evaluation is evaluated against some predetermined ground truth and extrensic evaluation is evaluated based on its impact on the performance of other systems.\n",
    "\n",
    "* <font color=\"red\">Intrinsic Evaluation</font>\n",
    "In terms of intrinsic evaluation, it is a bit more difficult since this requires a ground truth to compare against and we do not have a lot of truth data on rare diseases. In the paper above, they suggest evaluating using ngram overlap and named entity recognition. These two methods already offer a great way to evaluate the data; However, I would also propose that a researcher to look into utilizing Ontology coverage whereas the breadth of the HPO ensures the presence and frequency of the new terms in the corpora are represented adequately. One may also consider use-case simulation whereas you simulate utilizing the new findings to make clinical decisions which are similar in nature to those made before.\n",
    "\n",
    "\n",
    "* <font color=\"red\">Extrinsic Evaluation</font>\n",
    "The best method for extrinsic evaluation is by humans, in this case being domain experts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2b (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of 2023, transfer learning using large language models such as GPT-4, etc.. is the current best practise for a large number of tasks. There has been speculation in the popular press that these models will function as artificial general intelligences, making domain specific models redundant.\n",
    "\n",
    "* Making references to at least one paper discussed in class, describe performance results indicating that this is not the case. (10 points)\n",
    "\n",
    "* Describe at least 2 benefits of using a domain specific language model that has been fine-tuned on a task,  relative to a model like GPT-4 (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Making references to at least one paper discussed in class, describe performance results indicating that this is not the case. (10 points)</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Describe at least 2 benefits of using a domain specific language model that has been fine-tuned on a task, relative to a model like GPT-4 (10 points)</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp2023v2]",
   "language": "python",
   "name": "conda-env-nlp2023v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
