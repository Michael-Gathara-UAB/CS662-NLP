{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exam CS/INFO 662/762 Fall 2023\n",
    "CS/INFO 762: 100 points ; CS/INFO 662  90 points\n",
    "\n",
    "### <font color='red'>Due Dec 9th, 11:59am</font> - Submission via Canvas (.ipynb file)\n",
    "\n",
    "## STUDENT NAME: <font color='red'>MICHAEL GATHARA</font>\n",
    "\n",
    "\n",
    "* Question 1a: Medical Mention Normalization with SAPBERT (PhD Students must include one graph feature) - 35/25 points\n",
    "* Question 1b: Compute Recall - 15 points\n",
    "* Question 1c: Random Forest: Feature Importance - 10 points\n",
    "* Question 2: Language Model Questions (Long Written Answer) - 40 points\n",
    "\n",
    "<font color='red'>As always WORK ON YOUR OWN for this final exam. Like last year, the final exam will be run through plagarism detection software. You may email me for clarification, but don't post on Stack Overflow, Quota, Reddit, etc..  You MAY use ChatGPT for ANY question, but the usual rules for citation and prompt inclusion in your answer apply.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting obonet\n",
      "  Downloading obonet-1.0.0-py3-none-any.whl (9.2 kB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Installing collected packages: networkx, obonet\n",
      "Successfully installed networkx-3.2.1 obonet-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting py-rouge\n",
      "  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py-rouge\n",
      "Successfully installed py-rouge-1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting node2vec\n",
      "  Downloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.55.1 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from node2vec) (4.65.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.19.5 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from node2vec) (1.22.3)\n",
      "Collecting networkx<3.0,>=2.5\n",
      "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gensim<5.0.0,>=4.1.2 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from node2vec) (4.3.2)\n",
      "Requirement already satisfied: joblib<2.0.0,>=1.1.0 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from node2vec) (1.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (6.3.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (1.10.1)\n",
      "Installing collected packages: networkx, node2vec\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.2.1\n",
      "    Uninstalling networkx-3.2.1:\n",
      "      Successfully uninstalled networkx-3.2.1\n",
      "Successfully installed networkx-2.8.8 node2vec-0.4.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from rouge-score) (2.0.0)\n",
      "Requirement already satisfied: nltk in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from rouge-score) (1.22.3)\n",
      "Requirement already satisfied: six>=1.14.0 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: joblib in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from nltk->rouge-score) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from nltk->rouge-score) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from nltk->rouge-score) (2023.3.23)\n",
      "Requirement already satisfied: click in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from nltk->rouge-score) (8.1.7)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=2376b2f15033117f5d6a221c885b824f1ce4b4dcd15883a8619a5a4c031f3463\n",
      "  Stored in directory: /data/user/home/mikegtr/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# If needed\n",
    "#!pip uninstall --yes flair\n",
    "%pip install obonet\n",
    "%pip install py-rouge\n",
    "%pip install node2vec\n",
    "%pip install rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disease Ontology is currently size:11432 with 11462 edges\n",
      "Human Phenotype Ontology is currently size:17664 with 21975 edges\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import networkx\n",
    "import obonet\n",
    "import os\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "import heapq\n",
    "import pandas as pd\n",
    "#import scispacy\n",
    "import spacy\n",
    "import numpy as np\n",
    "import torch\n",
    "from io import StringIO\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\")  \n",
    "model = AutoModel.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\").cuda()\n",
    "\n",
    "do_url = 'https://raw.githubusercontent.com/DiseaseOntology/HumanDiseaseOntology/main/src/ontology/HumanDO.obo'\n",
    "hpo_url = 'http://purl.obolibrary.org/obo/hp.obo'\n",
    "do = obonet.read_obo(do_url)\n",
    "hpo = obonet.read_obo(hpo_url)\n",
    "print('Disease Ontology is currently size:'+str(len(do))+\" with \"+str(do.number_of_edges())+' edges')\n",
    "print('Human Phenotype Ontology is currently size:'+str(len(hpo))+\" with \"+str(hpo.number_of_edges())+' edges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Concept Normalization\n",
    "This question requires you to write use the SAPBERT embeddings you are familiar with from assignment #2 to generate candidate concepts for each input medical mentions for a merged overlapping knowledge graph of both the Disease Ontology (DO) and Human Phenotyper Ontology (HPO). \n",
    "\n",
    "### Set Up Knowledge Graph and Corpus Preparation \n",
    "This code is provided to you and creates:\n",
    "* The merged knowledge graph (kgs) from the both Disease Ontology (DO) and the Human Phenotype Ontology (HPO) as a dataframe. You also have access to the original graphs in obo format to get graph features, for example you can use node2vec.\n",
    "* The input corpus and medical mentions (labelled data) as a dataframe, \"mention_mapping\". It is built from the input corpus and you can assume that NER has already been done to identify the mentions to map. They are in the \"mention\" column and the correct concept (CUI) it should be mapped to is in the \"CUI\" column. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO Vocabulary: hpokg\n",
      "            HPOID       CUI  DOID                          HPO:Name DO:Name\n",
      "0      HP:0000001  C0444868  None                               All    None\n",
      "1      HP:0000002  C4025901  None        Abnormality of body height    None\n",
      "2      HP:0000003  C3714581  None      Multicystic kidney dysplasia    None\n",
      "3      HP:0000005  C1708511  None               Mode of inheritance    None\n",
      "4      HP:0000006  C0443147  None    Autosomal dominant inheritance    None\n",
      "...           ...       ...   ...                               ...     ...\n",
      "17659  HP:5201010      None  None  Microform cleft of the upper lip    None\n",
      "17660  HP:5201011      None  None      Complete bilateral cleft lip    None\n",
      "17661  HP:5201012      None  None    Incomplete bilateral cleft lip    None\n",
      "17662  HP:5201013      None  None     Microform bilateral cleft lip    None\n",
      "17663  HP:5201014      None  None    Asymmetric bilateral cleft lip    None\n",
      "\n",
      "[17664 rows x 5 columns]\n",
      "HPO and DO Joint Vocabulary:kgs\n",
      "            HPOID       CUI          DOID                  HPO:Name  \\\n",
      "9      HP:0000011  C0005697    DOID:12143        Neurogenic bladder   \n",
      "13     HP:0000015  C0156273    DOID:11353      Bladder diverticulum   \n",
      "20     HP:0000023  C0019294  DOID:0060320           Inguinal hernia   \n",
      "21     HP:0000024  C0033581    DOID:14654               Prostatitis   \n",
      "24     HP:0000027  C0004509    DOID:14227               Azoospermia   \n",
      "...           ...       ...           ...                       ...   \n",
      "16448  HP:0200018  C0155015    DOID:13910               Protanomaly   \n",
      "16451  HP:0200022  C0205770     DOID:2626  Choroid plexus papilloma   \n",
      "16452  HP:0200023  C0033117     DOID:9286                  Priapism   \n",
      "16480  HP:0200058  C0018923  DOID:0001816              Angiosarcoma   \n",
      "16532  HP:0200151  C1136033     DOID:3663    Cutaneous mastocytosis   \n",
      "\n",
      "                        DO:Name  \n",
      "9            neurogenic bladder  \n",
      "13         bladder diverticulum  \n",
      "20              inguinal hernia  \n",
      "21                  prostatitis  \n",
      "24                  azoospermia  \n",
      "...                         ...  \n",
      "16448       red color blindness  \n",
      "16451  choroid plexus papilloma  \n",
      "16452                  priapism  \n",
      "16480              angiosarcoma  \n",
      "16532    cutaneous mastocytosis  \n",
      "\n",
      "[970 rows x 5 columns]\n",
      "Input Corpus Mentions:mention_mapping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CUI</th>\n",
       "      <th>start1</th>\n",
       "      <th>stop1</th>\n",
       "      <th>start2</th>\n",
       "      <th>stop2</th>\n",
       "      <th>start3</th>\n",
       "      <th>stop3</th>\n",
       "      <th>mention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N000</td>\n",
       "      <td>C0011854</td>\n",
       "      <td>248</td>\n",
       "      <td>283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>insulin dependent diabetes mellitus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N001</td>\n",
       "      <td>C4303631</td>\n",
       "      <td>298</td>\n",
       "      <td>327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a right above-knee amputation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N003</td>\n",
       "      <td>C0085671</td>\n",
       "      <td>537</td>\n",
       "      <td>553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dressing changes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N004</td>\n",
       "      <td>C0011079</td>\n",
       "      <td>558</td>\n",
       "      <td>569</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>debridement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N005</td>\n",
       "      <td>C0003232</td>\n",
       "      <td>611</td>\n",
       "      <td>622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>antibiotics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6679</th>\n",
       "      <td>N139</td>\n",
       "      <td>C0442519</td>\n",
       "      <td>4695</td>\n",
       "      <td>4699</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6680</th>\n",
       "      <td>N140</td>\n",
       "      <td>C0699203</td>\n",
       "      <td>4731</td>\n",
       "      <td>4737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>motrin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6681</th>\n",
       "      <td>N141</td>\n",
       "      <td>C0593507</td>\n",
       "      <td>4740</td>\n",
       "      <td>4745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>advil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6682</th>\n",
       "      <td>N142</td>\n",
       "      <td>C0332575</td>\n",
       "      <td>4863</td>\n",
       "      <td>4870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>redness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6683</th>\n",
       "      <td>N143</td>\n",
       "      <td>C0205217</td>\n",
       "      <td>4853</td>\n",
       "      <td>4862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>increased</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6684 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID       CUI  start1  stop1  start2  stop2  start3  stop3  \\\n",
       "0     N000  C0011854     248    283     NaN    NaN     NaN    NaN   \n",
       "1     N001  C4303631     298    327     NaN    NaN     NaN    NaN   \n",
       "2     N003  C0085671     537    553     NaN    NaN     NaN    NaN   \n",
       "3     N004  C0011079     558    569     NaN    NaN     NaN    NaN   \n",
       "4     N005  C0003232     611    622     NaN    NaN     NaN    NaN   \n",
       "...    ...       ...     ...    ...     ...    ...     ...    ...   \n",
       "6679  N139  C0442519    4695   4699     NaN    NaN     NaN    NaN   \n",
       "6680  N140  C0699203    4731   4737     NaN    NaN     NaN    NaN   \n",
       "6681  N141  C0593507    4740   4745     NaN    NaN     NaN    NaN   \n",
       "6682  N142  C0332575    4863   4870     NaN    NaN     NaN    NaN   \n",
       "6683  N143  C0205217    4853   4862     NaN    NaN     NaN    NaN   \n",
       "\n",
       "                                  mention  \n",
       "0     insulin dependent diabetes mellitus  \n",
       "1           a right above-knee amputation  \n",
       "2                        dressing changes  \n",
       "3                             debridement  \n",
       "4                             antibiotics  \n",
       "...                                   ...  \n",
       "6679                                 home  \n",
       "6680                               motrin  \n",
       "6681                                advil  \n",
       "6682                              redness  \n",
       "6683                            increased  \n",
       "\n",
       "[6684 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createIndex(graph,prefix):\n",
    "    id2cui = {}\n",
    "    cui2id = {}\n",
    "    id_to_xref = {id_: data.get('xref') for id_, data in graph.nodes(data=True)}\n",
    "    for graph_id,xrefs in id_to_xref.items():\n",
    "        if(xrefs is None):\n",
    "            cui = None\n",
    "        else:\n",
    "            cui = next((x for x in xrefs if x.startswith(prefix)),None)\n",
    "            if(cui is not None):\n",
    "                cui = cui.replace(prefix,'')\n",
    "        id2cui[graph_id]=cui\n",
    "        if(cui is not None):\n",
    "            cui2id[cui]=graph_id\n",
    "    return(id2cui,cui2id)\n",
    "\n",
    "\n",
    "def convertCui2Doid(cui):\n",
    "    if cui in cui2do:\n",
    "        return cui2do[cui]\n",
    "    return None\n",
    "\n",
    "def hpoId2Name(oboid):\n",
    "    return hpoid_to_name[oboid]\n",
    "\n",
    "def doId2Name(oboid):\n",
    "    if(oboid is None):\n",
    "        return None\n",
    "    if (doid_to_name[oboid]):\n",
    "        return doid_to_name[oboid]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_mentions(filename,bardoc):\n",
    "    all_mentions = []\n",
    "    with open(filename, 'r') as file: \n",
    "        textdoc = file.read()\n",
    "        for line in bardoc.splitlines():\n",
    "            #print(line)\n",
    "            start = int(line.split(\"||\")[2])\n",
    "            stop = int(line.split(\"||\")[3])\n",
    "            mention = textdoc[start:stop]\n",
    "            if(not line.endswith(\"||||||\")):\n",
    "                start = int(line.split(\"||\")[4])\n",
    "                stop = int(line.split(\"||\")[5])\n",
    "                extramention = textdoc[start:stop]\n",
    "                mention = mention+' '+extramention\n",
    "                if(not line.endswith(\"||||\")):\n",
    "                    start = int(line.split(\"||\")[6])\n",
    "                    stop = int(line.split(\"||\")[7])\n",
    "                    extramention = textdoc[start:stop]\n",
    "                    mention = mention+' '+extramention\n",
    "            #print(mention)\n",
    "            all_mentions.append(mention)\n",
    "    return all_mentions\n",
    "\n",
    "def read_files(directory):\n",
    "    all_data = []\n",
    "    for file in os.listdir(directory):\n",
    "        #print(file)\n",
    "        if file.endswith(\".norm\"):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            with open(file_path, 'r') as file:\n",
    "                csv_string = file.read()\n",
    "            #normed = [line+\"||||\" for line in csv_string.splitlines() if line.count('|')==6]\n",
    "            normed = [line if line.count('|') == 14 else (line+\"||||\" if line.count('|') == 10 else line+\"||||||||\") for line in csv_string.splitlines()]\n",
    "            clean = '\\n'.join(normed)\n",
    "            note_file = (str(file.name).replace(\"train_norm\",\"train_note\").replace(\"norm\",\"txt\"))\n",
    "            mentions = get_mentions(note_file,clean)\n",
    "            df = pd.read_csv(StringIO(clean),engine='python',names=['ID', 'CUI', 'start1', 'stop1','start2','stop2','start3','stop3'],sep=\"\\|\\|\")\n",
    "            df['mention']=mentions\n",
    "        all_data.append(df)\n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "\n",
    "hpo2cui,cui2hpo = createIndex(hpo,'UMLS:')\n",
    "do2cui,cui2do = createIndex(do,'UMLS_CUI:')\n",
    "\n",
    "hpoid_to_name = {id_: data.get('name') for id_, data in hpo.nodes(data=True)}\n",
    "doid_to_name = {id_: data.get('name') for id_, data in do.nodes(data=True)}\n",
    "\n",
    "df = pd.DataFrame(list(hpo2cui.items()))\n",
    "df.columns=['HPOID','CUI']\n",
    "df['DOID'] = df['CUI'].apply(convertCui2Doid)\n",
    "df['HPO:Name'] = df['HPOID'].apply(hpoId2Name)\n",
    "df['DO:Name'] = df['DOID'].apply(doId2Name)\n",
    "hpokg = df.copy()\n",
    "print(\"HPO Vocabulary: hpokg\")\n",
    "print(hpokg)\n",
    "kgs = df.mask(df.eq('None')).dropna()\n",
    "\n",
    "# Graph properties that may be useful\n",
    "id_to_isa = {id_: data.get('is_a') for id_, data in hpo.nodes(data=True)}\n",
    "id_to_xref = {id_: data.get('xref') for id_, data in do.nodes(data=True)}\n",
    "result = next(iter(id_to_xref.values()))   \n",
    "\n",
    "print(\"HPO and DO Joint Vocabulary:kgs\")\n",
    "print(kgs)\n",
    "mention_mapping = read_files(\"train/train_norm/\")\n",
    "print(\"Input Corpus Mentions:mention_mapping\")\n",
    "mention_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1a: Generation of Candidate Concepts and their Features (35 points PhD/ 25 points MS)\n",
    "\n",
    "\n",
    "#### Write code to find the best N candidate concepts for the mention using SAPBERT in the small (for final exam performance purposes) merged kgs vocabularuy.\n",
    "\n",
    "The signature of the function should look something like this:\n",
    "``` \n",
    "def getCandidates(mention_embeddings, vocabulary_embeddings, max_candidates):\n",
    "```\n",
    "* mention_embeddings would be SAPBERT embeddings of the mentions\n",
    "* vocabulary_embeddings would be SAPBERT embeddings of the kgs vocabulary. You generate them using just DO concept text, just HPO concept text or perform a function to aggregate them.\n",
    "* max_candidates (max candidates to return from kgs)\n",
    "\n",
    "This function returns a list of the best N matches between the mention and the target merged knowledge graph based on feature similarity between the input node and the target node. Each match in the list is a tuple can contain any elements you need, but it should at least contain\n",
    " * a reference to the target concept, ie) row index|vocabulary_id\n",
    " * score (optional) or anything else you think you need\n",
    " \n",
    " \n",
    "#### Write code to get a set of features for each candidate concepts that can be used for ranking the top N concepts to pick the most correct concept\n",
    "The getFeatures function should generate features for an input mention text and one possible candidate mapping.\n",
    "```\n",
    "def getFeatures(mention_text, candidate_tuple_from_getCandidates)\n",
    "```\n",
    "These features will be used in Part 1b) to generate training data for a machine learning ranking algorithm.\n",
    "\n",
    "Masters student need at least 2 features in their getFeatures code, some examples of lexical features include:\n",
    "* counts of matching words or characters\n",
    "* longest common subsequence (RougeL)\n",
    "* ngram overlap, etc...\n",
    "\n",
    "PhD Students will need an additional graph-based feature using relations in the ontology or ontology node vector representations such as node2vec. For example, one relevant feature may be checking the similarity of the input node to the parent node of the target. They can also be generated per random-walks like node2vec.\n",
    "\n",
    "\n",
    "Hints:\n",
    " * stop words, stemming, lemmatizationm, headword matching are nice but not required for this tiny (mostly matching) gold data set\n",
    " * my advice is to do the minimal amount of work and come back later if you want to add more features\n",
    " * you may use ANY additional libraries as need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Candidate Generation Code\n",
    "def getCandidates(mention_embeddings, vocabulary_embeddings, max_candidates):\n",
    "    \"\"\"\n",
    "    * mention_embeddings would be SAPBERT embeddings of the mentions\n",
    "    * vocabulary_embeddings would be SAPBERT embeddings of the kgs vocabulary. You generate them using just DO concept text, just HPO concept text or perform a function to aggregate them.\n",
    "    * max_candidates (max candidates to return from kgs)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def getFeatures(mention_text, candidate_tuple_from_getCandidates):\n",
    "    \"\"\"\n",
    "    Masters student need at least 2 features in their getFeatures code, some examples of lexical features include:\n",
    "    * counts of matching words or characters\n",
    "    * longest common subsequence (RougeL)\n",
    "    * ngram overlap, etc...\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp2023v2]",
   "language": "python",
   "name": "conda-env-nlp2023v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
