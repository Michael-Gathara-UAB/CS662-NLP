{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EA_0nDtqeG0y"
   },
   "source": [
    "\n",
    "# Natural Language Processing Assignment #2  Fall 2023\n",
    "## (CS/INFO 662 : 220 points ;  CS 762/INFO 762 : 300 points)\n",
    "\n",
    "POS Annotation and Tagging, Machine Learning, Word Vectors, Transformers, BERT\n",
    "\n",
    "* Available Oct 1, 2023\n",
    "* Due October 25th, 11:59pm. Submit via Canvas.\n",
    "\n",
    "* Getting started on cheaha: https://docs.uabgrid.uab.edu/wiki/Cheaha_GettingStarted\n",
    "* Instructions on running Jupyter Notebook on cheaha: https://docs.uabgrid.uab.edu/wiki/Jupyter#Jupyter_on_Cheaha\n",
    "* IPython notebooks: https://ipython.org/ipython-doc/3/notebook/notebook.html#introduction\n",
    "\n",
    "## Reminders\n",
    "* Please test your code before submission. I will run all code in sequence and if it is non-functional you may receive no marks for that question.\n",
    "* If you have successfully run your code before, include the output. You may receive partial marks even if I can't run it.\n",
    "* Please make sure your code does not contain absolute paths to your home directory. You can assume that any needed files are in your current directory when this is run\n",
    "* All code should run on cheaha job requesting no more than 64 GB of RAM\n",
    "* The cheaha server GPUs may or may not be available when you need them. You may need several hours for a GPU. For this reason, the last question is a bonus question. If you want to really learn neural networks I suggest you start early on this assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RLpG2kVveG02"
   },
   "source": [
    "<font color='red'>For this assignment you will have access to MIMIC II clinical data that has been de-identified of personal identifiers. Please write your name below to indicate <b>you will not attempt to re-identify persons indicated in these documents or distribute clinical text outside the cheaha system.</b></font>\n",
    "## Name/Signature of Student:   <font color='red'> YOUR_NAME_HERE </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell  is reserved for any needed imports. \n",
    "### Some imports you may want to consider using are provided below as a convenience. \n",
    "### This is not meant to be a comprehensive list, but provided here to assist you\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify import accuracy\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('brown')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uSfIIB8peG03"
   },
   "source": [
    "## 1.  POS Annotation Task (40 points)\n",
    " * Annotate the geneRIFs assigned to you below using the BRAT annotation server at https://uabnlp.org/#/2023/ using Universal POS tags.\n",
    " \n",
    "Login by mousing over the blue bar at the top right an using username \"student\" and password \"nlp@uab\" to edit your file (listed below). Work on the file of 5 geneRIFs that corresponds to your BLAZER_ID. When you are done, you can download the annotations by mousing over the top blue bar, the \"Data\" option will appear and you can get a tarball of your annotations.\n",
    "\n",
    " For instructions on using BRAT, see https://brat.nlplab.org/\n",
    " \n",
    " For annotation instruction see https://universaldependencies.org/u/pos/\n",
    " \n",
    " Some common errors include:\n",
    "* Confusing punctation such as periods and commas with symbols like +\n",
    "* Not annotating abbreviations with the underlying POS tag they refer to, ex) b.i.d. would have 6 POS tags\n",
    "* Not using Google to look up unknown words. If you still can't find the word, make your best guess as to the POS tag. You will not be downgraded for lack of biological or medical understanding.\n",
    "\n",
    "<font color='red'>Do not annotate the first line of the file, or de-identified text between [** **].  </font> It is acceptable to look at previous POS annotations prior to 2022 by other students, but you will be assessed on the quality of your own annotation.\n",
    "\n",
    "* Save the text file into the variable document. Print it.\n",
    "* Save the annotation file into the variable ann. Print it.\n",
    "\n",
    "Both of these files can be obtained directly from the BRAT server by selecting the \"Data\" button which is visible after mousing over the top left portion of the top blue bar on the BRAT server. Select ann or txt to download each file. Example documents are shown below, replace with your assigned documents and the annotation you generate. You must complete this in order to answer question #2 additional questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MoUAm8nueG04",
    "outputId": "cb841952-e139-43e7-9927-1261ac152303",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "document = \"\"\"\n",
    "MY DOCUMENT HERE\n",
    "\"\"\"\n",
    "\n",
    "ann = \"\"\"\n",
    "The text of my .ann file here\n",
    "\"\"\"\n",
    "\n",
    "print(document)\n",
    "print(ann)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4970QtMjeG1B"
   },
   "source": [
    "## 2. Compare the MedSpacy POS Tagger results with your manual annotation (20 points)\n",
    "\n",
    "* Do NOT start this until you finish question #1\n",
    "* Use MedSpacy's universal tagger to tag your test document (5 points)\n",
    "* Pick an annotated sentence in your document that is the greatest or close to greatest in length and print out the differences between your manual annotation of that sentence and the MedSpacy version (ignoring any punctation differences) for each token (10 points)\n",
    "* <font color='green'>Explain any differences with the MedSpacy's POS tagger and fix any errors you believe may have made manually. (5 points) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "colab_type": "code",
    "id": "sQ-SrOSZeG1C",
    "outputId": "918aba10-355c-400d-c70e-0223dcdcf405",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Do not forget to answer the comparison question as a written response as well\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ajQWFZleG1H"
   },
   "source": [
    "## 3. Apply an embedding model to identify similar text (20 points) \n",
    "\n",
    "* Use the huggingface cambridgeltl/SapBERT-from-PubMedBERT-fulltext model to create embeddings for each geneRIF you manually annotated. (10 points)\n",
    "\n",
    "* Find the closest word to each of your geneRIFs to the geneRIFs in the \"test\" small GeneRIF data set that is not identical (10 points)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YakqhOzreG1I"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\")  \n",
    "model = AutoModel.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\").cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Create, train and evaluate your own neural network model that uses the Pytorch torch.nn.TransformerEncoder to encode 40K sentences from the Brown corpus to output the correct POS tag. (PhD - 100 Points; MS 50 Points Total, 50 Points Bonus)\n",
    "\n",
    "* Final accuracy should be over 90%, otherwise only 50 points are possible.\n",
    "* You may use ChatGPT to assist you\n",
    "* Do NOT use predefined embeddings and do NOT use a recurrent neural network of any type (LSTM, GRU, etc..)\n",
    "* Your solution may be \"BERT-like\" but do NOT use BERT or BERT weights directly (no transfer learning and NO huggingface)\n",
    "* For performance reasons use initially only a small number of sentences from the Brown Corpus, then increase the number of sentences for training to improve your results.\n",
    "* Your solution will take 2+ hours to run depending on your solution. Do not you leave yourself less than 48 hours to complete the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from nltk.corpus import brown\n",
    "\n",
    "brown_news_tagged = brown.tagged_sents(categories='news', tagset='universal')[:2000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare your Pytorch Encoder POS Tagger with your manual annotation (15 points)\n",
    "* Run your POS tagger from question #4 on your manual annotations from question #1  (5 points)\n",
    "* If you failed to complete question #4, use your NLTK's POS Tagger instead.\n",
    "* Generate a confusion matrix using sklearn for each tag type between your pytorch POS tagger (or NLTK POS tagger if your TransformerEncoder didn't work) and your manually annotated document (gold standard). Did you do better, worse or the same than the MedSpacy algorithm and why? (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% time\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Causal Language Model Training (75 points MS, 105 points PhD)\n",
    "The training of Casual Language Models form the basis of the GPT-X series of Language Models and EleutherAI has attempted to replicate GPT-3 with its GPT-Neo models. While GPT-Neo models have had some exposure to biomedical (PubMed) text, they have not had exposure to GeneRIFs. Included in this assignment is a directory with a large GeneRIF data set version (100K) and a small GeneRIF data set version (1K) lines, each of which has its train, validation and test data sets. Using the gpt-neo-125M model (smallest available):\n",
    "* Perform Casual Language Modeling on the SMALL geneRif data set (65 points, ALL students) and compute validation loss over at least 5 epochs. You will not be graded on the quality of training on this data set, it has been provided to allow you to get your code working without regard as to your hyperparameter settings or machine learning skills.\n",
    "* Perform Casual Language Modeling on the LARGE geneRIF data set, this time optimize hyperparameters and show improvement in validation loss (30 points, PhD Required, MS Bonus). Do NOT attempt until you have it working on the smaller data set. Be attentive to memory constraints, you may run on ampere nodes and optionally use both GPUs.\n",
    "* Speculate on the performance of your best working model and why I gave you such a tiny (125M) model tfor this task. Why does it train or not train well? (5 points) Propose a way to evaluate your updated model (5 points). \n",
    "* Additional bonus are available for students who have high performing models above 110 points.\n",
    "\n",
    "For this question you may work with ChatGPT (but not other students!) and use Google as much as like. Just cite any ChatGPT provided help and references as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "import torch\n",
    "import datasets\n",
    "\n",
    "from transformers import GPTNeoForCausalLM, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(f'torch.cuda.is_available(): {torch.cuda.is_available()}')\n",
    "print(f'torch.cuda.current_device(): {torch.cuda.current_device()}')\n",
    "print(f'torch.cuda.device(0): {torch.cuda.device(0)}')\n",
    "print(f'torch.cuda.device_count(): {torch.cuda.device_count()}')\n",
    "print(f'torch.cuda.get_device_name(0): {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "model_name=\"EleutherAI/gpt-neo-125M\"\n",
    "\n",
    "# YOU CODE HERE"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "Assignment2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:py310cuda116]",
   "language": "python",
   "name": "conda-env-py310cuda116-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
