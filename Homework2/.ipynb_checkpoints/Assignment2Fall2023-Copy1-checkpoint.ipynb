{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EA_0nDtqeG0y"
   },
   "source": [
    "\n",
    "# Natural Language Processing Assignment #2  Fall 2023\n",
    "## (CS/INFO 662 : 220 points ;  CS 762/INFO 762 : 300 points)\n",
    "\n",
    "POS Annotation and Tagging, Machine Learning, Word Vectors, Transformers, BERT\n",
    "\n",
    "* Available Oct 1, 2023\n",
    "* Due October 25th, 11:59pm. Submit via Canvas.\n",
    "\n",
    "* Getting started on cheaha: https://docs.uabgrid.uab.edu/wiki/Cheaha_GettingStarted\n",
    "* Instructions on running Jupyter Notebook on cheaha: https://docs.uabgrid.uab.edu/wiki/Jupyter#Jupyter_on_Cheaha\n",
    "* IPython notebooks: https://ipython.org/ipython-doc/3/notebook/notebook.html#introduction\n",
    "\n",
    "## Reminders\n",
    "* Please test your code before submission. I will run all code in sequence and if it is non-functional you may receive no marks for that question.\n",
    "* If you have successfully run your code before, include the output. You may receive partial marks even if I can't run it.\n",
    "* Please make sure your code does not contain absolute paths to your home directory. You can assume that any needed files are in your current directory when this is run\n",
    "* All code should run on cheaha job requesting no more than 64 GB of RAM\n",
    "* The cheaha server GPUs may or may not be available when you need them. You may need several hours for a GPU. For this reason, the last question is a bonus question. If you want to really learn neural networks I suggest you start early on this assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RLpG2kVveG02"
   },
   "source": [
    "<font color='red'>For this assignment you will have access to MIMIC II clinical data that has been de-identified of personal identifiers. Please write your name below to indicate <b>you will not attempt to re-identify persons indicated in these documents or distribute clinical text outside the cheaha system.</b></font>\n",
    "## Name/Signature of Student:   <font color='red'> Michael Gathara </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell  is reserved for any needed imports. \n",
    "### Some imports you may want to consider using are provided below as a convenience. \n",
    "### This is not meant to be a comprehensive list, but provided here to assist you\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import nltk\n",
    "\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify import accuracy\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('brown')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uSfIIB8peG03"
   },
   "source": [
    "## 1.  POS Annotation Task (40 points)\n",
    " * Annotate the geneRIFs assigned to you below using the BRAT annotation server at https://uabnlp.org/#/2023/ using Universal POS tags.\n",
    " \n",
    "Login by mousing over the blue bar at the top right an using username \"student\" and password \"nlp@uab\" to edit your file (listed below). Work on the file of 5 geneRIFs that corresponds to your BLAZER_ID. When you are done, you can download the annotations by mousing over the top blue bar, the \"Data\" option will appear and you can get a tarball of your annotations.\n",
    "\n",
    " For instructions on using BRAT, see https://brat.nlplab.org/\n",
    " \n",
    " For annotation instruction see https://universaldependencies.org/u/pos/\n",
    " \n",
    " Some common errors include:\n",
    "* Confusing punctation such as periods and commas with symbols like +\n",
    "* Not annotating abbreviations with the underlying POS tag they refer to, ex) b.i.d. would have 6 POS tags\n",
    "* Not using Google to look up unknown words. If you still can't find the word, make your best guess as to the POS tag. You will not be downgraded for lack of biological or medical understanding.\n",
    "\n",
    "<font color='red'>Do not annotate the first line of the file, or de-identified text between [** **].  </font> It is acceptable to look at previous POS annotations prior to 2022 by other students, but you will be assessed on the quality of your own annotation.\n",
    "\n",
    "* Save the text file into the variable document. Print it.\n",
    "* Save the annotation file into the variable ann. Print it.\n",
    "\n",
    "Both of these files can be obtained directly from the BRAT server by selecting the \"Data\" button which is visible after mousing over the top left portion of the top blue bar on the BRAT server. Select ann or txt to download each file. Example documents are shown below, replace with your assigned documents and the annotation you generate. You must complete this in order to answer question #2 additional questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MoUAm8nueG04",
    "outputId": "cb841952-e139-43e7-9927-1261ac152303",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['T50', 'VERB 340 348', 'involved'], ['T51', 'PROPN 349 351', 'in'], ['T7', 'CCONJ 51 54', 'and'], ['T58', 'NOUN 366 375', 'responses'], ['T72', 'NOUN 470 478', 'activity'], ['T26', 'ADJ 183 192', 'Important'], ['T70', 'DET 459 462', 'the'], ['T98', 'NOUN 629 639', 'progenitor'], ['T93', 'DET 603 608', 'which'], ['T53', 'CCONJ 355 358', 'and'], ['T20', 'NOUN 134 145', 'progenitors'], ['T63', 'NOUN 398 403', 'STAT6'], ['T92', 'NOUN 592 601', 'signaling'], ['T37', 'NOUN 262 273', 'correlation'], ['T71', 'NOUN 463 469', 'kinase'], ['T28', 'DET 197 200', 'the'], ['T61', 'VERB 387 392', 'shows'], ['T1', 'PROPN 0 5', 'Sulf1'], ['T90', 'NOUN 582 585', 'IL4'], ['T14', 'ADP 97 99', 'of'], ['T38', 'ADP 274 281', 'between'], ['T67', 'NOUN 433 443', 'production'], ['T64', 'ADV 404 414', 'negatively'], ['T56', 'PUNCT 360 361', '('], ['T84', 'NOUN 545 552', 'neurons'], ['T57', 'PUNCT 364 365', ')'], ['T2', 'VERB 6 14', 'triggers'], ['T15', 'NOUN 100 104', 'gene'], ['T40', 'NOUN 286 296', 'expression'], ['T27', 'ADP 193 196', 'for'], ['T68', 'ADP 444 446', 'by'], ['T69', 'VERB 447 458', 'attenuating'], ['T3', 'NOUN 15 18', 'Shh'], ['T62', 'ADP 393 397', 'that'], ['T76', 'NOUN 495 501', 'kinase'], ['T5', 'NOUN 29 37', 'activity'], ['T55', 'NOUN 361 364', 'reg'], ['T17', 'ADP 116 118', 'in'], ['T47', 'PROPN 323 327', 'with'], ['T66', 'NOUN 425 432', 'IFNphi1'], ['T6', 'VERB 41 50', 'establish'], ['T95', 'ADJ 617 623', 'neural'], ['T52', 'NOUN 352 354', 'Th'], ['T42', 'NOUN 302 305', 'bet'], ['T79', 'NOUN 510 517', 'results'], ['T60', 'NOUN 381 386', 'study'], ['T74', 'NOUN 482 486', 'TANK'], ['T91', 'NOUN 586 591', 'STAT6'], ['T75', 'VERB 487 494', 'binding'], ['T8', 'ADV 56 61', 'later'], ['T25', 'VERB 179 182', 'Are'], ['T41', 'ADP 297 299', 'of'], ['T82', 'NOUN 527 536', 'crosstalk'], ['T39', 'DET 282 285', 'the'], ['T29', 'NOUN 201 210', 'Integrity'], ['T12', 'ADJ 77 84', 'spatial'], ['T33', 'NOUN 229 239', 'Filtration'], ['T81', 'DET 525 526', 'a'], ['T100', 'NOUN 645 655', 'plasticity'], ['T46', 'NOUN 317 322', 'foxp3'], ['T85', 'CCONJ 553 556', 'and'], ['T94', 'VERB 609 616', 'induces'], ['T87', 'NOUN 564 569', 'cells'], ['T11', 'DET 73 76', 'the'], ['T30', 'ADP 211 213', 'of'], ['T34', 'NOUN 240 247', 'Barrier'], ['T102', 'NOUN 659 668', 'zebrafish'], ['T44', 'NOUN 307 312', 'stat6'], ['T45', 'CCONJ 313 316', 'and'], ['T59', 'DET 376 380', 'this'], ['T22', 'ADP 158 160', 'in'], ['T97', 'PUNCT 628 629', '/'], ['T103', 'NOUN 669 675', 'brains'], ['T89', 'PROPN 579 581', 'by'], ['T19', 'ADJ 127 133', 'neural'], ['T86', 'ADJ 557 563', 'immune'], ['T65', 'VERB 415 424', 'regulates'], ['T31', 'DET 214 217', 'the'], ['T54', 'NOUN 359 360', 'T'], ['T99', 'NOUN 640 644', 'cell'], ['T9', 'ADV 62 64', 'on'], ['T78', 'DET 504 509', 'These'], ['T10', 'VERB 66 72', 'modify'], ['T49', 'NOUN 334 339', 'genes'], ['T32', 'ADJ 218 228', 'Glomerular'], ['T18', 'ADJ 119 126', 'ventral'], ['T80', 'VERB 518 524', 'reveal'], ['T23', 'NOUN 161 171', 'Particular'], ['T83', 'ADP 537 544', 'between'], ['T16', 'NOUN 105 115', 'expression'], ['T77', 'NUM 502 503', '1'], ['T48', 'ADJ 328 333', 'other'], ['T24', 'PROPN 172 177', 'Sulf1'], ['T88', 'VERB 570 578', 'mediated'], ['T13', 'NOUN 85 96', 'arrangement'], ['T96', 'NOUN 624 628', 'stem'], ['T21', 'PROPN 146 156', 'Sulfatases'], ['T36', 'NOUN 251 260', 'Zebrafish'], ['T101', 'ADP 656 658', 'in'], ['T35', 'ADP 248 250', 'in'], ['T73', 'ADP 479 481', 'of'], ['T4', 'NOUN 19 28', 'signaling'], ['T43', 'NOUN 300 301', 't']]\n",
      "T56\tPUNCT 360 361\t(\n",
      "T57\tPUNCT 364 365\t)\n",
      "T81\tDET 525 526\ta\n",
      "T97\tPUNCT 628 629\t/\n",
      "T54\tNOUN 359 360\tT\n",
      "T77\tNUM 502 503\t1\n",
      "T43\tNOUN 300 301\tt\n",
      "T51\tPROPN 349 351\tin\n",
      "T14\tADP 97 99\tof\n",
      "T68\tADP 444 446\tby\n",
      "T17\tADP 116 118\tin\n",
      "T52\tNOUN 352 354\tTh\n",
      "T41\tADP 297 299\tof\n",
      "T30\tADP 211 213\tof\n",
      "T22\tADP 158 160\tin\n",
      "T89\tPROPN 579 581\tby\n",
      "T9\tADV 62 64\ton\n",
      "T101\tADP 656 658\tin\n",
      "T35\tADP 248 250\tin\n",
      "T73\tADP 479 481\tof\n",
      "T7\tCCONJ 51 54\tand\n",
      "T70\tDET 459 462\tthe\n",
      "T53\tCCONJ 355 358\tand\n",
      "T28\tDET 197 200\tthe\n",
      "T90\tNOUN 582 585\tIL4\n",
      "T27\tADP 193 196\tfor\n",
      "T3\tNOUN 15 18\tShh\n",
      "T55\tNOUN 361 364\treg\n",
      "T42\tNOUN 302 305\tbet\n",
      "T25\tVERB 179 182\tAre\n",
      "T39\tDET 282 285\tthe\n",
      "T85\tCCONJ 553 556\tand\n",
      "T11\tDET 73 76\tthe\n",
      "T45\tCCONJ 313 316\tand\n",
      "T31\tDET 214 217\tthe\n",
      "T15\tNOUN 100 104\tgene\n",
      "T62\tADP 393 397\tthat\n",
      "T47\tPROPN 323 327\twith\n",
      "T74\tNOUN 482 486\tTANK\n",
      "T59\tDET 376 380\tthis\n",
      "T99\tNOUN 640 644\tcell\n",
      "T96\tNOUN 624 628\tstem\n",
      "T93\tDET 603 608\twhich\n",
      "T63\tNOUN 398 403\tSTAT6\n",
      "T61\tVERB 387 392\tshows\n",
      "T1\tPROPN 0 5\tSulf1\n",
      "T60\tNOUN 381 386\tstudy\n",
      "T91\tNOUN 586 591\tSTAT6\n",
      "T8\tADV 56 61\tlater\n",
      "T46\tNOUN 317 322\tfoxp3\n",
      "T87\tNOUN 564 569\tcells\n",
      "T44\tNOUN 307 312\tstat6\n",
      "T78\tDET 504 509\tThese\n",
      "T49\tNOUN 334 339\tgenes\n",
      "T48\tADJ 328 333\tother\n",
      "T24\tPROPN 172 177\tSulf1\n",
      "T71\tNOUN 463 469\tkinase\n",
      "T76\tNOUN 495 501\tkinase\n",
      "T95\tADJ 617 623\tneural\n",
      "T103\tNOUN 669 675\tbrains\n",
      "T19\tADJ 127 133\tneural\n",
      "T86\tADJ 557 563\timmune\n",
      "T10\tVERB 66 72\tmodify\n",
      "T80\tVERB 518 524\treveal\n",
      "T38\tADP 274 281\tbetween\n",
      "T84\tNOUN 545 552\tneurons\n",
      "T66\tNOUN 425 432\tIFNphi1\n",
      "T79\tNOUN 510 517\tresults\n",
      "T75\tVERB 487 494\tbinding\n",
      "T12\tADJ 77 84\tspatial\n",
      "T94\tVERB 609 616\tinduces\n",
      "T34\tNOUN 240 247\tBarrier\n",
      "T18\tADJ 119 126\tventral\n",
      "T83\tADP 537 544\tbetween\n",
      "T50\tVERB 340 348\tinvolved\n",
      "T72\tNOUN 470 478\tactivity\n",
      "T2\tVERB 6 14\ttriggers\n",
      "T5\tNOUN 29 37\tactivity\n",
      "T88\tVERB 570 578\tmediated\n",
      "T58\tNOUN 366 375\tresponses\n",
      "T26\tADJ 183 192\tImportant\n",
      "T92\tNOUN 592 601\tsignaling\n",
      "T6\tVERB 41 50\testablish\n",
      "T82\tNOUN 527 536\tcrosstalk\n",
      "T29\tNOUN 201 210\tIntegrity\n",
      "T102\tNOUN 659 668\tzebrafish\n",
      "T65\tVERB 415 424\tregulates\n",
      "T36\tNOUN 251 260\tZebrafish\n",
      "T4\tNOUN 19 28\tsignaling\n",
      "T98\tNOUN 629 639\tprogenitor\n",
      "T67\tNOUN 433 443\tproduction\n",
      "T64\tADV 404 414\tnegatively\n",
      "T40\tNOUN 286 296\texpression\n",
      "T33\tNOUN 229 239\tFiltration\n",
      "T100\tNOUN 645 655\tplasticity\n",
      "T32\tADJ 218 228\tGlomerular\n",
      "T23\tNOUN 161 171\tParticular\n",
      "T16\tNOUN 105 115\texpression\n",
      "T21\tPROPN 146 156\tSulfatases\n",
      "T20\tNOUN 134 145\tprogenitors\n",
      "T37\tNOUN 262 273\tcorrelation\n",
      "T69\tVERB 447 458\tattenuating\n",
      "T13\tNOUN 85 96\tarrangement\n"
     ]
    }
   ],
   "source": [
    "document = \"\"\"Sulf1 triggers Shh signaling activity to establish and, later on, modify the spatial arrangement of gene expression in ventral neural progenitors\n",
    "Sulfatases, in Particular Sulf1, Are Important for the Integrity of the Glomerular Filtration Barrier in Zebrafish.\n",
    "correlation between the expression of t-bet, stat6 and foxp3 with other genes involved in Th and T(reg) responses\n",
    "this study shows that STAT6 negatively regulates IFNphi1 production by attenuating the kinase activity of TANK-binding kinase 1\n",
    "These results reveal a crosstalk between neurons and immune cells mediated by IL4/STAT6 signaling, which induces neural stem/progenitor cell plasticity in zebrafish brains.\"\"\"\n",
    "\n",
    "ann = \"\"\"T1\tPROPN 0 5\tSulf1\n",
    "T2\tVERB 6 14\ttriggers\n",
    "T3\tNOUN 15 18\tShh\n",
    "T4\tNOUN 19 28\tsignaling\n",
    "T5\tNOUN 29 37\tactivity\n",
    "T6\tVERB 41 50\testablish\n",
    "T7\tCCONJ 51 54\tand\n",
    "T8\tADV 56 61\tlater\n",
    "T9\tADV 62 64\ton\n",
    "T10\tVERB 66 72\tmodify\n",
    "T11\tDET 73 76\tthe\n",
    "T12\tADJ 77 84\tspatial\n",
    "T13\tNOUN 85 96\tarrangement\n",
    "T14\tADP 97 99\tof\n",
    "T15\tNOUN 100 104\tgene\n",
    "T16\tNOUN 105 115\texpression\n",
    "T17\tADP 116 118\tin\n",
    "T18\tADJ 119 126\tventral\n",
    "T19\tADJ 127 133\tneural\n",
    "T20\tNOUN 134 145\tprogenitors\n",
    "T21\tPROPN 146 156\tSulfatases\n",
    "T22\tADP 158 160\tin\n",
    "T23\tNOUN 161 171\tParticular\n",
    "T24\tPROPN 172 177\tSulf1\n",
    "T25\tVERB 179 182\tAre\n",
    "T26\tADJ 183 192\tImportant\n",
    "T27\tADP 193 196\tfor\n",
    "T28\tDET 197 200\tthe\n",
    "T29\tNOUN 201 210\tIntegrity\n",
    "T30\tADP 211 213\tof\n",
    "T31\tDET 214 217\tthe\n",
    "T32\tADJ 218 228\tGlomerular\n",
    "T33\tNOUN 229 239\tFiltration\n",
    "T34\tNOUN 240 247\tBarrier\n",
    "T35\tADP 248 250\tin\n",
    "T36\tNOUN 251 260\tZebrafish\n",
    "T37\tNOUN 262 273\tcorrelation\n",
    "T38\tADP 274 281\tbetween\n",
    "T39\tDET 282 285\tthe\n",
    "T40\tNOUN 286 296\texpression\n",
    "T41\tADP 297 299\tof\n",
    "T42\tNOUN 302 305\tbet\n",
    "T43\tNOUN 300 301\tt\n",
    "T44\tNOUN 307 312\tstat6\n",
    "T45\tCCONJ 313 316\tand\n",
    "T46\tNOUN 317 322\tfoxp3\n",
    "T47\tPROPN 323 327\twith\n",
    "T48\tADJ 328 333\tother\n",
    "T49\tNOUN 334 339\tgenes\n",
    "T50\tVERB 340 348\tinvolved\n",
    "T51\tPROPN 349 351\tin\n",
    "T52\tNOUN 352 354\tTh\n",
    "T53\tCCONJ 355 358\tand\n",
    "T54\tNOUN 359 360\tT\n",
    "T55\tNOUN 361 364\treg\n",
    "T56\tPUNCT 360 361\t(\n",
    "T57\tPUNCT 364 365\t)\n",
    "T58\tNOUN 366 375\tresponses\n",
    "T59\tDET 376 380\tthis\n",
    "T60\tNOUN 381 386\tstudy\n",
    "T61\tVERB 387 392\tshows\n",
    "T62\tADP 393 397\tthat\n",
    "T63\tNOUN 398 403\tSTAT6\n",
    "T64\tADV 404 414\tnegatively\n",
    "T65\tVERB 415 424\tregulates\n",
    "T66\tNOUN 425 432\tIFNphi1\n",
    "T67\tNOUN 433 443\tproduction\n",
    "T68\tADP 444 446\tby\n",
    "T69\tVERB 447 458\tattenuating\n",
    "T70\tDET 459 462\tthe\n",
    "T71\tNOUN 463 469\tkinase\n",
    "T72\tNOUN 470 478\tactivity\n",
    "T73\tADP 479 481\tof\n",
    "T74\tNOUN 482 486\tTANK\n",
    "T75\tVERB 487 494\tbinding\n",
    "T76\tNOUN 495 501\tkinase\n",
    "T77\tNUM 502 503\t1\n",
    "T78\tDET 504 509\tThese\n",
    "T79\tNOUN 510 517\tresults\n",
    "T80\tVERB 518 524\treveal\n",
    "T81\tDET 525 526\ta\n",
    "T82\tNOUN 527 536\tcrosstalk\n",
    "T83\tADP 537 544\tbetween\n",
    "T84\tNOUN 545 552\tneurons\n",
    "T85\tCCONJ 553 556\tand\n",
    "T86\tADJ 557 563\timmune\n",
    "T87\tNOUN 564 569\tcells\n",
    "T88\tVERB 570 578\tmediated\n",
    "T89\tPROPN 579 581\tby\n",
    "T90\tNOUN 582 585\tIL4\n",
    "T91\tNOUN 586 591\tSTAT6\n",
    "T92\tNOUN 592 601\tsignaling\n",
    "T93\tDET 603 608\twhich\n",
    "T94\tVERB 609 616\tinduces\n",
    "T95\tADJ 617 623\tneural\n",
    "T96\tNOUN 624 628\tstem\n",
    "T97\tPUNCT 628 629\t/\n",
    "T98\tNOUN 629 639\tprogenitor\n",
    "T99\tNOUN 640 644\tcell\n",
    "T100\tNOUN 645 655\tplasticity\n",
    "T101\tADP 656 658\tin\n",
    "T102\tNOUN 659 668\tzebrafish\n",
    "T103\tNOUN 669 675\tbrains\n",
    "\"\"\"\n",
    "\n",
    "# print(document)\n",
    "# print(ann)\n",
    "\n",
    "# Sorting to find longest word\n",
    "ann_list = list(set(ann.split(\"\\n\"))) # this will be out of order, sets have no order\n",
    "while \"\" in ann_list: ann_list.remove(\"\")    \n",
    "entry = [y.split(\"\\t\") for y in ann_list]\n",
    "\n",
    "# for x in entry:\n",
    "#     try:\n",
    "#         print(x[1].split()[1])\n",
    "#     except:\n",
    "#         print(x)\n",
    "sorted_ann = sorted(entry, key=lambda x: int(x[1].split()[2]) - int(x[1].split()[1]))\n",
    "\n",
    "actual_ann = \"\\n\".join([\"\\t\".join(entry) for entry in sorted_ann])\n",
    "print(actual_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4970QtMjeG1B"
   },
   "source": [
    "## 2. Compare the MedSpacy POS Tagger results with your manual annotation (20 points)\n",
    "\n",
    "* Do NOT start this until you finish question #1\n",
    "* Use MedSpacy's universal tagger to tag your test document (5 points)\n",
    "* Pick an annotated sentence in your document that is the greatest or close to greatest in length and print out the differences between your manual annotation of that sentence and the MedSpacy version (ignoring any punctation differences) for each token (10 points)\n",
    "* <font color='green'>Explain any differences with the MedSpacy's POS tagger and fix any errors you believe may have made manually. (5 points) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "colab_type": "code",
    "id": "sQ-SrOSZeG1C",
    "outputId": "918aba10-355c-400d-c70e-0223dcdcf405",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('These', 'DET', 'DT'), ('results', 'NOUN', 'NNS'), ('reveal', 'VERB', 'VBP'), ('a', 'DET', 'DT'), ('crosstalk', 'NOUN', 'NN'), ('between', 'ADP', 'IN'), ('neurons', 'NOUN', 'NNS'), ('and', 'CCONJ', 'CC'), ('immune', 'ADJ', 'JJ'), ('cells', 'NOUN', 'NNS'), ('mediated', 'VERB', 'VBN'), ('by', 'ADP', 'IN'), ('IL4', 'PROPN', 'NNP'), ('STAT6', 'PROPN', 'NNP'), ('signaling', 'VERB', 'VBG'), ('which', 'PRON', 'WDT'), ('induces', 'VERB', 'VBZ'), ('neural', 'ADJ', 'JJ'), ('stem', 'NOUN', 'NN'), ('progenitor', 'NOUN', 'NN'), ('cell', 'NOUN', 'NN'), ('plasticity', 'NOUN', 'NN'), ('in', 'ADP', 'IN'), ('zebrafish', 'ADJ', 'JJ'), ('brains', 'NOUN', 'NNS')]\n",
      "[['T51', 'PROPN 349 351', 'in'], ['T7', 'CCONJ 51 54', 'and'], ['T98', 'NOUN 629 639', 'progenitor'], ['T93', 'DET 603 608', 'which'], ['T53', 'CCONJ 355 358', 'and'], ['T63', 'NOUN 398 403', 'STAT6'], ['T92', 'NOUN 592 601', 'signaling'], ['T90', 'NOUN 582 585', 'IL4'], ['T38', 'ADP 274 281', 'between'], ['T84', 'NOUN 545 552', 'neurons'], ['T68', 'ADP 444 446', 'by'], ['T17', 'ADP 116 118', 'in'], ['T95', 'ADJ 617 623', 'neural'], ['T52', 'NOUN 352 354', 'Th'], ['T42', 'NOUN 302 305', 'bet'], ['T79', 'NOUN 510 517', 'results'], ['T91', 'NOUN 586 591', 'STAT6'], ['T82', 'NOUN 527 536', 'crosstalk'], ['T81', 'DET 525 526', 'a'], ['T100', 'NOUN 645 655', 'plasticity'], ['T85', 'CCONJ 553 556', 'and'], ['T94', 'VERB 609 616', 'induces'], ['T87', 'NOUN 564 569', 'cells'], ['T102', 'NOUN 659 668', 'zebrafish'], ['T45', 'CCONJ 313 316', 'and'], ['T22', 'ADP 158 160', 'in'], ['T103', 'NOUN 669 675', 'brains'], ['T89', 'PROPN 579 581', 'by'], ['T19', 'ADJ 127 133', 'neural'], ['T86', 'ADJ 557 563', 'immune'], ['T54', 'NOUN 359 360', 'T'], ['T99', 'NOUN 640 644', 'cell'], ['T9', 'ADV 62 64', 'on'], ['T78', 'DET 504 509', 'These'], ['T80', 'VERB 518 524', 'reveal'], ['T83', 'ADP 537 544', 'between'], ['T88', 'VERB 570 578', 'mediated'], ['T96', 'NOUN 624 628', 'stem'], ['T101', 'ADP 656 658', 'in'], ['T35', 'ADP 248 250', 'in'], ['T4', 'NOUN 19 28', 'signaling'], ['T43', 'NOUN 300 301', 't']]\n",
      "25\n",
      "42\n",
      "CPU times: user 369 ms, sys: 6.05 ms, total: 375 ms\n",
      "Wall time: 374 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Do not forget to answer the comparison question as a written response as well\n",
    "\n",
    "# !pip install spacy\n",
    "# !python3 -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "sentences = \"\"\"Sulf1 triggers Shh signaling activity to establish and, later on, modify the spatial arrangement of gene expression in ventral neural progenitors.\n",
    "Sulfatases, in Particular Sulf1, Are Important for the Integrity of the Glomerular Filtration Barrier in Zebrafish.\n",
    "correlation between the expression of t-bet, stat6 and foxp3 with other genes involved in Th and T(reg) responses.\n",
    "this study shows that STAT6 negatively regulates IFNphi1 production by attenuating the kinase activity of TANK-binding kinase 1.\n",
    "These results reveal a crosstalk between neurons and immune cells mediated by IL4/STAT6 signaling, which induces neural stem/progenitor cell plasticity in zebrafish brains\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Picking the longest sentence\"\"\"\n",
    "sentence_list = sentences.split(\".\")\n",
    "assert len(sentence_list) == 5\n",
    "sentence_list_broken = [sentence.split(\" \") for sentence in sentence_list]\n",
    "\n",
    "len_list = [len(x) for x in sentence_list_broken]\n",
    "longest_sentence = sentence_list[len_list.index(max(len_list))]\n",
    "\n",
    "longest_sentence_manual_tags = []\n",
    "for tags in entry:\n",
    "    if tags[2] in longest_sentence:\n",
    "        longest_sentence_manual_tags.append(tags)\n",
    "\n",
    "\n",
    "\n",
    "# I took the below from the official spacy documentation\n",
    "# https://spacy.io/usage/spacy-101\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(longest_sentence)\n",
    "spacy_tags = []\n",
    "for token in doc:\n",
    "    spacy_tags.append((token.text, token.pos_, token.tag_))\n",
    "longest_sentence_manual_tags.remove(['T97', 'PUNCT 628 629', '/'])\n",
    "    \n",
    "#     Super kludgy code here but it works\n",
    "while ('\\n', 'SPACE', '_SP') in spacy_tags: spacy_tags.remove(('\\n', 'SPACE', '_SP'))\n",
    "while (',', 'PUNCT', ',') in spacy_tags: spacy_tags.remove((',', 'PUNCT', ','))\n",
    "while ('/', 'SYM', 'SYM') in spacy_tags: spacy_tags.remove(('/', 'SYM', 'SYM'))\n",
    "print(spacy_tags)\n",
    "print(longest_sentence_manual_tags)\n",
    "print(len(spacy_tags))\n",
    "print(len(longest_sentence_manual_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ajQWFZleG1H"
   },
   "source": [
    "## 3. Apply an embedding model to identify similar text (20 points) \n",
    "\n",
    "* Use the huggingface cambridgeltl/SapBERT-from-PubMedBERT-fulltext model to create embeddings for each geneRIF you manually annotated. (10 points)\n",
    "\n",
    "* Find the closest word to each of your geneRIFs to the geneRIFs in the \"test\" small GeneRIF data set that is not identical (10 points)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YakqhOzreG1I"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between gene1 and gene2: 0.5010140538215637\n",
      "[[0.50101405]]\n",
      "CPU times: user 1.18 s, sys: 213 ms, total: 1.39 s\n",
      "Wall time: 1.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\")  \n",
    "model = AutoModel.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\").cuda()\n",
    "\n",
    "gene1 = \"These results reveal a crosstalk between neurons and immune cells mediated by IL4/STAT6 signaling, which induces neural stem/progenitor cell plasticity in zebrafish brains\"\n",
    "gene2 = \"A novel de novo mutation in COL1A1 leading to osteogenesis imperfecta confirmed by zebrafish model\"\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {key: val.cuda() for key, val in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Get embeddings for your geneRIF and the test geneRIF\n",
    "my_embedding = get_embedding(gene1)\n",
    "test_embedding = get_embedding(gene2)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "my_embedding = my_embedding.cpu().numpy().squeeze()\n",
    "test_embedding = test_embedding.cpu().numpy().squeeze()\n",
    "\n",
    "# Compute similarity\n",
    "similarity = cosine_similarity(my_embedding.reshape(1, -1), test_embedding.reshape(1, -1))\n",
    "\n",
    "print(f\"Cosine similarity between gene1 and gene2: {similarity[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Create, train and evaluate your own neural network model that uses the Pytorch torch.nn.TransformerEncoder to encode 40K sentences from the Brown corpus to output the correct POS tag. (PhD - 100 Points; MS 50 Points Total, 50 Points Bonus)\n",
    "\n",
    "* Final accuracy should be over 90%, otherwise only 50 points are possible.\n",
    "* You may use ChatGPT to assist you\n",
    "* Do NOT use predefined embeddings and do NOT use a recurrent neural network of any type (LSTM, GRU, etc..)\n",
    "* Your solution may be \"BERT-like\" but do NOT use BERT or BERT weights directly (no transfer learning and NO huggingface)\n",
    "* For performance reasons use initially only a small number of sentences from the Brown Corpus, then increase the number of sentences for training to improve your results.\n",
    "* Your solution will take 2+ hours to run depending on your solution. Do not you leave yourself less than 48 hours to complete the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/mikegtr/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/mikegtr/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 - Validation Loss: 0.2427, Accuracy: 91.72%\n",
      "Epoch 2/200 - Validation Loss: 0.2021, Accuracy: 93.02%\n",
      "Epoch 3/200 - Validation Loss: 0.1913, Accuracy: 93.50%\n",
      "Epoch 4/200 - Validation Loss: 0.1941, Accuracy: 93.42%\n",
      "Epoch 5/200 - Validation Loss: 0.1897, Accuracy: 93.68%\n",
      "Epoch 6/200 - Validation Loss: 0.1992, Accuracy: 93.85%\n",
      "Epoch 7/200 - Validation Loss: 0.2031, Accuracy: 93.94%\n",
      "Epoch 8/200 - Validation Loss: 0.2022, Accuracy: 93.97%\n",
      "Epoch 9/200 - Validation Loss: 0.2079, Accuracy: 93.90%\n",
      "Epoch 10/200 - Validation Loss: 0.2259, Accuracy: 93.61%\n",
      "Epoch 11/200 - Validation Loss: 0.2162, Accuracy: 94.10%\n",
      "Epoch 12/200 - Validation Loss: 0.2130, Accuracy: 94.07%\n",
      "Epoch 13/200 - Validation Loss: 0.2225, Accuracy: 93.95%\n",
      "Epoch 14/200 - Validation Loss: 0.2231, Accuracy: 94.06%\n",
      "Epoch 15/200 - Validation Loss: 0.2300, Accuracy: 94.03%\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "# We'll use the universal tagset for simplicity\n",
    "sentences = brown.tagged_sents(tagset='universal')[:40000]\n",
    "\n",
    "word_vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "tag_vocab = {\"<PAD>\": 0}\n",
    "\n",
    "for sentence in sentences:\n",
    "    for word, tag in sentence:\n",
    "        if word not in word_vocab:\n",
    "            word_vocab[word] = len(word_vocab)\n",
    "        if tag not in tag_vocab:\n",
    "            tag_vocab[tag] = len(tag_vocab)\n",
    "\n",
    "tag_vocab_size = len(tag_vocab)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "def collate_fn(batch):\n",
    "    words, tags = zip(*batch)\n",
    "    words_padded = pad_sequence(words, batch_first=True, padding_value=0)\n",
    "    tags_padded = pad_sequence(tags, batch_first=True, padding_value=0)\n",
    "    return words_padded, tags_padded\n",
    "\n",
    "\n",
    "class POSTaggingDataset(Dataset):\n",
    "    def __init__(self, sentences, word_vocab, tag_vocab):\n",
    "        self.sentences = sentences\n",
    "        self.word_vocab = word_vocab\n",
    "        self.tag_vocab = tag_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words, tags = zip(*self.sentences[idx])\n",
    "        words = [self.word_vocab.get(word, self.word_vocab[\"<UNK>\"]) for word in words]\n",
    "        tags = [self.tag_vocab[tag] for tag in tags]\n",
    "        return torch.tensor(words), torch.tensor(tags)\n",
    "\n",
    "train_sentences, test_sentences = train_test_split(sentences, test_size=0.2)\n",
    "train_dataset = POSTaggingDataset(train_sentences, word_vocab, tag_vocab)\n",
    "test_dataset = POSTaggingDataset(test_sentences, word_vocab, tag_vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, Embedding\n",
    "\n",
    "class POSTaggingModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, tag_vocab_size, d_model=512, nhead=8, num_layers=6):\n",
    "        super(POSTaggingModel, self).__init__()\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, d_model)\n",
    "        self.transformer = TransformerEncoder(\n",
    "            TransformerEncoderLayer(d_model, nhead), num_layers\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(d_model, tag_vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = POSTaggingModel(len(word_vocab), tag_vocab_size).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "def validate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for words, tags in test_loader:\n",
    "            words, tags = words.to(device), tags.to(device)\n",
    "            outputs = model(words)\n",
    "            loss = criterion(outputs.view(-1, tag_vocab_size), tags.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Convert outputs to predicted tags\n",
    "            _, predicted_tags = torch.max(outputs, dim=2)\n",
    "            \n",
    "            # Mask to avoid counting <PAD> in accuracy calculation\n",
    "            mask = tags != 0\n",
    "            \n",
    "            # Update counts\n",
    "            total_correct += (predicted_tags[mask] == tags[mask]).sum().item()\n",
    "            total_count += mask.sum().item()  # Total non-<PAD> tags\n",
    "            \n",
    "    return total_loss / len(test_loader), 100 * total_correct / total_count\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for words, tags in train_loader:\n",
    "        words, tags = words.to(device), tags.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(words)\n",
    "        loss = criterion(outputs.view(-1, tag_vocab_size), tags.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_accuracy = validate(model, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare your Pytorch Encoder POS Tagger with your manual annotation (15 points)\n",
    "* Run your POS tagger from question #4 on your manual annotations from question #1  (5 points)\n",
    "* If you failed to complete question #4, use your NLTK's POS Tagger instead.\n",
    "* Generate a confusion matrix using sklearn for each tag type between your pytorch POS tagger (or NLTK POS tagger if your TransformerEncoder didn't work) and your manually annotated document (gold standard). Did you do better, worse or the same than the MedSpacy algorithm and why? (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (8.1.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from ipywidgets) (8.15.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: stack-data in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: backcall in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: matplotlib-inline in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: exceptiongroup in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.3)\n",
      "Requirement already satisfied: pickleshare in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.16.1)\n",
      "Requirement already satisfied: decorator in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.3.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: pure-eval in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Config option `kernel_spec_manager_class` not recognized by `EnableNBExtensionApp`.\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      " - nvidia\n",
      " - pytorch\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /data/user/mikegtr/Conda_Env/nlp2023v2\n",
      "\n",
      "  added / updated specs:\n",
      "    - tensorboard\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _libgcc_mutex-0.1          |             main           2 KB  conda-forge\n",
      "    absl-py-2.0.0              |     pyhd8ed1ab_0         103 KB  conda-forge\n",
      "    aiohttp-3.8.1              |  py310h7f8727e_1         895 KB\n",
      "    aiosignal-1.3.1            |     pyhd8ed1ab_0          12 KB  conda-forge\n",
      "    async-timeout-4.0.3        |     pyhd8ed1ab_0          11 KB  conda-forge\n",
      "    attrs-23.1.0               |     pyh71513ae_1          54 KB  conda-forge\n",
      "    blinker-1.6.3              |     pyhd8ed1ab_0          18 KB  conda-forge\n",
      "    c-ares-1.18.1              |       h7f8727e_0         114 KB\n",
      "    cachetools-5.3.2           |     pyhd8ed1ab_0          14 KB  conda-forge\n",
      "    certifi-2023.7.22          |     pyhd8ed1ab_0         150 KB  conda-forge\n",
      "    click-8.1.7                |unix_pyh707e725_0          82 KB  conda-forge\n",
      "    frozenlist-1.2.0           |  py310h7f8727e_1         149 KB\n",
      "    google-auth-2.23.3         |     pyhca7485f_0         100 KB  conda-forge\n",
      "    google-auth-oauthlib-0.4.6 |     pyhd8ed1ab_0          19 KB  conda-forge\n",
      "    grpcio-1.42.0              |  py310hce63b2e_0        24.3 MB\n",
      "    importlib-metadata-6.8.0   |     pyha770c72_0          25 KB  conda-forge\n",
      "    libprotobuf-3.20.1         |       h4ff587b_0         2.1 MB\n",
      "    markdown-3.5               |     pyhd8ed1ab_0          75 KB  conda-forge\n",
      "    markupsafe-2.1.1           |  py310h7f8727e_0          34 KB\n",
      "    multidict-5.1.0            |  py310h7f8727e_2         148 KB\n",
      "    oauthlib-3.2.2             |     pyhd8ed1ab_0          90 KB  conda-forge\n",
      "    protobuf-3.20.1            |  py310h295c915_0         1.3 MB\n",
      "    pyasn1-0.5.0               |     pyhd8ed1ab_0          61 KB  conda-forge\n",
      "    pyasn1-modules-0.3.0       |     pyhd8ed1ab_0          93 KB  conda-forge\n",
      "    pyjwt-2.8.0                |     pyhd8ed1ab_0          24 KB  conda-forge\n",
      "    pyu2f-0.1.5                |     pyhd8ed1ab_0          31 KB  conda-forge\n",
      "    requests-oauthlib-1.3.1    |     pyhd8ed1ab_0          22 KB  conda-forge\n",
      "    rsa-4.9                    |     pyhd8ed1ab_0          29 KB  conda-forge\n",
      "    tensorboard-2.11.2         |     pyhd8ed1ab_0         5.4 MB  conda-forge\n",
      "    tensorboard-data-server-0.6.0|  py310hca6d32c_0         2.7 MB\n",
      "    tensorboard-plugin-wit-1.8.1|     pyhd8ed1ab_0         668 KB  conda-forge\n",
      "    typing-extensions-4.4.0    |  py310h06a4308_0           8 KB\n",
      "    werkzeug-3.0.1             |     pyhd8ed1ab_0         236 KB  conda-forge\n",
      "    yarl-1.6.3                 |  py310h7f8727e_1         244 KB\n",
      "    zipp-3.17.0                |     pyhd8ed1ab_0          19 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        39.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  absl-py            conda-forge/noarch::absl-py-2.0.0-pyhd8ed1ab_0 \n",
      "  aiohttp            pkgs/main/linux-64::aiohttp-3.8.1-py310h7f8727e_1 \n",
      "  aiosignal          conda-forge/noarch::aiosignal-1.3.1-pyhd8ed1ab_0 \n",
      "  async-timeout      conda-forge/noarch::async-timeout-4.0.3-pyhd8ed1ab_0 \n",
      "  attrs              conda-forge/noarch::attrs-23.1.0-pyh71513ae_1 \n",
      "  blinker            conda-forge/noarch::blinker-1.6.3-pyhd8ed1ab_0 \n",
      "  c-ares             pkgs/main/linux-64::c-ares-1.18.1-h7f8727e_0 \n",
      "  cachetools         conda-forge/noarch::cachetools-5.3.2-pyhd8ed1ab_0 \n",
      "  click              conda-forge/noarch::click-8.1.7-unix_pyh707e725_0 \n",
      "  frozenlist         pkgs/main/linux-64::frozenlist-1.2.0-py310h7f8727e_1 \n",
      "  google-auth        conda-forge/noarch::google-auth-2.23.3-pyhca7485f_0 \n",
      "  google-auth-oauth~ conda-forge/noarch::google-auth-oauthlib-0.4.6-pyhd8ed1ab_0 \n",
      "  grpcio             pkgs/main/linux-64::grpcio-1.42.0-py310hce63b2e_0 \n",
      "  importlib-metadata conda-forge/noarch::importlib-metadata-6.8.0-pyha770c72_0 \n",
      "  libprotobuf        pkgs/main/linux-64::libprotobuf-3.20.1-h4ff587b_0 \n",
      "  markdown           conda-forge/noarch::markdown-3.5-pyhd8ed1ab_0 \n",
      "  markupsafe         pkgs/main/linux-64::markupsafe-2.1.1-py310h7f8727e_0 \n",
      "  multidict          pkgs/main/linux-64::multidict-5.1.0-py310h7f8727e_2 \n",
      "  oauthlib           conda-forge/noarch::oauthlib-3.2.2-pyhd8ed1ab_0 \n",
      "  protobuf           pkgs/main/linux-64::protobuf-3.20.1-py310h295c915_0 \n",
      "  pyasn1             conda-forge/noarch::pyasn1-0.5.0-pyhd8ed1ab_0 \n",
      "  pyasn1-modules     conda-forge/noarch::pyasn1-modules-0.3.0-pyhd8ed1ab_0 \n",
      "  pyjwt              conda-forge/noarch::pyjwt-2.8.0-pyhd8ed1ab_0 \n",
      "  pyu2f              conda-forge/noarch::pyu2f-0.1.5-pyhd8ed1ab_0 \n",
      "  requests-oauthlib  conda-forge/noarch::requests-oauthlib-1.3.1-pyhd8ed1ab_0 \n",
      "  rsa                conda-forge/noarch::rsa-4.9-pyhd8ed1ab_0 \n",
      "  tensorboard        conda-forge/noarch::tensorboard-2.11.2-pyhd8ed1ab_0 \n",
      "  tensorboard-data-~ pkgs/main/linux-64::tensorboard-data-server-0.6.0-py310hca6d32c_0 \n",
      "  tensorboard-plugi~ conda-forge/noarch::tensorboard-plugin-wit-1.8.1-pyhd8ed1ab_0 \n",
      "  typing-extensions  pkgs/main/linux-64::typing-extensions-4.4.0-py310h06a4308_0 \n",
      "  werkzeug           conda-forge/noarch::werkzeug-3.0.1-pyhd8ed1ab_0 \n",
      "  yarl               pkgs/main/linux-64::yarl-1.6.3-py310h7f8727e_1 \n",
      "  zipp               conda-forge/noarch::zipp-3.17.0-pyhd8ed1ab_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  openssl                                 1.1.1v-h7f8727e_0 --> 1.1.1w-h7f8727e_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  _libgcc_mutex                                   pkgs/main --> conda-forge \n",
      "  certifi            pkgs/main/linux-64::certifi-2023.7.22~ --> conda-forge/noarch::certifi-2023.7.22-pyhd8ed1ab_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c-ares-1.18.1        | 114 KB    |                                       |   0% \n",
      "pyasn1-modules-0.3.0 | 93 KB     |                                       |   0% \u001b[A\n",
      "\n",
      "yarl-1.6.3           | 244 KB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "attrs-23.1.0         | 54 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "aiosignal-1.3.1      | 12 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "blinker-1.6.3        | 18 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "oauthlib-3.2.2       | 90 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensorboard-data-ser | 2.7 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensorboard-plugin-w | 668 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pyu2f-0.1.5          | 31 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "absl-py-2.0.0        | 103 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "requests-oauthlib-1. | 22 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "multidict-5.1.0      | 148 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aiohttp-3.8.1        | 895 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "async-timeout-4.0.3  | 11 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cachetools-5.3.2     | 14 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rsa-4.9              | 29 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "importlib-metadata-6 | 25 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zipp-3.17.0          | 19 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "certifi-2023.7.22    | 150 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "markdown-3.5         | 75 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "typing-extensions-4. | 8 KB      |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "google-auth-2.23.3   | 100 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "c-ares-1.18.1        | 114 KB    | ##########4                           |  28% [A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "yarl-1.6.3           | 244 KB    | ##4                                   |   7% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "attrs-23.1.0         | 54 KB     | ###########                           |  30% \u001b[A\u001b[A\u001b[A\n",
      "pyasn1-modules-0.3.0 | 93 KB     | ######3                               |  17% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "aiosignal-1.3.1      | 12 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "aiosignal-1.3.1      | 12 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "oauthlib-3.2.2       | 90 KB     | ######5                               |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "c-ares-1.18.1        | 114 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "yarl-1.6.3           | 244 KB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "yarl-1.6.3           | 244 KB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "blinker-1.6.3        | 18 KB     | #################################3    |  90% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensorboard-plugin-w | 668 KB    | 8                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensorboard-data-ser | 2.7 MB    | 2                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "blinker-1.6.3        | 18 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pyasn1-modules-0.3.0 | 93 KB     | ##################################### | 100% \u001b[A\n",
      "pyasn1-modules-0.3.0 | 93 KB     | ##################################### | 100% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "absl-py-2.0.0        | 103 KB    | #####7                                |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "oauthlib-3.2.2       | 90 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "oauthlib-3.2.2       | 90 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pyu2f-0.1.5          | 31 KB     | ###################                   |  51% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "requests-oauthlib-1. | 22 KB     | ###########################3          |  74% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "absl-py-2.0.0        | 103 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "requests-oauthlib-1. | 22 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aiohttp-3.8.1        | 895 KB    | 6                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pyu2f-0.1.5          | 31 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "async-timeout-4.0.3  | 11 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensorboard-data-ser | 2.7 MB    | ############7                         |  34% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cachetools-5.3.2     | 14 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "async-timeout-4.0.3  | 11 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cachetools-5.3.2     | 14 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "multidict-5.1.0      | 148 KB    | ####                                  |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rsa-4.9              | 29 KB     | ####################2                 |  55% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "importlib-metadata-6 | 25 KB     | #######################3              |  63% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensorboard-data-ser | 2.7 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensorboard-plugin-w | 668 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensorboard-plugin-w | 668 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zipp-3.17.0          | 19 KB     | ###############################9      |  86% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "certifi-2023.7.22    | 150 KB    | ###9                                  |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "importlib-metadata-6 | 25 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rsa-4.9              | 29 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "markdown-3.5         | 75 KB     | #######9                              |  21% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "typing-extensions-4. | 8 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "google-auth-2.23.3   | 100 KB    | #####9                                |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zipp-3.17.0          | 19 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aiohttp-3.8.1        | 895 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiohttp-3.8.1        | 895 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "certifi-2023.7.22    | 150 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "multidict-5.1.0      | 148 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "multidict-5.1.0      | 148 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "typing-extensions-4. | 8 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "markdown-3.5         | 75 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "markdown-3.5         | 75 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "google-auth-2.23.3   | 100 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "google-auth-2.23.3   | 100 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "from ipywidgets import FloatProgress\n",
    "!conda install -c conda-forge tensorboard -y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Causal Language Model Training (75 points MS, 105 points PhD)\n",
    "The training of Casual Language Models form the basis of the GPT-X series of Language Models and EleutherAI has attempted to replicate GPT-3 with its GPT-Neo models. While GPT-Neo models have had some exposure to biomedical (PubMed) text, they have not had exposure to GeneRIFs. Included in this assignment is a directory with a large GeneRIF data set version (100K) and a small GeneRIF data set version (1K) lines, each of which has its train, validation and test data sets. Using the gpt-neo-125M model (smallest available):\n",
    "* Perform Casual Language Modeling on the SMALL geneRif data set (65 points, ALL students) and compute validation loss over at least 5 epochs. You will not be graded on the quality of training on this data set, it has been provided to allow you to get your code working without regard as to your hyperparameter settings or machine learning skills.\n",
    "* Perform Casual Language Modeling on the LARGE geneRIF data set, this time optimize hyperparameters and show improvement in validation loss (30 points, PhD Required, MS Bonus). Do NOT attempt until you have it working on the smaller data set. Be attentive to memory constraints, you may run on ampere nodes and optionally use both GPUs.\n",
    "* Speculate on the performance of your best working model and why I gave you such a tiny (125M) model tfor this task. Why does it train or not train well? (5 points) Propose a way to evaluate your updated model (5 points). \n",
    "* Additional bonus are available for students who have high performing models above 110 points.\n",
    "\n",
    "For this question you may work with ChatGPT (but not other students!) and use Google as much as like. Just cite any ChatGPT provided help and references as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 s, sys: 0 ns, total: 2 s\n",
      "Wall time: 5.72 s\n",
      "torch.cuda.is_available(): True\n",
      "torch.cuda.current_device(): 0\n",
      "torch.cuda.device(0): <torch.cuda.device object at 0x2aaab4e74940>\n",
      "torch.cuda.device_count(): 2\n",
      "torch.cuda.get_device_name(0): NVIDIA A100 80GB PCIe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the  Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26570 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is model on GPU: True\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import torch\n",
    "import datasets\n",
    "\n",
    "from transformers import GPTNeoForCausalLM, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import SGD\n",
    "from transformers import GPTNeoConfig\n",
    "\n",
    "print(f'torch.cuda.is_available(): {torch.cuda.is_available()}')\n",
    "print(f'torch.cuda.current_device(): {torch.cuda.current_device()}')\n",
    "print(f'torch.cuda.device(0): {torch.cuda.device(0)}')\n",
    "print(f'torch.cuda.device_count(): {torch.cuda.device_count()}')\n",
    "print(f'torch.cuda.get_device_name(0): {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer.pad_token = tokenizer.eos_token  # Manually set padding token\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# Load existing config from the pretrained model\n",
    "# config = GPTNeoConfig.from_pretrained(model_name)\n",
    "\n",
    "# # Update dropout rate\n",
    "# config.dropout = 0.05  # You can set any value between 0 and 1\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# class CustomTextDataset(Dataset):\n",
    "#     def __init__(self, file_path, tokenizer, block_size):\n",
    "#         super(CustomTextDataset, self).__init__()\n",
    "        \n",
    "#         # Read the file\n",
    "#         with open(file_path, 'r') as f:\n",
    "#             lines = f.readlines()\n",
    "        \n",
    "#         # Tokenize the text and truncate\n",
    "#         self.examples = [tokenizer(line, truncation=True, padding='max_length', max_length=block_size)['input_ids'] for line in lines]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.examples)\n",
    "\n",
    "#     def __getitem__(self, i):\n",
    "#         return torch.tensor(self.examples[i])\n",
    "\n",
    "block_size = 128\n",
    "# train_dataset = CustomTextDataset('miniDataSet/train.txt', tokenizer, block_size)\n",
    "# print(f\"\\n\\n{train_dataset}\\n\\n\")\n",
    "# val_dataset = CustomTextDataset('miniDataSet/validation.txt', tokenizer, block_size)\n",
    "# test_dataset = CustomTextDataset('miniDataSet/test.txt', tokenizer, block_size)\n",
    "\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"miniDataSet/train.txt\",\n",
    "    block_size=block_size\n",
    ")\n",
    "\n",
    "val_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"miniDataSet/validation.txt\",\n",
    "    block_size=block_size\n",
    ")\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"miniDataSet/test.txt\",\n",
    "    block_size=block_size\n",
    ")\n",
    "\n",
    "# optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# TrainingArguments and DataCollator\n",
    "training_args = TrainingArguments(\n",
    "#     gradient_accumulation_steps=2,\n",
    "    output_dir='./output_small',\n",
    "    overwrite_output_dir=True,\n",
    "#     eval_accumulation_steps=1,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "#     per_device_eval_batch_size=32,\n",
    "#     weight_decay=0.01,\n",
    "#     learning_rate=0.001,\n",
    "#     logging_dir='./logs_small',\n",
    "#     logging_steps=100,\n",
    "#     save_steps=5000,\n",
    "#     save_total_limit=2,\n",
    "#     remove_unused_columns=False,\n",
    "#     report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     optimizers=(optimizer, None)\n",
    ")\n",
    "\n",
    "# Training and Validation for SMALL dataset\n",
    "print(f\"Is model on GPU: {next(model.parameters()).is_cuda}\")\n",
    "print(f\"Training\")\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()\n",
    "\n",
    "# Test\n",
    "print(f\"Testing\")\n",
    "trainer.evaluate(test_dataset)\n",
    "\n",
    "# Save the trained model\n",
    "print(f\"Saving the model\")\n",
    "trainer.save_model('./saved_model_small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cheaha setup (this is just for my sake)\n",
    "\n",
    "Load anaconda\n",
    "`module load Anaconda3/2022.05`\n",
    "\n",
    "\n",
    "``\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "Assignment2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:nlp2023v2]",
   "language": "python",
   "name": "conda-env-nlp2023v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
