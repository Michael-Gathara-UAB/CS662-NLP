{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Natural Language Processing  Assignment #1 Fall 2023 \n",
    "## Due September 20th, 2023  11:59pm CST\n",
    "## Student Name:<font color='red'> Michael Gathara </font>\n",
    "Regular Expressions, Tokenization and Normalization, Dynamic Programming, Document Classification, Perplexity\n",
    "\n",
    "* Getting started on cheaha: https://docs.uabgrid.uab.edu/wiki/Cheaha_GettingStarted\n",
    "* Instructions on running Jupyter Notebook on cheaha: https://docs.uabgrid.uab.edu/wiki/Jupyter#Jupyter_on_Cheaha\n",
    "* IPython notebooks: https://ipython.org/ipython-doc/3/notebook/notebook.html#introduction\n",
    "\n",
    "<font color=\"red\">Do not forget to answer the written questions! </font>Just coding will not get you full points. You can answer the questions in the same text box as they are asked.\n",
    "\n",
    "As of the time of writing, a suitable conda environment with NLTK and other needed libraries should be or should be soon available as a Jupyter kernel - \"nlp2022\" or \"DeepPhe\". Ambitious students may still want to learn to create their own environment, but code should work in the provided environment. Instructions for creating your own custom environment on cheaha are provided below:\n",
    "https://docs.uabgrid.uab.edu/wiki/Jupyter#Adding_Custom_Conda_Environments_to_Jupyter\n",
    "\n",
    "Note: I give \"hints\" via imports, but not all imports are shown.\n",
    "\n",
    "Submission will be via Canvas. Attempt all questions, butn not all questions may be marked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. De-identification with Regular Expressions (20 points)\n",
    "\n",
    "You started working for a company that needs to scrub identifiers from user uploaded files in order to comply with EU General Data Protection Regulation (GDPR) . A beta version of a piece of de-identification software used in the United States removed physicians name, patient names and some location names from the note below, but failed to location and some date information. \n",
    "\n",
    "\n",
    "* Write a regular expression/s to replace ALL date and location references with XX/XX/XXXX and XXXXX respectively. \n",
    "\n",
    "* Make use of a dictionary (small set of terms) to assist with this. \n",
    "\n",
    "Your regular expressions should work for this note, but not be so specific that they are unlikely to be useful on other notes. \n",
    "\n",
    "* Print before and after documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT TEXT\n",
      "\n",
      "                XXXXXX, XXXXXXX\n",
      "                ##########\n",
      "                London Clinic\n",
      "                XXXXX \n",
      "                XXXXX, W1G 6BW\n",
      "                Date 07/01/2023\n",
      "        \n",
      "NEUROLOGY AND PAIN MEDICINE\n",
      "INITIAL OUTPATIENT NOTE\n",
      "\n",
      "XXXXXX, XXXXXX\n",
      "PATIENT HISTORY\n",
      "Ms. XXXXXX is a 56y women seen in consultation at the request of Dr. XXXXXXXXXX. The patient is alert \n",
      "and orinted to self, place and cirumstance. She self-reports stress due the lingering effects of the\n",
      "her 17 year-old son's suicide and has difficulty continuing her work. Denies depression, SI. PMH BPD.\n",
      "\n",
      "Consitution: Well developed, well nourished in no apparent distress.\n",
      "\n",
      "Assessment: Fibromyalgia, stress\n",
      "\n",
      "TREATMENT/PLAN:\n",
      "Duloxetine\n",
      "Suggested finding more time for exercise and sleep for stress\n",
      "\n",
      "REFERRING PHYSICIAN\n",
      "XXXXXX, XXXXXX, MD\n",
      "\n",
      "Electronially signed by XXXXXX, XXXXXX DO, PhD on 07/02/2023\n",
      "\n",
      "CONVERTED TEXT------------------\n",
      "\n",
      "                XXXXXX, XXXXXXX\n",
      "                ##########\n",
      "XXXXX\n",
      "                Date XX/XX/XXXX\n",
      "        \n",
      "NEUROLOGY AND PAIN MEDICINE\n",
      "INITIAL OUTPATIENT NOTE\n",
      "\n",
      "XXXXXX, XXXXXX\n",
      "XXXXX\n",
      "her 17 year-old son's suicide and has difficulty continuing her work. Denies depression, SI. PMH BPD.\n",
      "\n",
      "Consitution: Well developed, well nourished in no apparent distress.\n",
      "\n",
      "Assessment: Fibromyalgia, stress\n",
      "\n",
      "TREATMENT/PLAN:\n",
      "Duloxetine\n",
      "Suggested finding more time for exercise and sleep for stress\n",
      "\n",
      "REFERRING PHYSICIAN\n",
      "XXXXXX, XXXXXX, MD\n",
      "\n",
      "Electronially signed by XXXXXX, XXXXXX DO, PhD on XX/XX/XXXX\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os;\n",
    "import re;\n",
    "fakedoc = \"\"\"\n",
    "                XXXXXX, XXXXXXX\n",
    "                ##########\n",
    "                London Clinic\n",
    "                XXXXX \n",
    "                XXXXX, W1G 6BW\n",
    "                Date 07/01/2023\n",
    "        \n",
    "NEUROLOGY AND PAIN MEDICINE\n",
    "INITIAL OUTPATIENT NOTE\n",
    "\n",
    "XXXXXX, XXXXXX\n",
    "PATIENT HISTORY\n",
    "Ms. XXXXXX is a 56y women seen in consultation at the request of Dr. XXXXXXXXXX. The patient is alert \n",
    "and orinted to self, place and cirumstance. She self-reports stress due the lingering effects of the\n",
    "her 17 year-old son's suicide and has difficulty continuing her work. Denies depression, SI. PMH BPD.\n",
    "\n",
    "Consitution: Well developed, well nourished in no apparent distress.\n",
    "\n",
    "Assessment: Fibromyalgia, stress\n",
    "\n",
    "TREATMENT/PLAN:\n",
    "Duloxetine\n",
    "Suggested finding more time for exercise and sleep for stress\n",
    "\n",
    "REFERRING PHYSICIAN\n",
    "XXXXXX, XXXXXX, MD\n",
    "\n",
    "Electronially signed by XXXXXX, XXXXXX DO, PhD on 07/02/2023\n",
    "\"\"\"\n",
    "print(\"CURRENT TEXT\")\n",
    "print(fakedoc)\n",
    "\n",
    "# Write regular expression/s here, be as general as possible with your regular expression\n",
    "# fakedoc = re.sub(r\"(0?[1-9]|[12][0-9]|3[01])/[0-9]+/((20[012]\\d|19\\d\\d)|(1\\d|2[0123]))\", \"XX/XX/XXXX\", fakedoc)\n",
    "# fakedoc = re.sub(r\"(\\t*)\" + \"(.+)\\n(.+)\\n(.+),\\s(.+)\\n\", r'\\1' + \"XXXXX\\n\", fakedoc)\n",
    "# Using dictionary\n",
    "regexHolder = {\n",
    "    r\"(0?[1-9]|[12][0-9]|3[01])/[0-9]+/((20[012]\\d|19\\d\\d)|(1\\d|2[0123]))\" : \"XX/XX/XXXX\",\n",
    "    r\"(\\t*)\" + \"(.+)\\n(.+)\\n(.+),\\s(.+)\\n\" : r'\\1' + \"XXXXX\\n\"\n",
    "}\n",
    "\n",
    "for regex in regexHolder:\n",
    "    fakedoc = re.sub(regex, regexHolder[regex], fakedoc)\n",
    "\n",
    "print(\"CONVERTED TEXT------------------\")\n",
    "print(fakedoc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. De-identification with ChatGPT and Use of Regex (10 points+5 bonus points)\n",
    "\n",
    "#### Design a prompt to have ChatGPT (GPT-3.5 @ https://chat.openai.com/) do Question 1 for you (2 pts).\n",
    "\n",
    "* <strong>Show both the prompt you designed (1 pt) and the output from ChatGPT (1 pt). </strong>\n",
    "- This is my prompt\n",
    "```\n",
    "You are an expert programmer, who has been utilizing Python with the re library. You understand everything about regex and how it works. Now think of the following exercise\n",
    "\n",
    "```START EXERCISE\n",
    "You started working for a company that needs to scrub identifiers from user uploaded files in order to comply with EU General Data Protection Regulation (GDPR) . A beta version of a piece of de-identification software used in the United States removed physicians name, patient names and some location names from the note below, but failed to location and some date information. \n",
    "\n",
    "\n",
    "* Write a regular expression/s to replace ALL date and location references with XX/XX/XXXX and XXXXX respectively. \n",
    "\n",
    "* Make use of a dictionary (small set of terms) to assist with this. \n",
    "\n",
    "Your regular expressions should work for this note, but not be so specific that they are unlikely to be useful on other notes. \n",
    "\n",
    "* Print before and after documents.\n",
    "```END EXERCISE\n",
    "\n",
    "This is some starter code to get you started\n",
    "```START STARTER\n",
    "import os;\n",
    "import re;\n",
    "fakedoc = \"\"\"\n",
    "                XXXXXX, XXXXXXX\n",
    "                ##########\n",
    "                London Clinic\n",
    "                XXXXX \n",
    "                XXXXX, W1G 6BW\n",
    "                Date 07/01/2023\n",
    "        \n",
    "NEUROLOGY AND PAIN MEDICINE\n",
    "INITIAL OUTPATIENT NOTE\n",
    "\n",
    "XXXXXX, XXXXXX\n",
    "PATIENT HISTORY\n",
    "Ms. XXXXXX is a 56y women seen in consultation at the request of Dr. XXXXXXXXXX. The patient is alert \n",
    "and orinted to self, place and cirumstance. She self-reports stress due the lingering effects of the\n",
    "her 17 year-old son's suicide and has difficulty continuing her work. Denies depression, SI. PMH BPD.\n",
    "\n",
    "Consitution: Well developed, well nourished in no apparent distress.\n",
    "\n",
    "Assessment: Fibromyalgia, stress\n",
    "\n",
    "TREATMENT/PLAN:\n",
    "Duloxetine\n",
    "Suggested finding more time for exercise and sleep for stress\n",
    "\n",
    "REFERRING PHYSICIAN\n",
    "XXXXXX, XXXXXX, MD\n",
    "\n",
    "Electronially signed by XXXXXX, XXXXXX DO, PhD on 07/02/2023\n",
    "\"\"\"\n",
    "\n",
    "print(\"CURRENT TEXT\")\n",
    "print(fakedoc)\n",
    "```END START STARTER\n",
    "\n",
    "Can you think deeply and carefully about what you need to do to accomplish this exercise then accmplish it\n",
    "```\n",
    "This is the output from ChatGPT, GPT3.5\n",
    "```\n",
    "Sure, I understand the exercise. Your goal is to write regular expressions to replace all date and location references in the given document with \"XX/XX/XXXX\" and \"XXXXX\" respectively, while making use of a small set of terms stored in a dictionary. Here's how you can accomplish this:\n",
    "```\n",
    "```python\n",
    "import re\n",
    "\n",
    "# Define the dictionary for location and date replacements\n",
    "replacement_dict = {\n",
    "    'London': 'XXXXX',\n",
    "    'W1G 6BW': 'XXXXX',\n",
    "    '07/01/2023': 'XX/XX/XXXX',\n",
    "    '07/02/2023': 'XX/XX/XXXX'\n",
    "}\n",
    "\n",
    "# Define the regular expression patterns for date and location\n",
    "date_pattern = r'\\d{2}/\\d{2}/\\d{4}'\n",
    "location_pattern = r'\\b[A-Z][a-z]+\\b'\n",
    "\n",
    "# Read the document text\n",
    "fakedoc = \"\"\"\n",
    "# ... (document text as provided in the starter code)\n",
    "\"\"\"\n",
    "\n",
    "# Function to replace text using the dictionary\n",
    "def replace_with_dict(match):\n",
    "    return replacement_dict.get(match.group(0), match.group(0))\n",
    "\n",
    "# Replace date references using the date_pattern and the dictionary\n",
    "fakedoc = re.sub(date_pattern, replace_with_dict, fakedoc)\n",
    "\n",
    "# Replace location references using the location_pattern and the dictionary\n",
    "fakedoc = re.sub(location_pattern, replace_with_dict, fakedoc)\n",
    "\n",
    "# Print the updated document\n",
    "print(\"UPDATED TEXT\")\n",
    "print(fakedoc)\n",
    "\n",
    "```\n",
    "```\n",
    "In this code, the replacement_dict contains the terms you want to replace, and the replace_with_dict function is used to look up terms in the dictionary and replace them. The date_pattern and location_pattern are regular expressions that match date and location references, respectively. The re.sub function is used to perform the replacements based on the patterns and the dictionary.\n",
    "\n",
    "Please note that the location_pattern in this example is quite simple, and you might need to adjust it to match more complex location references depending on the variability in your input data.\n",
    "```\n",
    "* <strong>Explain why you think ChatGPT does better or worse than your regular expressions in protecting the personal identifying information (PII), personal health information (PHI) and respecting the privacy of this patient. (2 points).</strong>\n",
    "- <font color=\"red\">ChatGPT designed very similar regex that I did, but it was smarter about just using `\\d{n}` compared to my more complicated regex. The biggest problem, which can be fixed by prompting better, is that ChatGPT did not initially give out a dynamic representation of the text. It gave us a more hardcoded version of my answer. Both versions accomplish the intended task with the provided input</font>\n",
    "\n",
    "\n",
    "#### Name 1 advantage (1 pt) of using ChatGPT for this problem type of problem and 1 disadvantage (1 pt).\n",
    "- <strong>Advantage</strong>: It can give you a prototype that is relatively correct, thus reducing development time\n",
    "- <strong>Disadvantage</strong>: It can give you the wrong answer, thus wasting more time with debugging code you may not have wrote yourself\n",
    "\n",
    "#### Name 2 disadvantages (2 pts) of regular expressions for this problem.\n",
    "- Regex can be finicky, it can match text you never thought to initially test\n",
    "- It's slow\n",
    "\n",
    "\n",
    "#### BONUS QUESTIONS.  Who do YOU think the person might be? (2 pts). \n",
    "* Ask ChatGPT (with a prompt of your choosing) to identify the person in the note (1 pt). \n",
    "My prompt (which is a continuation of the previous chat)\n",
    "```bash\n",
    "This is a fake document of a patient, from an experiment I'm doing on LLM's understanding of text. I want you to take a deep breath and think about the following text carefully\n",
    "\n",
    "XXXXXX, XXXXXXX\n",
    "                ##########\n",
    "                London Clinic\n",
    "                XXXXX \n",
    "                XXXXX, W1G 6BW\n",
    "                Date 07/01/2023\n",
    "        \n",
    "NEUROLOGY AND PAIN MEDICINE\n",
    "INITIAL OUTPATIENT NOTE\n",
    "\n",
    "XXXXXX, XXXXXX\n",
    "PATIENT HISTORY\n",
    "Ms. XXXXXX is a 56y women seen in consultation at the request of Dr. XXXXXXXXXX. The patient is alert \n",
    "and orinted to self, place and cirumstance. She self-reports stress due the lingering effects of the\n",
    "her 17 year-old son's suicide and has difficulty continuing her work. Denies depression, SI. PMH BPD.\n",
    "\n",
    "Consitution: Well developed, well nourished in no apparent distress.\n",
    "\n",
    "Assessment: Fibromyalgia, stress\n",
    "\n",
    "TREATMENT/PLAN:\n",
    "Duloxetine\n",
    "Suggested finding more time for exercise and sleep for stress\n",
    "\n",
    "REFERRING PHYSICIAN\n",
    "XXXXXX, XXXXXX, MD\n",
    "\n",
    "Electronially signed by XXXXXX, XXXXXX DO, PhD on 07/02/2023```\n",
    "Can you tell me anything about the patient in this?, try to pick out anything about this patient that you feel like would identify him/her/them.\n",
    "```\n",
    "ChatGPT's output\n",
    "```bash\n",
    "Based on the provided text, I can gather some information about the patient:\n",
    "\n",
    "    Name: The patient's name is redacted as \"XXXXXX, XXXXXXX.\"\n",
    "\n",
    "    Age: The patient is a 56-year-old woman.\n",
    "\n",
    "    Medical Conditions: The patient has been diagnosed with Fibromyalgia and is experiencing stress. There is no mention of any other specific medical conditions.\n",
    "\n",
    "    Mental Health: The patient self-reports stress related to the lingering effects of her 17-year-old son's suicide. She denies depression and suicidal ideation (SI). There is also a reference to \"PMH BPD,\" which might suggest a previous medical history of Borderline Personality Disorder.\n",
    "\n",
    "    Physical Appearance: The patient is described as \"well developed, well nourished,\" and \"in no apparent distress.\"\n",
    "\n",
    "    Treatment/Plan: The treatment plan for the patient includes the prescription of Duloxetine and the suggestion to find more time for exercise and sleep to address stress.\n",
    "\n",
    "    Referring Physician: The referring physician who requested the consultation is mentioned as \"XXXXXX, XXXXXX, MD.\"\n",
    "\n",
    "    Signature: The document is electronically signed by \"XXXXXX, XXXXXX DO, PhD\" on 07/02/2023.\n",
    "\n",
    "Please note that the patient's name and other identifying information have been redacted in the text, so it is not possible to identify the patient specifically from this information.\n",
    "```\n",
    "Which seems to be the correct answer, it noticed that the person's name was taken out and replaced.\n",
    "* Give 2 limitations of ChatGPT to explain why it is likely to have difficulty with identifying this person (2 pts).\n",
    "- It works by predicting the next text \n",
    "- It can hallucinate and make up answers\n",
    "These limitations can take it down the wrong road of answering the question. The data gives no clear answer as to exactly who the person is as their information is taken away, thus leaving the interpretation strictly to ChatGPT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-tokenization (20 points)\n",
    "GeneRIF (https://www.ncbi.nlm.nih.gov/gene/about-generif) is like Twitter for genes. Biomedical researchers add small free text annotations linking their publication to a particular gene. Each GeneRIF can be considered a document.\n",
    "\n",
    "For this question <font color=\"red\"> DO NOT use ChatGPT for this problem</font> and :\n",
    "* Import a subset of GeneRIF documents, the file is available in canvas and called generifs1000.tsv\n",
    "* Create a corpus, in this case a python list of 1000 geneRIF documents (do not include meta-data in the corpus) in a variable called corpus and print out the number of documents in a corpus (the most basic corpus statistic)\n",
    "* Implement a function that uses the sequential hugging face pre-tokenizer that tokenizes on Whitespace and Punctation. You will use this function again in later questions.\n",
    "* Why is pre-tokenization needed before tokenization?\n",
    "- <font color=\"red\"> Pre-tokenization is needed before tokenization because it helps to create a standard for tokenization to occur. It helps to increase consistency, accuracy and efficiency.</font>\n",
    "* Print out and save (max1 and max2) distinct geneRIFs that have the highest token count in the data set based on your own whitespace/comma tokenizer. Print out should include token count for both geneRIFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the corpus is: 1000\n",
      "Max 1 is 68 and Max 2 is 64\n",
      "CPU times: user 13.1 ms, sys: 13.8 ms, total: 26.9 ms\n",
      "Wall time: 1.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import io\n",
    "from tokenizers import Tokenizer, pre_tokenizers, BertWordPieceTokenizer, models, trainers\n",
    "from tokenizers.pre_tokenizers import Whitespace, Sequence, Punctuation\n",
    "import csv\n",
    "\n",
    "corpus = []\n",
    "with open(\"generifs1000.tsv\", \"r\") as f:\n",
    "    corpus += [line.split(\"\\t\")[4:] for line in f]\n",
    "corpus = corpus[1:]\n",
    "\n",
    "# Since this will be used later, I made it a function\n",
    "\n",
    "fullText = \"\"\n",
    "for x in range(len(corpus)):\n",
    "    fullText += ''.join(corpus[x])\n",
    "\n",
    "def huggingFaceTrainer(trainingData, size):\n",
    "    filePath = f\"{size}.json\"\n",
    "    if os.path.isfile(filePath):\n",
    "        return Tokenizer.from_file(filePath)\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    trainer = trainers.BpeTrainer() \n",
    "    tokenizer.train_from_iterator(trainingData, trainer=trainer)\n",
    "    tokenizer.save(filePath)\n",
    "    return tokenizer\n",
    "\n",
    "def huggingFace(content, tokenizer, removeStop = False) -> list:\n",
    "    tokenizer.pre_tokenizer = Sequence([Whitespace(), Punctuation()])\n",
    "    token = tokenizer.encode(content).tokens\n",
    "    if removeStop:\n",
    "        token = [x for x in token if x not in stop]\n",
    "    return [token, len(token)] \n",
    "\n",
    "def myTokenizer(content) -> list:\n",
    "    tokenized = content.replace(',', '').split(' ')\n",
    "    return [tokenized, len(tokenized)] \n",
    "\n",
    "print(f\"The length of the corpus is: {len(corpus)}\")\n",
    "mytokenizedData = dict()\n",
    "lengths = [['', 0]]\n",
    "for text in corpus:\n",
    "    actualText = text[0]\n",
    "    hashedText = hash(actualText)\n",
    "    if mytokenizedData.get(hashedText):\n",
    "        continue\n",
    "    mytokenizedtext = myTokenizer(actualText)\n",
    "    mytokenizedData[hashedText] = mytokenizedtext\n",
    "    thisTokLen = mytokenizedtext[1]\n",
    "    if int(thisTokLen) > int(lengths[-1][1]):\n",
    "        lengths.insert(len(lengths) ,[actualText, thisTokLen])\n",
    "    elif int(thisTokLen) > int(lengths[-2][1]):\n",
    "        lengths.insert(len(lengths) - 1, [actualText, thisTokLen])\n",
    "\n",
    "max1 = lengths[-1]\n",
    "max2 = lengths[-2]\n",
    "    \n",
    "print(f\"Max 1 is {max1[1]} and Max 2 is {max2[1]}\")\n",
    "# print(f\"Max 1 text \\n {max1[0]} \\n\\tand Max 2 text \\n {max2[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenization (20 points)\n",
    "\n",
    "Tokenize the GeneRIF corpus using nltk's TweetTokenizer and word_tokenize. Remove stopwords using nltk's English stopwords from both the word_tokenize tokens and the TweetTokenizer tokens. Print out average number of tokens per geneRIF for (1) TweetTokenizer and (2) NLTK word_tokenize. \n",
    "\n",
    "\n",
    "* Write a function to compute the Jacard distance between 2 token sets. Find the single geneRIF/s in the set for which is the most disimilar in terms of Jacard differences between the TweetTokenizer and nltk's workd_tokenize after stopword removal. Print it out.\n",
    "\n",
    "* Which tokenizer is more appropriate and why? Do you think this type of corpus requires a better tokenizer?\n",
    "<font color=\"red\">I would say word tokenizer. The word tokenizer was able to tokenize for more text than the tweet tokenizer</font>\n",
    "\n",
    "* Create a new tokenizer by training your own Byte Pair Encoding tokenizer using the pre-tokenizer you wrote in question (3) using geneRIFS100K as training data. Tokenize the largest geneRIF/s using your newly trained tokenizer and comment on the advantages of using Byte Pair Encoding versus traditional tokenization approaches.\n",
    "<font color=\"red\">I think it should be based on your needs, BPE lets you have more control and generalization on your data as compared to having a more specific tokenization.</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 4.29 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mikegtr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mikegtr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tweet avg is 21.479229989868287 and word average is 20.214792299898683\n",
      "The most dissimilar text is: thermostability of cyp175a1\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "import nltk, os\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "corpus = []\n",
    "with open(\"generifs1000.tsv\", \"r\") as f:\n",
    "    corpus += [line.split(\"\\t\")[4:] for line in f]\n",
    "corpus = corpus[1:]\n",
    "stop = set(stopwords.words('english'))\n",
    "tweetCounter = 0\n",
    "wordCounter = 0\n",
    "nltkdata = dict()\n",
    "\n",
    "# Jaccard Distance function\n",
    "def jacDist(first: set, second: set):\n",
    "    intersect = len(first.intersection(second))\n",
    "    union = len(first.union(second))\n",
    "    return 1 - (intersect / union)\n",
    "\n",
    "# Tweet Tokenizer function\n",
    "def myTweetTokenizer(content) -> list:\n",
    "    TTokenizer = TweetTokenizer()\n",
    "    tokenized = TTokenizer.tokenize(content)\n",
    "    Atokenized = [x for x in tokenized if x not in stop]\n",
    "    global tweetCounter\n",
    "    tweetCounter += len(Atokenized)\n",
    "    return Atokenized\n",
    "\n",
    "# NLTK Tokenizer function\n",
    "def myWordTokenizer(content) -> list:\n",
    "    tokenized = word_tokenize(content)\n",
    "    Atokenized = [x for x in tokenized if x not in stop]\n",
    "    global wordCounter\n",
    "    wordCounter += len(Atokenized)\n",
    "    return Atokenized\n",
    "\n",
    "# Main processing loop\n",
    "maxJacDist = 0\n",
    "dissimilar = \"\"\n",
    "disHolder = []\n",
    "\n",
    "for text in corpus:\n",
    "    actualText = text[0].lower()\n",
    "    if nltkdata.get(hash(actualText)):\n",
    "        continue\n",
    "    tweetTokenized = set(myTweetTokenizer(actualText))\n",
    "    wordTokenized = set(myWordTokenizer(actualText))\n",
    "\n",
    "    jDist = jacDist(tweetTokenized, wordTokenized)\n",
    "\n",
    "    if jDist > maxJacDist:\n",
    "        maxJacDist = jDist\n",
    "        disHolder.append(actualText)\n",
    "        dissimilar = actualText\n",
    "\n",
    "    nltkdata[hash(actualText)] = [tweetTokenized, wordTokenized]\n",
    "\n",
    "# Using myTokenizedData length cause I remove duplicates within the corpus\n",
    "size = len(nltkdata)\n",
    "\n",
    "avgTweet = tweetCounter / size\n",
    "avgWord = wordCounter / size\n",
    "\n",
    "print(f\"\\n\\nTweet avg is {avgTweet} and word average is {avgWord}\")\n",
    "print(f\"The most dissimilar text is: {dissimilar}\")\n",
    "\n",
    "\n",
    "\n",
    "# Byte Pair Encoding\n",
    "trainingData = []\n",
    "with open(\"generifs100K\", \"r\") as f:\n",
    "    trainingData += f\n",
    "\n",
    "trainingData = trainingData[1:]\n",
    "size = len(trainingData)\n",
    "\n",
    "tokenizer = huggingFaceTrainer(trainingData, size)\n",
    "myData = dict()\n",
    "largestTokText = max1[0]\n",
    "secondLargestText = max2[0]\n",
    "tokenizedLarge = huggingFace(largestTokText, tokenizer)\n",
    "tokenizedSecLarge = huggingFace(secondLargestText, tokenizer)\n",
    "# for text in corpus:\n",
    "#     actualText = text[0].lower()\n",
    "#     if myData.get(hash(actualText)):\n",
    "#         continue\n",
    "#     tokenized = huggingFace(actualText, tokenizer)\n",
    "#     myData[hash(actualText)] = tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. KL Divergence (15 Points Available, 5 are Required for all students, remaining 10 are bonus for 662, required for 762)\n",
    "\n",
    "You may use ChatGPT for this question. Create 3 overall corpus-level probability distributions for the tokens in the geneRIF corpus, based on your results from the NLTK's tokenizer, the TweetTokenizer and your own BytePairEncoding Scheme after stopword removal. (5 points) Use native python code only so you understand how this function is computed. \n",
    "\n",
    "Look up KL Divergence in Wikipedia or online. What major pitfall do you anticipate if you use this metric to compare token probability distributions? Compute the KL Divergence (a non-symetric measure) using only native python code (no libraries) between your own BytePairEncoding scheme and the 2 other distributions using LaPlace smoothing. (10 points). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kl is 0.0002948241409138963 while kltweet is 0.0003051122762235661 while klbpe is 0.00016321206907736583\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "KL divergence is a non-symmetric metric that measures the relative entropy or difference in information represented by two distributions. It can be thought of as measuring the distance between two data distributions showing how different the two distributions are from each other.\n",
    "\n",
    "In model monitoring, KL divergence is used to monitor production environments, specifically around feature and prediction data. KL Divergence is utilized to ensure that input or output data in production doesn’t drastically change from a baseline. The baseline can be a training production window of data or a training or validation dataset.\n",
    "\n",
    "Drift monitoring can be especially useful for teams that receive delayed ground truth to compare against production model decisions. These teams can rely on changes in prediction and feature distributions as a proxy for performance.\n",
    "\n",
    "KL divergence is typically applied to each feature independently; it is not designed as a covariant feature measure but rather a metric that shows how each feature has diverged independently from the baseline values.\n",
    "\"\"\"\n",
    "import math\n",
    "\n",
    "def freqToProb(tokenList):\n",
    "    freqDist = {}\n",
    "    totalTokens = len(tokenList)\n",
    "    \n",
    "    for token in tokenList:\n",
    "        if token in freqDist:\n",
    "            freqDist[token] += 1\n",
    "        else:\n",
    "            freqDist[token] = 1\n",
    "            \n",
    "    return {token: count / totalTokens for token, count in freqDist.items()}\n",
    "\n",
    "\n",
    "def klDivergence(P, Q, alpha=1):\n",
    "    divergence = 0\n",
    "    V = set(P.keys()) | set(Q.keys())  \n",
    "    vSize = len(V)  \n",
    "    \n",
    "    for token in V:\n",
    "        p = (P.get(token, 0) * sum(P.values()) + alpha) / (sum(P.values()) + vSize * alpha)\n",
    "        q = (Q.get(token, 0) * sum(Q.values()) + alpha) / (sum(Q.values()) + vSize * alpha)\n",
    "        \n",
    "        divergence += p * (0 if p == 0 else (0 if q == 0 else (0 if p / q == 0 else math.log(p / q))))\n",
    "        \n",
    "    return divergence\n",
    "\n",
    "# Top two longest\n",
    "top = max1[0]\n",
    "bottom = max2[0]\n",
    "\n",
    "topTokens = myWordTokenizer(top)\n",
    "bottomTokens = myWordTokenizer(bottom)\n",
    "\n",
    "topTweetTokens = myTweetTokenizer(top)\n",
    "bottomTweetTokens = myTweetTokenizer(bottom)\n",
    "\n",
    "topBpeTokens = huggingFace(top, tokenizer, True)\n",
    "bottomBpeTokens = huggingFace(bottom, tokenizer, True)\n",
    "\n",
    "topProb = freqToProb(topTokens)\n",
    "bottomProb = freqToProb(bottomTokens)\n",
    "\n",
    "topTweetProb = freqToProb(topTweetTokens)\n",
    "bottomTweetProb = freqToProb(bottomTweetTokens)\n",
    "\n",
    "topBpeProb = freqToProb(topBpeTokens[0])\n",
    "bottomBpeProb = freqToProb(bottomBpeTokens[0])\n",
    "\n",
    "kl = klDivergence(topProb, bottomProb)\n",
    "kltweet = klDivergence(topTweetProb, bottomTweetProb)\n",
    "klbpe = klDivergence(topBpeProb, bottomBpeProb)\n",
    "\n",
    "\n",
    "print(f\"Kl is {kl} while kltweet is {kltweet} while klbpe is {klbpe}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Edit Distance (30 points)\n",
    "Implement the edit distance algorithm described in the textbook in Section 2.17 (Jan 2023 edition, page 24) in a function called Min-Edit-Distance. Do *not* use any available library functions for this but write the implementation in Python yourself from scratch. Use a *substituion cost of 3 and insertion and deletion cost of 1*. \n",
    "\n",
    "\n",
    "Test your code with the examples from class - \"execution\" and \"intention\".\n",
    "\n",
    "Compute the edit distance between the 2 largest distinct GeneRIFs as defined in question 3. Print it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
      "Wall time: 14.3 µs\n",
      "Both component proteins of the Fe-nitrogenase system, the electron-delivery Fe protein (AnfH) plus the catalytic FeFe protein (AnfDGK), were purified. Under turnover conditions, N2 reduction is inhibited by H2. HD is formed when presented with N2 and D2 due to accumulation of 4 reducing equivalents as 2 metal-bound hydrides and 2 protons at the FeFe-cofactor, with activation by reductive elimination of H2. AnfH. AnfDGK.\n",
      "\n",
      "8\n",
      "510\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# based on the pseudocode from book and wikipedia\n",
    "#https://www.wikipedia.org/en/Levenshtein_distance \n",
    "def minEdit(source, target, subCost=3, insCost=1, delCost=1):\n",
    "    m, n = len(source), \\\n",
    "           len(target)\n",
    "    matrix = [[0 for _ in range(n + 1)] \n",
    "                 for _ in range(m + 1)]\n",
    "    for i in range(m + 1): matrix[i][0] = i * delCost\n",
    "    for j in range(n + 1): matrix[0][j] = j * insCost\n",
    "    \n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if source[i-1] == target[j-1]: \n",
    "                matrix[i][j] = matrix[i-1][j-1]\n",
    "            else: matrix[i][j] = min(matrix[i-1][j-1] \n",
    "                                   + subCost, matrix[i-1][j] \n",
    "                                   + delCost, matrix[i][j-1] \n",
    "                                   + insCost)\n",
    "    return matrix[m][n]\n",
    "\n",
    "source = max2[0]\n",
    "target = max1[0]\n",
    "print(minEdit(\"execution\", \"intention\"))\n",
    "print(minEdit(source, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perplexity Question (15 points)\n",
    "You may use ChatGPT for this question, if you do, include your prompt, original generated code as well as your final modified code. Install any needed tokenizers or other tools if they are not included in the starting kernel.\n",
    "<font color=\"red\"> I did use ChatGPT for this question, this was my prompt \"Give me an example of acquiring perplexity using the the huggingface library, use 3 different libraries, GPT-2, Microsoft's BioGPT, BERT base uncased\"</font>\n",
    "ChatGPT response:\n",
    "```python\n",
    "from evaluate import load\n",
    "from transformers import AutoModelForCausalLM, AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "# Load the perplexity metric from the evaluate library\n",
    "perplexity_metric = load(\"perplexity\")\n",
    "\n",
    "def calculatePerplexity(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), inputs[\"input_ids\"].view(-1), reduction='mean')\n",
    "    return perplexity_metric(loss)\n",
    "\n",
    "# GPT-2\n",
    "gpt2Model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "gpt2Tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Microsoft's bioGPT\n",
    "biogptModel = AutoModelForCausalLM.from_pretrained(\"microsoft/BioGPT\")\n",
    "biogptTokenizer = AutoTokenizer.from_pretrained(\"microsoft/BioGPT\")\n",
    "\n",
    "# BERT base uncased\n",
    "bertModel = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "bertTokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "gpt2Perplexities = [calculatePerplexity(gpt2Model, gpt2Tokenizer, text) for text in corpus]\n",
    "biogptPerplexities = [calculatePerplexity(biogptModel, biogptTokenizer, text) for text in corpus]\n",
    "bertPerplexities = [calculatePerplexity(bertModel, bertTokenizer, text) for text in corpus]\n",
    "\n",
    "meanPerplexityGpt2 = sum(gpt2Perplexities) / len(gpt2Perplexities)\n",
    "meanPerplexitybiogpt = sum(biogptPerplexities) / len(biogptPerplexities)\n",
    "meanPerplexityBert = sum(bertPerplexities) / len(bertPerplexities)\n",
    "\n",
    "print(f\"Mean Perplexity for GPT-2: {meanPerplexityGpt2}\")\n",
    "print(f\"Mean Perplexity for BioGPT: {meanPerplexitybiogpt}\")\n",
    "print(f\"Mean Perplexity for BERT: {meanPerplexityBert}\")\n",
    "```\n",
    "This wasn't quite using the hugging face eval library so I looked at the documentation to change this to below\n",
    "\n",
    "Compute the mean perplexity of the GeneRIF corpus using the huggingface evaluate library. Use 3 different libraries: (9 points)\n",
    "* GPT-2\n",
    "* Microsoft's biogpt\n",
    "* BERT base uncased\n",
    "\n",
    "Explain the difference in perplexity between these models, you may need to refer to either the original papers or hugging-face documentation to understand these models. (6 points)\n",
    "<font color=\"red\"> GPT-2 is a general-purpose language model trained on a diverse range of internet text.It may not be specialized for biomedical text, which could result in higher perplexity when evaluated on the GeneRIF corpus. Microsoft's model is fine-tuned specifically for biomedical text. Therefore, it is expected to have lower perplexity on the GeneRIF corpus compared to GPT-2. BERT is designed for a wide array of NLP tasks and is not specialized for text generation. It uses a masked language model, which is different from the causal language models used by GPT-2 and BioGPT. This could result in different perplexity scores.</font>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/mikegtr/nlp/CS662/Homework1/Assignment1_Fall2023.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcheaha.rc.uab.edu/home/mikegtr/nlp/CS662/Homework1/Assignment1_Fall2023.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#!pip install evaluate\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcheaha.rc.uab.edu/home/mikegtr/nlp/CS662/Homework1/Assignment1_Fall2023.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#!pip install ipywidgets\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcheaha.rc.uab.edu/home/mikegtr/nlp/CS662/Homework1/Assignment1_Fall2023.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39;49msystem(\u001b[39m'\u001b[39;49m\u001b[39mpip install sacremoses\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcheaha.rc.uab.edu/home/mikegtr/nlp/CS662/Homework1/Assignment1_Fall2023.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcheaha.rc.uab.edu/home/mikegtr/nlp/CS662/Homework1/Assignment1_Fall2023.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mCompute the mean perplexity of the GeneRIF corpus using the huggingface evaluate library. Use 3 different libraries: (9 points)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcheaha.rc.uab.edu/home/mikegtr/nlp/CS662/Homework1/Assignment1_Fall2023.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m* GPT-2\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcheaha.rc.uab.edu/home/mikegtr/nlp/CS662/Homework1/Assignment1_Fall2023.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcheaha.rc.uab.edu/home/mikegtr/nlp/CS662/Homework1/Assignment1_Fall2023.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mExplain the difference in perplexity between these models, you may need to refer to either the original papers or hugging-face documentation to understand these models. (6 points)\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcheaha.rc.uab.edu/home/mikegtr/nlp/CS662/Homework1/Assignment1_Fall2023.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mevaluate\u001b[39;00m \u001b[39mimport\u001b[39;00m load\n",
      "File \u001b[0;32m/data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages/ipykernel/zmqshell.py:655\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser_ns[\u001b[39m\"\u001b[39m\u001b[39m_exit_code\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m system(cmd)\n\u001b[1;32m    654\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser_ns[\u001b[39m\"\u001b[39m\u001b[39m_exit_code\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m system(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvar_expand(cmd, depth\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n",
      "File \u001b[0;32m/data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages/IPython/utils/_process_posix.py:148\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    146\u001b[0m     child \u001b[39m=\u001b[39m pexpect\u001b[39m.\u001b[39mspawnb(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msh, args\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m-c\u001b[39m\u001b[39m'\u001b[39m, cmd]) \u001b[39m# Pexpect-U\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m     child \u001b[39m=\u001b[39m pexpect\u001b[39m.\u001b[39;49mspawn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msh, args\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39m-c\u001b[39;49m\u001b[39m'\u001b[39;49m, cmd])  \u001b[39m# Vanilla Pexpect\u001b[39;00m\n\u001b[1;32m    149\u001b[0m flush \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mflush\n\u001b[1;32m    150\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[39m# res is the index of the pattern that caused the match, so we\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[39m# know whether we've finished (if we matched EOF) or not\u001b[39;00m\n",
      "File \u001b[0;32m/data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages/pexpect/pty_spawn.py:205\u001b[0m, in \u001b[0;36mspawn.__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m<pexpect factory incomplete>\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spawn(command, args, preexec_fn, dimensions)\n\u001b[1;32m    206\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_poll \u001b[39m=\u001b[39m use_poll\n",
      "File \u001b[0;32m/data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages/pexpect/pty_spawn.py:303\u001b[0m, in \u001b[0;36mspawn._spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[39m# Encode command line using the specified encoding\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m [a \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, \u001b[39mbytes\u001b[39m) \u001b[39melse\u001b[39;00m a\u001b[39m.\u001b[39mencode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding)\n\u001b[1;32m    301\u001b[0m                  \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs]\n\u001b[0;32m--> 303\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mptyproc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spawnpty(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[1;32m    304\u001b[0m                              cwd\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcwd, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    306\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mptyproc\u001b[39m.\u001b[39mpid\n\u001b[1;32m    307\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchild_fd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mptyproc\u001b[39m.\u001b[39mfd\n",
      "File \u001b[0;32m/data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages/pexpect/pty_spawn.py:315\u001b[0m, in \u001b[0;36mspawn._spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_spawnpty\u001b[39m(\u001b[39mself\u001b[39m, args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    314\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m ptyprocess\u001b[39m.\u001b[39;49mPtyProcess\u001b[39m.\u001b[39;49mspawn(args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/site-packages/ptyprocess/ptyprocess.py:230\u001b[0m, in \u001b[0;36mPtyProcess.spawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions, pass_fds)\u001b[0m\n\u001b[1;32m    227\u001b[0m exec_err_pipe_read, exec_err_pipe_write \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpipe()\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m use_native_pty_fork:\n\u001b[0;32m--> 230\u001b[0m     pid, fd \u001b[39m=\u001b[39m pty\u001b[39m.\u001b[39;49mfork()\n\u001b[1;32m    231\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     \u001b[39m# Use internal fork_pty, for Solaris\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     pid, fd \u001b[39m=\u001b[39m _fork_pty\u001b[39m.\u001b[39mfork_pty()\n",
      "File \u001b[0;32m/data/user/mikegtr/Conda_Env/nlp2023v2/lib/python3.10/pty.py:102\u001b[0m, in \u001b[0;36mfork\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[39mreturn\u001b[39;00m pid, fd\n\u001b[1;32m    101\u001b[0m master_fd, slave_fd \u001b[39m=\u001b[39m openpty()\n\u001b[0;32m--> 102\u001b[0m pid \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mfork()\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m pid \u001b[39m==\u001b[39m CHILD:\n\u001b[1;32m    104\u001b[0m     \u001b[39m# Establish a new session.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     os\u001b[39m.\u001b[39msetsid()\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "#!pip install evaluate\n",
    "#!pip install ipywidgets\n",
    "!pip install sacremoses\n",
    "\"\"\"\n",
    "Compute the mean perplexity of the GeneRIF corpus using the huggingface evaluate library. Use 3 different libraries: (9 points)\n",
    "* GPT-2\n",
    "* Microsoft's biogpt\n",
    "* BERT base uncased\n",
    "\n",
    "Explain the difference in perplexity between these models, you may need to refer to either the original papers or hugging-face documentation to understand these models. (6 points)\"\"\"\n",
    "from evaluate import load\n",
    "from transformers import AutoModelForCausalLM, AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "# Load the perplexity metric from the evaluate library\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "\n",
    "# Initialize models and tokenizers\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "biogpt_model = AutoModelForCausalLM.from_pretrained(\"microsoft/BioGPT\")\n",
    "biogpt_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BioGPT\")\n",
    "\n",
    "bert_model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Assuming corpus is a list of text samples\n",
    "gpt2_results = []\n",
    "biogpt_results = []\n",
    "bert_results = []\n",
    "\n",
    "for text in corpus:\n",
    "    # Tokenize and get output text\n",
    "    gpt2_output = gpt2_tokenizer.decode(gpt2_model.generate(gpt2_tokenizer.encode(text, return_tensors=\"pt\"))[0])\n",
    "    biogpt_output = biogpt_tokenizer.decode(biogpt_model.generate(biogpt_tokenizer.encode(text, return_tensors=\"pt\"))[0])\n",
    "    bert_output = bert_tokenizer.decode(bert_model.generate(bert_tokenizer.encode(text, return_tensors=\"pt\"))[0])\n",
    "\n",
    "    # Compute perplexity\n",
    "    gpt2_results.append(perplexity.compute(predictions=gpt2_output, references=text))\n",
    "    biogpt_results.append(perplexity.compute(predictions=biogpt_output, references=text))\n",
    "    bert_results.append(perplexity.compute(predictions=bert_output, references=text))\n",
    "\n",
    "# Calculate mean perplexity\n",
    "mean_perplexity_gpt2 = sum(result['perplexity'] for result in gpt2_results) / len(gpt2_results)\n",
    "mean_perplexity_biogpt = sum(result['perplexity'] for result in biogpt_results) / len(biogpt_results)\n",
    "mean_perplexity_bert = sum(result['perplexity'] for result in bert_results) / len(bert_results)\n",
    "\n",
    "print(f\"Mean Perplexity for GPT-2: {mean_perplexity_gpt2}\")\n",
    "print(f\"Mean Perplexity for BioGPT: {mean_perplexity_biogpt}\")\n",
    "print(f\"Mean Perplexity for BERT: {mean_perplexity_bert}\")\n",
    "\n",
    "\n",
    "\n",
    "# import evaluate\n",
    "# from evaluate import load\n",
    "# from transformers import AutoModelForCausalLM, AutoModelForMaskedLM, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# def calculatePerplexity(model, tokenizer, text):\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.logits\n",
    "#         loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), inputs[\"input_ids\"].view(-1), reduction='mean')\n",
    "#     return torch.exp(loss).item()\n",
    "\n",
    "# # GPT-2\n",
    "# gpt2Model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "# gpt2Tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# # Microsoft's bioGPT\n",
    "# biogptModel = AutoModelForCausalLM.from_pretrained(\"microsoft/BioGPT\")\n",
    "# biogptTokenizer = AutoTokenizer.from_pretrained(\"microsoft/BioGPT\")\n",
    "\n",
    "# # BERT base uncased\n",
    "# bertModel = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "# bertTokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# gpt2Perplexities = [calculatePerplexity(gpt2Model, gpt2Tokenizer, text) for text in corpus]\n",
    "# biogptPerplexities = [calculatePerplexity(biogptModel, biogptTokenizer, text) for text in corpus]\n",
    "# bertPerplexities = [calculatePerplexity(bertModel, bertTokenizer, text) for text in corpus]\n",
    "\n",
    "# meanPerplexityGpt2 = sum(gpt2Perplexities) / len(gpt2Perplexities)\n",
    "# meanPerplexitybiogpt = sum(biogptPerplexities) / len(biogptPerplexities)\n",
    "# meanPerplexityBert = sum(bertPerplexities) / len(bertPerplexities)\n",
    "\n",
    "# print(f\"Mean Perplexity for GPT-2: {meanPerplexityGpt2}\")\n",
    "# print(f\"Mean Perplexity for BioGPT: {meanPerplexitybiogpt}\")\n",
    "# print(f\"Mean Perplexity for BERT: {meanPerplexityBert}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  7. Document Classification (20 points)\n",
    "ChatGPT is reccomended, but not required for this question. Use git to pull the data set from inside this notebook.  Write a Naive Bayes algorithm in python (without using any 3rd party libraries) to classify SMS messages as Ham or Spam from the csv file at https://github.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/blob/master/spam.csv (10 pts)\n",
    "\n",
    "There are many solutions available for this online, I suggest using ChatGPT but if you use other websites to assist you, do not forget to cite them. Regardless if you use ChatGPT or not, provide an explaination as to what each generated (or written) function does in English. (10 pts)\n",
    "\n",
    "<font color=\"red\">The Naive Bayes Classifier is a probability based classifier. It trains on a piece of data that included examples of ham and spam examples which were labeled, which makes this supervised learning. I counted all of these up respectively of whether it was ham or spam. For classification I assigned a probability value based on how many I saw and how many hams/spams there was. I then went through the newly seen piece of text and counted the words and their probability given if they were seen prior in ham or spam then return ham or spam depending on which probability was higher.</font>\n",
    "\n",
    "<font color=\"red\"> I asked ChatGPT for Naive Bayes in Java, using the following prompt \"Naive Bayes in java\" and it gave me a java version, which I read and implemented in Python</font>\n",
    "Java Output\n",
    "```java\n",
    "import java.util.HashMap;\n",
    "import java.util.Map;\n",
    "\n",
    "public class NaiveBayesClassifier {\n",
    "    private Map<String, Integer> positiveCounts = new HashMap<>();\n",
    "    private Map<String, Integer> negativeCounts = new HashMap<>();\n",
    "    private int positiveDocs = 0;\n",
    "    private int negativeDocs = 0;\n",
    "    private int totalDocs = 0;\n",
    "\n",
    "    public void train(String text, String label) {\n",
    "        String[] words = text.split(\" \");\n",
    "        for (String word : words) {\n",
    "            if (\"Positive\".equals(label)) {\n",
    "                positiveCounts.put(word, positiveCounts.getOrDefault(word, 0) + 1);\n",
    "            } else {\n",
    "                negativeCounts.put(word, negativeCounts.getOrDefault(word, 0) + 1);\n",
    "            }\n",
    "        }\n",
    "\n",
    "        if (\"Positive\".equals(label)) {\n",
    "            positiveDocs++;\n",
    "        } else {\n",
    "            negativeDocs++;\n",
    "        }\n",
    "        totalDocs++;\n",
    "    }\n",
    "\n",
    "    public String classify(String text) {\n",
    "        String[] words = text.split(\" \");\n",
    "        double positiveProb = Math.log((double) positiveDocs / totalDocs);\n",
    "        double negativeProb = Math.log((double) negativeDocs / totalDocs);\n",
    "\n",
    "        for (String word : words) {\n",
    "            positiveProb += Math.log((positiveCounts.getOrDefault(word, 1) + 1.0) / (positiveDocs + 2.0));\n",
    "            negativeProb += Math.log((negativeCounts.getOrDefault(word, 1) + 1.0) / (negativeDocs + 2.0));\n",
    "        }\n",
    "\n",
    "        return positiveProb > negativeProb ? \"Positive\" : \"Negative\";\n",
    "    }\n",
    "\n",
    "    public static void main(String[] args) {\n",
    "        NaiveBayesClassifier classifier = new NaiveBayesClassifier();\n",
    "\n",
    "        // Training\n",
    "        classifier.train(\"I love this product\", \"Positive\");\n",
    "        classifier.train(\"This is amazing\", \"Positive\");\n",
    "        classifier.train(\"I hate this\", \"Negative\");\n",
    "        classifier.train(\"This is terrible\", \"Negative\");\n",
    "\n",
    "        // Classification\n",
    "        String result1 = classifier.classify(\"I love this\");\n",
    "        String result2 = classifier.classify(\"This is terrible\");\n",
    "\n",
    "        System.out.println(\"Classification of 'I love this': \" + result1);\n",
    "        System.out.println(\"Classification of 'This is terrible': \" + result2);\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The naive bayes classifier has a naive accuracy score of 0.8699551569506726\n"
     ]
    }
   ],
   "source": [
    "class Naive():\n",
    "    def __init__(self):\n",
    "        self.ham = dict()\n",
    "        self.spam = dict()\n",
    "        self.hamc = 0\n",
    "        self.spamc = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def train(self, text, label):\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            hashed = hash(word)\n",
    "            if label == \"ham\":\n",
    "                self.ham[hashed] = self.ham.get(hashed, 0) + 1\n",
    "            else: \n",
    "                self.spam[hashed] = self.spam.get(hashed, 0) + 1\n",
    "            \n",
    "        if label == \"ham\":\n",
    "            self.hamc += 1\n",
    "        else:\n",
    "            self.spamc += 1\n",
    "\n",
    "        self.total += 1\n",
    "    \n",
    "    def classify(self, text):\n",
    "        probabilityHam = self.hamc / self.total\n",
    "        probabilitySpam = self.spamc / self.total\n",
    "        words = text.split()\n",
    "\n",
    "        for word in words:\n",
    "            hashed = hash(word)\n",
    "            probabilityHam *= (self.ham.get(hashed, 1) + 1) / self.hamc + 2\n",
    "            probabilitySpam *= (self.spam.get(hashed, 1) + 1) / self.spamc + 2\n",
    "\n",
    "        return \"ham\" if probabilityHam > probabilitySpam else \"spam\"\n",
    "\n",
    "naive = Naive()\n",
    "\n",
    "labels = []\n",
    "text = []\n",
    "with open(\"spam.csv\", \"r\", encoding=\"latin-1\") as f:\n",
    "    header = f.readline().strip().split(',')\n",
    "    labelsIdx = header.index('v1')\n",
    "    textIdx = header.index('v2')\n",
    "\n",
    "    for line in f:\n",
    "        fields = line.strip().split(',')\n",
    "        labels.append(fields[labelsIdx])\n",
    "        text.append(fields[textIdx])\n",
    "\n",
    "# 80/20 split, for training and testing\n",
    "length = len(labels)\n",
    "eighty = int(length - (length * 0.20))\n",
    "twenty = int(length - eighty)\n",
    "for i in range(0, eighty):\n",
    "    naive.train(text[i], labels[i])\n",
    "\n",
    "truth = []\n",
    "for i in range(eighty, length):\n",
    "    if naive.classify(text[i]) == labels[i]:\n",
    "        truth.append(0)\n",
    "    else:\n",
    "        truth.append(1)\n",
    "\n",
    "accuracy = truth.count(0) / twenty\n",
    "print(f\"The naive bayes classifier has a naive accuracy score of {accuracy}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp2023v2]",
   "language": "python",
   "name": "conda-env-nlp2023v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
