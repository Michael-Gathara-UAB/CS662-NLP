{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Natural Language Processing  Assignment #1 Fall 2023 \n",
    "## Due September 20th, 2023  11:59pm CST\n",
    "## Student Name:<font color='red'> Michael Gathara </font>\n",
    "Regular Expressions, Tokenization and Normalization, Dynamic Programming, Document Classification, Perplexity\n",
    "\n",
    "* Getting started on cheaha: https://docs.uabgrid.uab.edu/wiki/Cheaha_GettingStarted\n",
    "* Instructions on running Jupyter Notebook on cheaha: https://docs.uabgrid.uab.edu/wiki/Jupyter#Jupyter_on_Cheaha\n",
    "* IPython notebooks: https://ipython.org/ipython-doc/3/notebook/notebook.html#introduction\n",
    "\n",
    "<font color=\"red\">Do not forget to answer the written questions! </font>Just coding will not get you full points. You can answer the questions in the same text box as they are asked.\n",
    "\n",
    "As of the time of writing, a suitable conda environment with NLTK and other needed libraries should be or should be soon available as a Jupyter kernel - \"nlp2022\" or \"DeepPhe\". Ambitious students may still want to learn to create their own environment, but code should work in the provided environment. Instructions for creating your own custom environment on cheaha are provided below:\n",
    "https://docs.uabgrid.uab.edu/wiki/Jupyter#Adding_Custom_Conda_Environments_to_Jupyter\n",
    "\n",
    "Note: I give \"hints\" via imports, but not all imports are shown.\n",
    "\n",
    "Submission will be via Canvas. Attempt all questions, butn not all questions may be marked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. De-identification with Regular Expressions (20 points)\n",
    "\n",
    "You started working for a company that needs to scrub identifiers from user uploaded files in order to comply with EU General Data Protection Regulation (GDPR) . A beta version of a piece of de-identification software used in the United States removed physicians name, patient names and some location names from the note below, but failed to location and some date information. \n",
    "\n",
    "\n",
    "* Write a regular expression/s to replace ALL date and location references with XX/XX/XXXX and XXXXX respectively. \n",
    "\n",
    "* Make use of a dictionary (small set of terms) to assist with this. \n",
    "\n",
    "Your regular expressions should work for this note, but not be so specific that they are unlikely to be useful on other notes. \n",
    "\n",
    "* Print before and after documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT TEXT\n",
      "\n",
      "                XXXXXX, XXXXXXX\n",
      "                ##########\n",
      "                London Clinic\n",
      "                XXXXX \n",
      "                XXXXX, W1G 6BW\n",
      "                Date 07/01/2023\n",
      "        \n",
      "NEUROLOGY AND PAIN MEDICINE\n",
      "INITIAL OUTPATIENT NOTE\n",
      "\n",
      "XXXXXX, XXXXXX\n",
      "PATIENT HISTORY\n",
      "Ms. XXXXXX is a 56y women seen in consultation at the request of Dr. XXXXXXXXXX. The patient is alert \n",
      "and orinted to self, place and cirumstance. She self-reports stress due the lingering effects of the\n",
      "her 17 year-old son's suicide and has difficulty continuing her work. Denies depression, SI. PMH BPD.\n",
      "\n",
      "Consitution: Well developed, well nourished in no apparent distress.\n",
      "\n",
      "Assessment: Fibromyalgia, stress\n",
      "\n",
      "TREATMENT/PLAN:\n",
      "Duloxetine\n",
      "Suggested finding more time for exercise and sleep for stress\n",
      "\n",
      "REFERRING PHYSICIAN\n",
      "XXXXXX, XXXXXX, MD\n",
      "\n",
      "Electronially signed by XXXXXX, XXXXXX DO, PhD on 07/02/2023\n",
      "\n",
      "CONVERTED TEXT------------------\n",
      "\n",
      "                XXXXXX, XXXXXXX\n",
      "                ##########\n",
      "XXXXX\n",
      "                Date XX/XX/XXXX\n",
      "        \n",
      "NEUROLOGY AND PAIN MEDICINE\n",
      "INITIAL OUTPATIENT NOTE\n",
      "\n",
      "XXXXXX, XXXXXX\n",
      "XXXXX\n",
      "her 17 year-old son's suicide and has difficulty continuing her work. Denies depression, SI. PMH BPD.\n",
      "\n",
      "Consitution: Well developed, well nourished in no apparent distress.\n",
      "\n",
      "Assessment: Fibromyalgia, stress\n",
      "\n",
      "TREATMENT/PLAN:\n",
      "Duloxetine\n",
      "Suggested finding more time for exercise and sleep for stress\n",
      "\n",
      "REFERRING PHYSICIAN\n",
      "XXXXXX, XXXXXX, MD\n",
      "\n",
      "Electronially signed by XXXXXX, XXXXXX DO, PhD on XX/XX/XXXX\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os;\n",
    "import re;\n",
    "fakedoc = \"\"\"\n",
    "                XXXXXX, XXXXXXX\n",
    "                ##########\n",
    "                London Clinic\n",
    "                XXXXX \n",
    "                XXXXX, W1G 6BW\n",
    "                Date 07/01/2023\n",
    "        \n",
    "NEUROLOGY AND PAIN MEDICINE\n",
    "INITIAL OUTPATIENT NOTE\n",
    "\n",
    "XXXXXX, XXXXXX\n",
    "PATIENT HISTORY\n",
    "Ms. XXXXXX is a 56y women seen in consultation at the request of Dr. XXXXXXXXXX. The patient is alert \n",
    "and orinted to self, place and cirumstance. She self-reports stress due the lingering effects of the\n",
    "her 17 year-old son's suicide and has difficulty continuing her work. Denies depression, SI. PMH BPD.\n",
    "\n",
    "Consitution: Well developed, well nourished in no apparent distress.\n",
    "\n",
    "Assessment: Fibromyalgia, stress\n",
    "\n",
    "TREATMENT/PLAN:\n",
    "Duloxetine\n",
    "Suggested finding more time for exercise and sleep for stress\n",
    "\n",
    "REFERRING PHYSICIAN\n",
    "XXXXXX, XXXXXX, MD\n",
    "\n",
    "Electronially signed by XXXXXX, XXXXXX DO, PhD on 07/02/2023\n",
    "\"\"\"\n",
    "print(\"CURRENT TEXT\")\n",
    "print(fakedoc)\n",
    "\n",
    "# Write regular expression/s here, be as general as possible with your regular expression\n",
    "# fakedoc = re.sub(r\"(0?[1-9]|[12][0-9]|3[01])/[0-9]+/((20[012]\\d|19\\d\\d)|(1\\d|2[0123]))\", \"XX/XX/XXXX\", fakedoc)\n",
    "# fakedoc = re.sub(r\"(\\t*)\" + \"(.+)\\n(.+)\\n(.+),\\s(.+)\\n\", r'\\1' + \"XXXXX\\n\", fakedoc)\n",
    "# Using dictionary\n",
    "regexHolder = {\n",
    "    r\"(0?[1-9]|[12][0-9]|3[01])/[0-9]+/((20[012]\\d|19\\d\\d)|(1\\d|2[0123]))\" : \"XX/XX/XXXX\",\n",
    "    r\"(\\t*)\" + \"(.+)\\n(.+)\\n(.+),\\s(.+)\\n\" : r'\\1' + \"XXXXX\\n\"\n",
    "}\n",
    "\n",
    "for regex in regexHolder:\n",
    "    fakedoc = re.sub(regex, regexHolder[regex], fakedoc)\n",
    "\n",
    "print(\"CONVERTED TEXT------------------\")\n",
    "print(fakedoc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. De-identification with ChatGPT and Use of Regex (10 points+5 bonus points)\n",
    "\n",
    "#### Design a prompt to have ChatGPT (GPT-3.5 @ https://chat.openai.com/) do Question 1 for you (2 pts).\n",
    "\n",
    "* <strong>Show both the prompt you designed (1 pt) and the output from ChatGPT (1 pt). </strong>\n",
    "- This is my prompt\n",
    "```\n",
    "You are an expert programmer, who has been utilizing Python with the re library. You understand everything about regex and how it works. Now think of the following exercise\n",
    "\n",
    "```START EXERCISE\n",
    "You started working for a company that needs to scrub identifiers from user uploaded files in order to comply with EU General Data Protection Regulation (GDPR) . A beta version of a piece of de-identification software used in the United States removed physicians name, patient names and some location names from the note below, but failed to location and some date information. \n",
    "\n",
    "\n",
    "* Write a regular expression/s to replace ALL date and location references with XX/XX/XXXX and XXXXX respectively. \n",
    "\n",
    "* Make use of a dictionary (small set of terms) to assist with this. \n",
    "\n",
    "Your regular expressions should work for this note, but not be so specific that they are unlikely to be useful on other notes. \n",
    "\n",
    "* Print before and after documents.\n",
    "```END EXERCISE\n",
    "\n",
    "This is some starter code to get you started\n",
    "```START STARTER\n",
    "import os;\n",
    "import re;\n",
    "fakedoc = \"\"\"\n",
    "                XXXXXX, XXXXXXX\n",
    "                ##########\n",
    "                London Clinic\n",
    "                XXXXX \n",
    "                XXXXX, W1G 6BW\n",
    "                Date 07/01/2023\n",
    "        \n",
    "NEUROLOGY AND PAIN MEDICINE\n",
    "INITIAL OUTPATIENT NOTE\n",
    "\n",
    "XXXXXX, XXXXXX\n",
    "PATIENT HISTORY\n",
    "Ms. XXXXXX is a 56y women seen in consultation at the request of Dr. XXXXXXXXXX. The patient is alert \n",
    "and orinted to self, place and cirumstance. She self-reports stress due the lingering effects of the\n",
    "her 17 year-old son's suicide and has difficulty continuing her work. Denies depression, SI. PMH BPD.\n",
    "\n",
    "Consitution: Well developed, well nourished in no apparent distress.\n",
    "\n",
    "Assessment: Fibromyalgia, stress\n",
    "\n",
    "TREATMENT/PLAN:\n",
    "Duloxetine\n",
    "Suggested finding more time for exercise and sleep for stress\n",
    "\n",
    "REFERRING PHYSICIAN\n",
    "XXXXXX, XXXXXX, MD\n",
    "\n",
    "Electronially signed by XXXXXX, XXXXXX DO, PhD on 07/02/2023\n",
    "\"\"\"\n",
    "\n",
    "print(\"CURRENT TEXT\")\n",
    "print(fakedoc)\n",
    "```END START STARTER\n",
    "\n",
    "Can you think deeply and carefully about what you need to do to accomplish this exercise then accmplish it\n",
    "```\n",
    "This is the output from ChatGPT, GPT3.5\n",
    "```\n",
    "Sure, I understand the exercise. Your goal is to write regular expressions to replace all date and location references in the given document with \"XX/XX/XXXX\" and \"XXXXX\" respectively, while making use of a small set of terms stored in a dictionary. Here's how you can accomplish this:\n",
    "```\n",
    "```python\n",
    "import re\n",
    "\n",
    "# Define the dictionary for location and date replacements\n",
    "replacement_dict = {\n",
    "    'London': 'XXXXX',\n",
    "    'W1G 6BW': 'XXXXX',\n",
    "    '07/01/2023': 'XX/XX/XXXX',\n",
    "    '07/02/2023': 'XX/XX/XXXX'\n",
    "}\n",
    "\n",
    "# Define the regular expression patterns for date and location\n",
    "date_pattern = r'\\d{2}/\\d{2}/\\d{4}'\n",
    "location_pattern = r'\\b[A-Z][a-z]+\\b'\n",
    "\n",
    "# Read the document text\n",
    "fakedoc = \"\"\"\n",
    "# ... (document text as provided in the starter code)\n",
    "\"\"\"\n",
    "\n",
    "# Function to replace text using the dictionary\n",
    "def replace_with_dict(match):\n",
    "    return replacement_dict.get(match.group(0), match.group(0))\n",
    "\n",
    "# Replace date references using the date_pattern and the dictionary\n",
    "fakedoc = re.sub(date_pattern, replace_with_dict, fakedoc)\n",
    "\n",
    "# Replace location references using the location_pattern and the dictionary\n",
    "fakedoc = re.sub(location_pattern, replace_with_dict, fakedoc)\n",
    "\n",
    "# Print the updated document\n",
    "print(\"UPDATED TEXT\")\n",
    "print(fakedoc)\n",
    "\n",
    "```\n",
    "```\n",
    "In this code, the replacement_dict contains the terms you want to replace, and the replace_with_dict function is used to look up terms in the dictionary and replace them. The date_pattern and location_pattern are regular expressions that match date and location references, respectively. The re.sub function is used to perform the replacements based on the patterns and the dictionary.\n",
    "\n",
    "Please note that the location_pattern in this example is quite simple, and you might need to adjust it to match more complex location references depending on the variability in your input data.\n",
    "```\n",
    "* <strong>Explain why you think ChatGPT does better or worse than your regular expressions in protecting the personal identifying information (PII), personal health information (PHI) and respecting the privacy of this patient. (2 points).</strong>\n",
    "- <font color=\"red\">ChatGPT designed very similar regex that I did, but it was smarter about just using `\\d{n}` compared to my more complicated regex. The biggest problem, which can be fixed by prompting better, is that ChatGPT did not initially give out a dynamic representation of the text. It gave us a more hardcoded version of my answer. Both versions accomplish the intended task with the provided input</font>\n",
    "\n",
    "\n",
    "#### Name 1 advantage (1 pt) of using ChatGPT for this problem type of problem and 1 disadvantage (1 pt).\n",
    "- <strong>Advantage</strong>: It can give you a prototype that is relatively correct, thus reducing development time\n",
    "- <strong>Disadvantage</strong>: It can give you the wrong answer, thus wasting more time with debugging code you may not have wrote yourself\n",
    "\n",
    "#### Name 2 disadvantages (2 pts) of regular expressions for this problem.\n",
    "- Regex can be finicky, it can match text you never thought to initially test\n",
    "- It's slow\n",
    "\n",
    "\n",
    "#### BONUS QUESTIONS.  Who do YOU think the person might be? (2 pts). \n",
    "* Ask ChatGPT (with a prompt of your choosing) to identify the person in the note (1 pt). \n",
    "My prompt (which is a continuation of the previous chat)\n",
    "```bash\n",
    "This is a fake document of a patient, from an experiment I'm doing on LLM's understanding of text. I want you to take a deep breath and think about the following text carefully\n",
    "\n",
    "XXXXXX, XXXXXXX\n",
    "                ##########\n",
    "                London Clinic\n",
    "                XXXXX \n",
    "                XXXXX, W1G 6BW\n",
    "                Date 07/01/2023\n",
    "        \n",
    "NEUROLOGY AND PAIN MEDICINE\n",
    "INITIAL OUTPATIENT NOTE\n",
    "\n",
    "XXXXXX, XXXXXX\n",
    "PATIENT HISTORY\n",
    "Ms. XXXXXX is a 56y women seen in consultation at the request of Dr. XXXXXXXXXX. The patient is alert \n",
    "and orinted to self, place and cirumstance. She self-reports stress due the lingering effects of the\n",
    "her 17 year-old son's suicide and has difficulty continuing her work. Denies depression, SI. PMH BPD.\n",
    "\n",
    "Consitution: Well developed, well nourished in no apparent distress.\n",
    "\n",
    "Assessment: Fibromyalgia, stress\n",
    "\n",
    "TREATMENT/PLAN:\n",
    "Duloxetine\n",
    "Suggested finding more time for exercise and sleep for stress\n",
    "\n",
    "REFERRING PHYSICIAN\n",
    "XXXXXX, XXXXXX, MD\n",
    "\n",
    "Electronially signed by XXXXXX, XXXXXX DO, PhD on 07/02/2023```\n",
    "Can you tell me anything about the patient in this?, try to pick out anything about this patient that you feel like would identify him/her/them.\n",
    "```\n",
    "ChatGPT's output\n",
    "```bash\n",
    "Based on the provided text, I can gather some information about the patient:\n",
    "\n",
    "    Name: The patient's name is redacted as \"XXXXXX, XXXXXXX.\"\n",
    "\n",
    "    Age: The patient is a 56-year-old woman.\n",
    "\n",
    "    Medical Conditions: The patient has been diagnosed with Fibromyalgia and is experiencing stress. There is no mention of any other specific medical conditions.\n",
    "\n",
    "    Mental Health: The patient self-reports stress related to the lingering effects of her 17-year-old son's suicide. She denies depression and suicidal ideation (SI). There is also a reference to \"PMH BPD,\" which might suggest a previous medical history of Borderline Personality Disorder.\n",
    "\n",
    "    Physical Appearance: The patient is described as \"well developed, well nourished,\" and \"in no apparent distress.\"\n",
    "\n",
    "    Treatment/Plan: The treatment plan for the patient includes the prescription of Duloxetine and the suggestion to find more time for exercise and sleep to address stress.\n",
    "\n",
    "    Referring Physician: The referring physician who requested the consultation is mentioned as \"XXXXXX, XXXXXX, MD.\"\n",
    "\n",
    "    Signature: The document is electronically signed by \"XXXXXX, XXXXXX DO, PhD\" on 07/02/2023.\n",
    "\n",
    "Please note that the patient's name and other identifying information have been redacted in the text, so it is not possible to identify the patient specifically from this information.\n",
    "```\n",
    "Which seems to be the correct answer, it noticed that the person's name was taken out and replaced.\n",
    "* Give 2 limitations of ChatGPT to explain why it is likely to have difficulty with identifying this person (2 pts).\n",
    "- It works by predicting the next text \n",
    "- It can hallucinate and make up answers\n",
    "These limitations can take it down the wrong road of answering the question. The data gives no clear answer as to exactly who the person is as their information is taken away, thus leaving the interpretation strictly to ChatGPT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-tokenization (20 points)\n",
    "GeneRIF (https://www.ncbi.nlm.nih.gov/gene/about-generif) is like Twitter for genes. Biomedical researchers add small free text annotations linking their publication to a particular gene. Each GeneRIF can be considered a document.\n",
    "\n",
    "For this question <font color=\"red\"> DO NOT use ChatGPT for this problem</font> and :\n",
    "* Import a subset of GeneRIF documents, the file is available in canvas and called generifs1000.tsv\n",
    "* Create a corpus, in this case a python list of 1000 geneRIF documents (do not include meta-data in the corpus) in a variable called corpus and print out the number of documents in a corpus (the most basic corpus statistic)\n",
    "* Implement a function that uses the sequential hugging face pre-tokenizer that tokenizes on Whitespace and Punctation. You will use this function again in later questions.\n",
    "* Why is pre-tokenization needed before tokenization?\n",
    "- <font color=\"red\"> Pre-tokenization is needed before tokenization because it helps to create a standard for tokenization to occur. It helps to increase consistency, accuracy and efficiency.</font>\n",
    "* Print out and save (max1 and max2) distinct geneRIFs that have the highest token count in the data set based on your own whitespace/comma tokenizer. Print out should include token count for both geneRIFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the corpus is: 1000\n",
      "Max 1 is 68 and Max 2 is 64\n",
      "Max 1 text \n",
      " Data show that TraA requires TraB to form an active adhesin for self-recognition. In the absence of TraB, TraA localizes to the cell surface but does not function in cell- binding. TraA contains a predicted sorting motif, MYXO-CTERM, contributing to its cell surface display. TraB, with a beta-barrel and cell wall binding domain, may help anchor TraA to the cell envelop during adhesion and outer membrane exchange. [TraA...\n",
      " \n",
      "\tand Max 2 text \n",
      " Both component proteins of the Fe-nitrogenase system, the electron-delivery Fe protein (AnfH) plus the catalytic FeFe protein (AnfDGK), were purified. Under turnover conditions, N2 reduction is inhibited by H2. HD is formed when presented with N2 and D2 due to accumulation of 4 reducing equivalents as 2 metal-bound hydrides and 2 protons at the FeFe-cofactor, with activation by reductive elimination of H2. AnfH. AnfDGK.\n",
      "\n",
      "CPU times: user 6.34 ms, sys: 0 ns, total: 6.34 ms\n",
      "Wall time: 18.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import io\n",
    "from tokenizers import Tokenizer, pre_tokenizers, BertWordPieceTokenizer, models, trainers\n",
    "from tokenizers.pre_tokenizers import Whitespace, Sequence, Punctuation\n",
    "import csv\n",
    "\n",
    "corpus = []\n",
    "with open(\"generifs1000.tsv\", \"r\") as f:\n",
    "    corpus += [line.split(\"\\t\")[4:] for line in f]\n",
    "corpus = corpus[1:]\n",
    "\n",
    "# Since this will be used later, I made it a function\n",
    "\n",
    "fullText = \"\"\n",
    "for x in range(len(corpus)):\n",
    "    fullText += ''.join(corpus[x])\n",
    "\n",
    "def huggingFaceTrainer(trainingData, size):\n",
    "    filePath = f\"{size}.json\"\n",
    "    if os.path.isfile(filePath):\n",
    "        return Tokenizer.from_file(filePath)\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    trainer = trainers.BpeTrainer(vocab_size=size) \n",
    "    tokenizer.train_from_iterator(trainingData, trainer=trainer)\n",
    "    tokenizer.save(filePath)\n",
    "    return tokenizer\n",
    "\n",
    "def huggingFace(content, tokenizer) -> list:\n",
    "    tokenizer.pre_tokenizer = Sequence([Whitespace(), Punctuation()])\n",
    "    token = tokenizer.encode(content).tokens    \n",
    "    return [token, len(token)] \n",
    "\n",
    "def myTokenizer(content) -> list:\n",
    "    tokenized = content.replace(',', '').split(' ')\n",
    "    return [tokenized, len(tokenized)] \n",
    "\n",
    "print(f\"The length of the corpus is: {len(corpus)}\")\n",
    "mytokenizedData = dict()\n",
    "lengths = [['', 0]]\n",
    "for text in corpus:\n",
    "    actualText = text[0]\n",
    "    hashedText = hash(actualText)\n",
    "    if mytokenizedData.get(hashedText):\n",
    "        continue\n",
    "    mytokenizedtext = myTokenizer(actualText)\n",
    "    mytokenizedData[hashedText] = mytokenizedtext\n",
    "    thisTokLen = mytokenizedtext[1]\n",
    "    if int(thisTokLen) > int(lengths[-1][1]):\n",
    "        lengths.insert(len(lengths) ,[actualText, thisTokLen])\n",
    "    elif int(thisTokLen) > int(lengths[-2][1]):\n",
    "        lengths.insert(len(lengths) - 1, [actualText, thisTokLen])\n",
    "\n",
    "max1 = lengths[-1]\n",
    "max2 = lengths[-2]\n",
    "    \n",
    "print(f\"Max 1 is {max1[1]} and Max 2 is {max2[1]}\")\n",
    "print(f\"Max 1 text \\n {max1[0]} \\n\\tand Max 2 text \\n {max2[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenization (20 points)\n",
    "\n",
    "Tokenize the GeneRIF corpus using nltk's TweetTokenizer and word_tokenize. Remove stopwords using nltk's English stopwords from both the word_tokenize tokens and the TweetTokenizer tokens. Print out average number of tokens per geneRIF for (1) TweetTokenizer and (2) NLTK word_tokenize. \n",
    "\n",
    "\n",
    "* Write a function to compute the Jacard distance between 2 token sets. Find the single geneRIF/s in the set for which is the most disimilar in terms of Jacard differences between the TweetTokenizer and nltk's workd_tokenize after stopword removal. Print it out.\n",
    "\n",
    "* Which tokenizer is more appropriate and why? Do you think this type of corpus requires a better tokenizer?\n",
    "<font color=\"red\">I would say word tokenizer. The word tokenizer was able to tokenize for more text than the tweet tokenizer</font>\n",
    "\n",
    "* Create a new tokenizer by training your own Byte Pair Encoding tokenizer using the pre-tokenizer you wrote in question (3) using geneRIFS100K as training data. Tokenize the largest geneRIF/s using your newly trained tokenizer and comment on the advantages of using Byte Pair Encoding versus traditional tokenization approaches.\n",
    "<font color=\"red\">I think it should be based on your needs, BPE lets you have more control and generalization on your data as compared to having a more specific tokenization.</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mikegtr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mikegtr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tweet avg is 21.479229989868287 and word average is 20.214792299898683\n",
      "The most dissimilar text is: thermostability of cyp175a1\n",
      "\n",
      "[['Data', 'show', 'that', 'TraA', 'requires', 'TraB', 'to', 'form', 'an', 'active', 'adhesin', 'for', 'self', '-', 'recognition', '.', 'In', 'the', 'absence', 'of', 'TraB', ',', 'TraA', 'localizes', 'to', 'the', 'cell', 'surface', 'but', 'does', 'not', 'function', 'in', 'cell', '-', 'binding', '.', 'TraA', 'contains', 'a', 'predicted', 'sorting', 'motif', ',', 'MYXO', '-', 'CTERM', ',', 'contributing', 'to', 'its', 'cell', 'surface', 'display', '.', 'TraB', ',', 'with', 'a', 'beta', '-', 'barrel', 'and', 'cell', 'wall', 'binding', 'domain', ',', 'may', 'help', 'anchor', 'TraA', 'to', 'the', 'cell', 'envelop', 'during', 'adhesion', 'and', 'outer', 'membrane', 'exchange', '.', '[', 'TraA', '.', '.', '.'], 88]\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "import nltk, os\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "corpus = []\n",
    "with open(\"generifs1000.tsv\", \"r\") as f:\n",
    "    corpus += [line.split(\"\\t\")[4:] for line in f]\n",
    "corpus = corpus[1:]\n",
    "stop = set(stopwords.words('english'))\n",
    "tweetCounter = 0\n",
    "wordCounter = 0\n",
    "mytokenizedData = dict()\n",
    "\n",
    "# Jaccard Distance function\n",
    "def jacDist(first: set, second: set):\n",
    "    intersect = len(first.intersection(second))\n",
    "    union = len(first.union(second))\n",
    "    return 1 - (intersect / union)\n",
    "\n",
    "# Tweet Tokenizer function\n",
    "def myTweetTokenizer(content) -> list:\n",
    "    TTokenizer = TweetTokenizer()\n",
    "    tokenized = TTokenizer.tokenize(content)\n",
    "    Atokenized = [x for x in tokenized if x not in stop]\n",
    "    global tweetCounter\n",
    "    tweetCounter += len(Atokenized)\n",
    "    return Atokenized\n",
    "\n",
    "# NLTK Tokenizer function\n",
    "def myWordTokenizer(content) -> list:\n",
    "    tokenized = word_tokenize(content)\n",
    "    Atokenized = [x for x in tokenized if x not in stop]\n",
    "    global wordCounter\n",
    "    wordCounter += len(Atokenized)\n",
    "    return Atokenized\n",
    "\n",
    "# Main processing loop\n",
    "maxJacDist = 0\n",
    "dissimilar = \"\"\n",
    "disHolder = []\n",
    "\n",
    "for text in corpus:\n",
    "    actualText = text[0].lower()\n",
    "    if mytokenizedData.get(hash(actualText)):\n",
    "        continue\n",
    "    tweetTokenized = set(myTweetTokenizer(actualText))\n",
    "    wordTokenized = set(myWordTokenizer(actualText))\n",
    "\n",
    "    jDist = jacDist(tweetTokenized, wordTokenized)\n",
    "\n",
    "    if jDist > maxJacDist:\n",
    "        maxJacDist = jDist\n",
    "        disHolder.append(actualText)\n",
    "        dissimilar = actualText\n",
    "\n",
    "    mytokenizedData[hash(actualText)] = [tweetTokenized, wordTokenized]\n",
    "\n",
    "# Using myTokenizedData length cause I remove duplicates within the corpus\n",
    "size = len(mytokenizedData)\n",
    "\n",
    "avgTweet = tweetCounter / size\n",
    "avgWord = wordCounter / size\n",
    "\n",
    "print(f\"\\n\\nTweet avg is {avgTweet} and word average is {avgWord}\")\n",
    "print(f\"The most dissimilar text is: {dissimilar}\")\n",
    "\n",
    "\n",
    "\n",
    "# Byte Pair Encoding\n",
    "trainingData = []\n",
    "with open(\"generifs100K\", \"r\") as f:\n",
    "    trainingData += f\n",
    "\n",
    "trainingData = trainingData[1:]\n",
    "size = len(trainingData)\n",
    "\n",
    "tokenizer = huggingFaceTrainer(trainingData, size)\n",
    "myData = dict()\n",
    "largestTokText = max1[0]\n",
    "secondLargestText = max2[0]\n",
    "tokenizedLarge = huggingFace(largestTokText, tokenizer)\n",
    "tokenizedSecLarge = huggingFace(secondLargestText, tokenizer)\n",
    "print(tokenizedLarge)\n",
    "# for text in corpus:\n",
    "#     actualText = text[0].lower()\n",
    "#     if myData.get(hash(actualText)):\n",
    "#         continue\n",
    "#     tokenized = huggingFace(actualText, tokenizer)\n",
    "#     myData[hash(actualText)] = tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. KL Divergence (15 Points Available, 5 are Required for all students, remaining 10 are bonus for 662, required for 762)\n",
    "\n",
    "You may use ChatGPT for this question. Create 3 overall corpus-level probability distributions for the tokens in the geneRIF corpus, based on your results from the NLTK's tokenizer, the TweetTokenizer and your own BytePairEncoding Scheme after stopword removal. (5 points) Use native python code only so you understand how this function is computed. \n",
    "\n",
    "Look up KL Divergence in Wikipedia or online. What major pitfall do you anticipate if you use this metric to compare token probability distributions? Compute the KL Divergence (a non-symetric measure) using only native python code (no libraries) between your own BytePairEncoding scheme and the 2 other distributions using LaPlace smoothing. (10 points). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Edit Distance (30 points)\n",
    "Implement the edit distance algorithm described in the textbook in Section 2.17 (Jan 2023 edition, page 24) in a function called Min-Edit-Distance. Do *not* use any available library functions for this but write the implementation in Python yourself from scratch. Use a *substituion cost of 3 and insertion and deletion cost of 1*. \n",
    "\n",
    "\n",
    "Test your code with the examples from class - \"execution\" and \"intention\".\n",
    "\n",
    "Compute the edit distance between the 2 largest distinct GeneRIFs as defined in question 3. Print it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 7.63 µs\n",
      "19\n",
      "510\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# based on the pseudocode from book and wikipedia\n",
    "#https://www.wikipedia.org/en/Levenshtein_distance \n",
    "def minEdit(source, target, subCost=3, insCost=1, delCost=1):\n",
    "    m, n = len(source), \\\n",
    "           len(target)\n",
    "    matrix = [[0 for _ in range(n + 1)] \n",
    "                 for _ in range(m + 1)]\n",
    "    for i in range(m + 1): matrix[i][0] = i * delCost\n",
    "    for j in range(n + 1): matrix[0][j] = j * insCost\n",
    "    \n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if source[i-1] == target[j-1]: \n",
    "                matrix[i][j] = matrix[i-1][j-1]\n",
    "            else: matrix[i][j] = min(matrix[i-1][j-1] \n",
    "                                   + subCost, matrix[i-1][j] \n",
    "                                   + delCost, matrix[i][j-1] \n",
    "                                   + insCost)\n",
    "    return matrix[m][n]\n",
    "\n",
    "source = max2[0]\n",
    "target = max1[0]\n",
    "print(source)\n",
    "print(minEdit(\"execution\", \"intention\"))\n",
    "print(minEdit(source, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perplexity Question (15 points)\n",
    "You may use ChatGPT for this question, if you do, include your prompt, original generated code as well as your final modified code. Install any needed tokenizers or other tools if they are not included in the starting kernel.\n",
    "\n",
    "Compute the mean perplexity of the GeneRIF corpus using the huggingface evaluate library. Use 3 different libraries: (9 points)\n",
    "* GPT-2\n",
    "* Microsoft's biogpt\n",
    "* BERT base uncased\n",
    "\n",
    "Explain the difference in perplexity between these models, you may need to refer to either the original papers or hugging-face documentation to understand these models. (6 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install evaluate\n",
    "#!pip install ipywidgets\n",
    "import evaluate\n",
    "from evaluate import load\n",
    "from transformers import AutoModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  7. Document Classification (20 points)\n",
    "ChatGPT is reccomended, but not required for this question. Use git to pull the data set from inside this notebook.  Write a Naive Bayes algorithm in python (without using any 3rd party libraries) to classify SMS messages as Ham or Spam from the csv file at https://github.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/blob/master/spam.csv (10 pts)\n",
    "\n",
    "There are many solutions available for this online, I suggest using ChatGPT but if you use other websites to assist you, do not forget to cite them. Regardless if you use ChatGPT or not, provide an explaination as to what each generated (or written) function does in English. (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp2023v2]",
   "language": "python",
   "name": "conda-env-nlp2023v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
